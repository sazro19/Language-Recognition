<!DOCTYPE html>
<html lang="ru" data-vue-meta="%7B%22lang%22:%7B%22ssr%22:%22ru%22%7D%7D">
<head >
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0,viewport-fit=cover">
  <title>Нейросети могут оказаться проще, чем принято считать / Хабр</title>
  <style>
    /* cyrillic-ext */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveSxf6TF0.woff2) format('woff2');
      unicode-range: U+0460-052F, U+1C80-1C88, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;
    }

    /* cyrillic */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveQhf6TF0.woff2) format('woff2');
      unicode-range: U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;
    }

    /* latin-ext */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveSBf6TF0.woff2) format('woff2');
      unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
    }

    /* latin */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveRhf6.woff2) format('woff2');
      unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
    }
  </style>
  <link rel="preload" href="https://assets.habr.com/habr-web/css/chunk-vendors.e7843cc0.css" as="style"><link rel="preload" href="https://assets.habr.com/habr-web/js/chunk-vendors.c2c3fc9a.js" as="script"><link rel="preload" href="https://assets.habr.com/habr-web/css/app.02ee25a4.css" as="style"><link rel="preload" href="https://assets.habr.com/habr-web/js/app.c0af73e7.js" as="script">
  <link rel="stylesheet" href="https://assets.habr.com/habr-web/css/chunk-vendors.e7843cc0.css"><link rel="stylesheet" href="https://assets.habr.com/habr-web/css/app.02ee25a4.css">
  <script>window.i18nFetch = new Promise((res, rej) => {
          const xhr = new XMLHttpRequest();
          xhr.open('GET', '/js/i18n/ru-compiled.85eb77f0b17c8235e7b64b9f81ea5ec2.json');
          xhr.responseType = 'json';
          xhr.onload = function(e) {
            if (this.status === 200) {
              res({ru: xhr.response});
            } else {
              rej(e);
            }
          };
          xhr.send();
        });</script>
  
  <script data-vue-meta="ssr" src="/js/ads.js" onload="window['zhY4i4nJ9K'] = true" data-vmid="checkad"></script><script data-vue-meta="ssr" type="application/ld+json" data-vmid="ldjson-schema">{"@context":"http:\/\/schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/habr.com\/ru\/company\/skillfactory\/blog\/584170\/"},"headline":"Нейросети могут оказаться проще, чем принято считать","datePublished":"2021-10-18T22:06:26+03:00","dateModified":"2021-10-22T18:02:20+03:00","author":{"@type":"Person","name":"honyaki"},"publisher":{"@type":"Organization","name":"Habr","logo":{"@type":"ImageObject","url":"https:\/\/habrastorage.org\/webt\/a_\/lk\/9m\/a_lk9mjkccjox-zccjrpfolmkmq.png"}},"description":"Нейросети отчасти будто подрывают традиционную теорию машинного обучения, которая сильно опирается на идеи теории вероятности и статистики. В чём же заключается...","url":"https:\/\/habr.com\/ru\/company\/skillfactory\/blog\/584170\/#post-content-body","about":["c_skillfactory","h_maths","h_read","h_popular_science","h_artificial_intelligence","f_develop","f_management","f_popsci"],"image":["https:\/\/habr.com\/share\/publication\/584170\/3740c1e31006d6fca3a1d5842c7f5c25\/","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/8a6\/371\/45e\/8a637145e56ff345a069bf048d528c13.jpg","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/e01\/b89\/876\/e01b89876f6a2e7e499e544a77e72bba.png"]}</script>
  <script src="//www.googletagservices.com/tag/js/gpt.js" async></script>
  <style>.grecaptcha-badge{visibility: hidden;}</style>
  <meta name="habr-version" content="2.49.0">
  
  <meta data-vue-meta="ssr" property="fb:app_id" content="444736788986613"><meta data-vue-meta="ssr" property="fb:pages" content="472597926099084"><meta data-vue-meta="ssr" name="twitter:card" content="summary_large_image"><meta data-vue-meta="ssr" name="twitter:site" content="@habr_eng"><meta data-vue-meta="ssr" property="og:title" content="Нейросети могут оказаться проще, чем принято считать" data-vmid="og:title"><meta data-vue-meta="ssr" name="twitter:title" content="Нейросети могут оказаться проще, чем принято считать" data-vmid="twitter:title"><meta data-vue-meta="ssr" name="aiturec:title" content="Нейросети могут оказаться проще, чем принято считать" data-vmid="aiturec:title"><meta data-vue-meta="ssr" name="description" content="Нейросети отчасти будто подрывают традиционную теорию машинного обучения, которая сильно опирается на идеи теории вероятности и статистики. В чём же заключается загадка их успеха?Исследователи..." data-vmid="description"><meta data-vue-meta="ssr" itemprop="description" content="Нейросети отчасти будто подрывают традиционную теорию машинного обучения, которая сильно опирается на идеи теории вероятности и статистики. В чём же заключается загадка их успеха?Исследователи..." data-vmid="description:itemprop"><meta data-vue-meta="ssr" property="og:description" content="Нейросети отчасти будто подрывают традиционную теорию машинного обучения, которая сильно опирается на идеи теории вероятности и статистики. В чём же заключается загадка их успеха?Исследователи..." data-vmid="og:description"><meta data-vue-meta="ssr" name="twitter:description" content="Нейросети отчасти будто подрывают традиционную теорию машинного обучения, которая сильно опирается на идеи теории вероятности и статистики. В чём же заключается загадка их успеха?Исследователи..." data-vmid="twitter:description"><meta data-vue-meta="ssr" property="aiturec:description" content="Нейросети отчасти будто подрывают традиционную теорию машинного обучения, которая сильно опирается на идеи теории вероятности и статистики. В чём же заключается загадка их успеха?Исследователи..." data-vmid="aiturec:description"><meta data-vue-meta="ssr" itemprop="image" content="https://habrastorage.org/getpro/habr/upload_files/775/a09/173/775a091730993f28871abea6cc2ca62e.jpg" data-vmid="image:itemprop"><meta data-vue-meta="ssr" property="og:image" content="https://habrastorage.org/getpro/habr/upload_files/775/a09/173/775a091730993f28871abea6cc2ca62e.jpg" data-vmid="og:image"><meta data-vue-meta="ssr" property="aiturec:image" content="https://habrastorage.org/getpro/habr/upload_files/775/a09/173/775a091730993f28871abea6cc2ca62e.jpg" data-vmid="aiturec:image"><meta data-vue-meta="ssr" name="twitter:image" content="https://habrastorage.org/getpro/habr/upload_files/775/a09/173/775a091730993f28871abea6cc2ca62e.jpg" data-vmid="twitter:image"><meta data-vue-meta="ssr" property="vk:image" content="https://habrastorage.org/getpro/habr/upload_files/775/a09/173/775a091730993f28871abea6cc2ca62e.jpg" data-vmid="vk:image"><meta data-vue-meta="ssr" property="aiturec:item_id" content="584170" data-vmid="aiturec:item_id"><meta data-vue-meta="ssr" property="aiturec:datetime" content="2021-10-18T19:06:26.000Z" data-vmid="aiturec:datetime"><meta data-vue-meta="ssr" property="og:type" content="article" data-vmid="og:type"><meta data-vue-meta="ssr" property="og:locale" content="ru_RU" data-vmid="og:locale"><meta data-vue-meta="ssr" property="og:image:width" content="1200" data-vmid="og:image:width"><meta data-vue-meta="ssr" property="og:image:height" content="630" data-vmid="og:image:height">
  <link data-vue-meta="ssr" href="https://habr.com/ru/rss/post/584170/?fl=ru" type="application/rss+xml" title="" rel="alternate" name="rss"><link data-vue-meta="ssr" href="https://habr.com/ru/company/skillfactory/blog/584170/" rel="canonical" data-vmid="canonical"><link data-vue-meta="ssr" data-vmid="hreflang"><link data-vue-meta="ssr" image_src="image" href="https://habrastorage.org/getpro/habr/upload_files/775/a09/173/775a091730993f28871abea6cc2ca62e.jpg" data-vmid="image:href"><link data-vue-meta="ssr" rel="amphtml" href="https://habr.com/ru/amp/post/584170/">
  <meta name="apple-mobile-web-app-status-bar-style" content="#303b44">
  <meta name="msapplication-TileColor" content="#629FBC">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="mobile-web-app-capable" content="yes">
  <link
    rel="shortcut icon"
    type="image/png"
    sizes="16x16"
    href="https://assets.habr.com/habr-web/img/favicons/favicon-16.png"
  >
  <link
    rel="shortcut icon"
    type="image/png"
    sizes="32x32"
    href="https://assets.habr.com/habr-web/img/favicons/favicon-32.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="76x76"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-76.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="120x120"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-120.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="152x152"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-152.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="180x180"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-180.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="256x256"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-256.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1136x640.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2436x1125.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1792x828.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_828x1792.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1334x750.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1242x2668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2208x1242.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1125x2436.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1242x2208.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2732x2048.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2688x1242.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2224x1668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_750x1334.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2048x2732.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2388x1668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1668x2224.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_640x1136.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1668x2388.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2048x1536.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1536x2048.png"
  >
  <link
    rel="mask-icon"
    color="#77a2b6"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-120.svg"
  >
  <link
    crossorigin="use-credentials"
    href="/manifest.webmanifest"
    rel="manifest"
  >
</head>
<body>


<div id="app" data-server-rendered="true" data-async-called="true"><div class="tm-layout__wrapper"><!----> <div></div> <!----> <header class="tm-header"><div class="tm-page-width"><div class="tm-header__container"><!----> <span class="tm-header__logo-wrap"><a href="/ru/" class="tm-header__logo tm-header__logo_ru"><svg height="16" width="16" class="tm-svg-img tm-header__icon"><title>Хабр</title> <use xlink:href="/img/habr-logo-ru.svg#logo"></use></svg></a> <span class="tm-header__beta-sign" style="display:none;">β</span></span> <div class="tm-dropdown tm-header__projects"><div class="tm-dropdown__head"><button class="tm-header__dropdown-toggle"><svg height="16" width="16" class="tm-svg-img tm-header__icon tm-header__icon_dropdown"><title>Открыть список</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#arrow-down"></use></svg></button></div> <!----></div> <a href="/ru/sandbox/start/" class="tm-header__become-author-btn">
            Как стать автором
          </a> <div class="tm-feature tm-header__feature tm-feature_variant-inline"><!----></div> <!----> <!----></div></div></header> <div class="tm-layout"><div class="tm-page-progress-bar"></div> <div data-menu-sticky="true" class="tm-base-layout__header tm-base-layout__header_is-sticky"><div class="tm-page-width"><div class="tm-base-layout__header-wrapper"><div class="tm-main-menu"><div class="tm-main-menu__section"><nav class="tm-main-menu__section-content"><!----> <a href="/ru/all/" class="tm-main-menu__item">
        Все потоки
      </a> <a href="/ru/flows/develop/" class="tm-main-menu__item">
          Разработка
        </a><a href="/ru/flows/admin/" class="tm-main-menu__item">
          Администрирование
        </a><a href="/ru/flows/design/" class="tm-main-menu__item">
          Дизайн
        </a><a href="/ru/flows/management/" class="tm-main-menu__item">
          Менеджмент
        </a><a href="/ru/flows/marketing/" class="tm-main-menu__item">
          Маркетинг
        </a><a href="/ru/flows/popsci/" class="tm-main-menu__item">
          Научпоп
        </a></nav></div></div> <div class="tm-header-user-menu tm-base-layout__user-menu"><a href="/ru/search/" class="tm-header-user-menu__item tm-header-user-menu__search"><svg height="24" width="24" class="tm-svg-img tm-header-user-menu__icon tm-header-user-menu__icon_search tm-header-user-menu__icon_dark"><title>Поиск</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#search"></use></svg></a> <!----> <!----> <!----> <div class="tm-header-user-menu__item tm-header-user-menu__user_desktop"><div class="tm-dropdown"><div class="tm-dropdown__head"><svg height="24" width="24" data-test-id="menu-toggle-guest" class="tm-svg-img tm-header-user-menu__icon"><title>Профиль</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#header-user"></use></svg> <!----></div> <!----></div> <!----></div> <!----></div></div></div></div> <!----> <div class="tm-page-width"></div> <main class="tm-layout__container"><div hl="ru" companyName="skillfactory" data-async-called="true" class="tm-page"><div class="tm-page-width"><div class="tm-page__header"><!----></div> <div class="tm-page__wrapper"><div class="tm-page__main tm-page__main_has-sidebar"><div class="pull-down"><div class="pull-down__header" style="height:0px;"><div class="pull-down__content" style="bottom:10px;"><svg height="24" width="24" class="tm-svg-img pull-down__arrow"><title>Обновить</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#pull-arrow"></use></svg></div></div> <div class="tm-article-presenter"><div class="tm-company-card tm-company-article__company-card"><div class="tm-company-card__info"><div class="tm-company-card__header"><a href="/ru/company/skillfactory/profile/" class="tm-company-card__avatar"><div class="tm-entity-image"><img alt="" height="48" src="//habrastorage.org/getpro/habr/company/9bb/5f7/d50/9bb5f7d50871bf0981f64b1970f22fe1.png" width="48" class="tm-entity-image__pic"></div></a> <!----> <div class="tm-rating tm-company-card__rating"><div class="tm-rating__header"> <div class="tm-rating__counter">146.01</div></div> <div class="tm-rating__text">
    Рейтинг
  </div></div></div> <div class="tm-company-card__info"><a href="/ru/company/skillfactory/profile/" class="tm-company-card__name">
        SkillFactory
      </a> <div class="tm-company-card__description">Школа Computer Science. Скидка 45% по коду HABR</div></div></div> <div class="tm-company-card__buttons"><!----> <!----></div></div> <div class="tm-article-presenter__body"><div class="tm-misprint-area"><div class="tm-misprint-area__wrapper"><article class="tm-article-presenter__content tm-article-presenter__content_narrow"><div class="tm-article-presenter__header"> <div class="tm-article-snippet tm-article-presenter__snippet"><div class="tm-article-snippet__meta-container"><div class="tm-article-snippet__meta"><span class="tm-user-info tm-article-snippet__author"><a href="/ru/users/honyaki/" title="honyaki" class="tm-user-info__userpic"><div class="tm-entity-image"><img alt="" height="24" loading="lazy" src="//habrastorage.org/r/w32/getpro/habr/avatars/96e/81d/bc0/96e81dbc01247c6c855b1806b355efa3.jpg" width="24" class="tm-entity-image__pic"></div></a> <span class="tm-user-info__user"><a href="/ru/users/honyaki/" class="tm-user-info__username">
      honyaki
    </a> </span></span> <span class="tm-article-snippet__datetime-published"><time datetime="2021-10-18T19:06:26.000Z" title="2021-10-18, 22:06">18  октября   в 22:06</time></span></div> <!----></div> <h1 lang="ru" class="tm-article-snippet__title tm-article-snippet__title_h1"><span>Нейросети могут оказаться проще, чем принято считать</span></h1> <div class="tm-article-snippet__hubs"><span class="tm-article-snippet__hubs-item"><a href="/ru/company/skillfactory/blog/" class="tm-article-snippet__hubs-item-link router-link-active"><span>Блог компании SkillFactory</span> <!----></a></span><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/maths/" class="tm-article-snippet__hubs-item-link"><span>Математика</span> <span title="Профильный хаб" class="tm-article-snippet__profiled-hub">*</span></a></span><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/read/" class="tm-article-snippet__hubs-item-link"><span>Читальный зал</span> <!----></a></span><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/popular_science/" class="tm-article-snippet__hubs-item-link"><span>Научно-популярное</span> <!----></a></span><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/artificial_intelligence/" class="tm-article-snippet__hubs-item-link"><span>Искусственный интеллект</span> <!----></a></span></div> <div class="tm-article-snippet__labels"><div class="tm-article-snippet__label"><span>
        Перевод
      </span></div></div> <!----> <!----></div></div> <div class="tm-article-presenter__origin"><a href="https://www.quantamagazine.org/a-new-link-to-an-old-model-could-crack-the-mystery-of-deep-learning-20211011/" target="_blank" class="tm-article-presenter__origin-link">
                Автор оригинала:
                <span>
                  Anil Ananthaswamy
                </span></a></div> <div data-gallery-root="" lang="ru" class="tm-article-body"><div id="post-content-body" class="article-formatted-body article-formatted-body_version-2"><div xmlns="http://www.w3.org/1999/xhtml"><figure class="full-width "><img src="https://habrastorage.org/r/w780q1/getpro/habr/upload_files/8a6/371/45e/8a637145e56ff345a069bf048d528c13.jpg" width="780" height="440" data-src="https://habrastorage.org/getpro/habr/upload_files/8a6/371/45e/8a637145e56ff345a069bf048d528c13.jpg" data-blurred="true"/><figcaption></figcaption></figure><p>Нейросети отчасти будто подрывают традиционную теорию машинного обучения, которая сильно опирается на идеи теории вероятности и статистики. В чём же заключается загадка их успеха?</p><p>Исследователи показывают, что сети с бесконечным числом нейронов математически эквивалентны более простым моделям машинного обучения — ядерным методам. Поразительные результаты можно объяснить, если эта эквивалентность простирается дальше «идеальных» нейронных сетей. Подробности рассказываем к старту нашего флагманского <a href="https://skillfactory.ru/data-scientist-pro?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=data-science_dspr_181021&amp;utm_term=lead">курса по Data Science</a>.</p><hr/><p>Обычно считается, что модели ML работают лучше, когда имеют подходящее число параметров. Если параметров слишком мало, модель может оказаться слишком простой и не улавливать все нюансы. Слишком много параметров — и модель усложняется, изучая настолько тонкие детали, что затем не может обобщать. Это и называется переобучением. </p><blockquote><p>«Это баланс между тем, чтобы слишком хорошо обучиться на данных, и тем, чтобы не обучиться вообще. Хочется быть посередине», — рассказывает взволнованный новыми перспективами <a href="https://datascience.ucsd.edu/about/our-team/name/mikhail-belkin/">Михаил Белкин</a>, исследователь машинного обучения из Калифорнийского университета в Сан-Диего.</p></blockquote><p>Глубокие нейронные сети, например VGG, по общему мнению, имеют слишком много параметров, а значит, их прогнозы должны страдать от переобучения. Но этого не происходит. Напротив,  такие сети обобщают новые данные с удивительным успехом. Почему? Ответа на этот вопрос не знал никто, хотя выяснить причину пытались. </p><p>Специалист в компьютерных науках и нейробиолог из Еврейского университета Иерусалима, Нафтали Тишби, утверждал, что глубокие нейронные сети сначала обучаются на данных, а затем <a href="https://arxiv.org/abs/1503.02406">проходят</a> через информационное бутылочное горлышко, отбрасывая нерелевантную информацию. Это и <a href="https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/">помогает </a>в обобщении. Другие учёные<a href="https://openreview.net/pdf?id=ry_WPG-A-"> считают</a>, что это происходит не во всех сетях.</p><p>Математическая эквивалентность ядерных методов и идеализированных нейронных сетей даёт ключ к тому, почему и как сети с огромным числом параметров приходят к своим решениям.</p><p>Ядерные методы — это алгоритмы, которые находят закономерности в данных через их проекцию на очень высокие измерения. Изучая более понятные эквиваленты ядер идеализированных нейронных сетей, исследователи могут узнать, почему сложные глубокие сети сходятся в процессе обучения к решениям, которые хорошо обобщаются на новые данные.</p><blockquote><p>«Нейронная сеть немного похожа на машину Руба Голдберга. Неизвестно, что в этой машине по-настоящему важно, — утверждает Белкин. — Ядерные методы не настолько сложны. Думаю, что упрощение нейронных сетей до ядерных методов позволяет выделить движущую силу происходящего».</p></blockquote><h3>Немного истории</h3><p>Ядерные методы полагаются на область математики с долгой историей. История эта восходит к немецкому математику XIX века Карлу Гауссу. Именно он придумал ядро, названное в его честь — гауссовым. Такое ядро отображает переменную <em>X</em> на знаменитую функцию распределения в форме колокола. </p><p>В начале XX века с помощью ядер английский математик Джеймс Мерсер решил интегральные уравнения, после этого работать с ядрами стали намного чаще, а к 1960 году их применили в машинном обучении, чтобы выполнять сложные классификации.</p><p>Понимание методов ядра требуется уже в линейных классификаторах. Что такое линейный классификатор? Допустим, кошек и собак можно разделить на две группы при помощи только двух признаков, (иначе говоря — данных в двух измерениях). </p><p>Чтобы отличить животных друг от друга, вам нужны две функции (к примеру, размер морды, который мы можем откладывать на оси <em>x</em>, и размер ушей, — на оси <em>y</em>. Если отобразить эти данные на плоскость, <em>XY</em>, кошки должны оказаться в одном кластере, а собаки — в другом.</p><h3>Поиск прямой</h3><p>Чтобы найти разделяющую кластеры прямую, то есть вычислить коэффициенты уравнения прямой [y = ax + b], можно обучить линейный классификатор на размеченных данных. После этого посмотрите, на какую сторону от линии попадают данные — это и будет прогнозом классификатора.</p><p>Любители собак и кошек могут возмутиться: реальные данные о мордах и ушах многочисленных видов кошек и собак почти наверняка нельзя разделить линейно; но тогда их можно преобразовать или спроецировать в пространство более высокой размерности. </p><p>Простой способ этого добиться — умножить значение двух признаков, чтобы получить третий. Возможно, есть нечто, что отличает собак от кошек по соотношению размеров морды и ушей.</p><p>Линейный разделитель (гиперплоскость) легче найти в обобщённом случае, когда пространство имеет более трёх измерений. Проецируясь обратно в нижние измерения, гиперплоскость принимает форму нелинейной функции с кривыми и волнистыми линиями. Эти линии разделяют исходные данные в нижних измерениях на два кластера. </p><p>Если вы работаете с реальными данными, поиск коэффициентов гиперплоскости в высоких измерениях часто оказывается вычислительно неэффективным, а иногда невозможным. Но это не касается ядерных методов.</p><h3>Ядерные методы</h3><p>Мощь ядерных методов заключена в способности делать две вещи. Они: </p><ol><li><p>Сопоставляют каждую точку в наборе данных низкой размерности с точкой из более высоких измерений. Размерность этого гиперпространства может быть бесконечной в зависимости от отображения. </p><p>Когда данные проецируются в бесконечное число измерений, нахождение коэффициентов разделяющей гиперплоскости предполагает вычисление внутреннего произведения для каждой пары признаков в высокой размерности становится сложным, когда данные проецируются в бесконечномерное пространство.</p></li><li><p>Получив две точки данных низкой размерности, они используют функцию ядра, чтобы вывести внутреннее произведение соответствующих признаков высокой размерности. </p><p>Очень важно, что с помощью этого трюка алгоритм может найти коэффициенты гиперплоскости, не касаясь пространства высокой размерности.</p></li></ol><blockquote><p>«Замечательная особенность трюка с ядром заключается в том, что все вычисления происходят в пространстве низкой размерности, а не в пространстве, где оно может иметь бесконечное число измерений», — рассказывает <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/boser.html">Бернхард Бозер</a>, заслуженный профессор Калифорнийского университета в Беркли.</p></blockquote><p>В конце 1980-х — начале 1990-x Бозер с коллегами <a href="https://guyon.chalearn.org/">Изабель Гийон</a> и <a href="https://datascience.columbia.edu/people/vladimir-vapnik/">Владимиром Вапником</a> изобрёл класс ядерных методов — методы опорных векторов<a href="https://dl.acm.org/doi/10.1145/130385.130401"> (SVM</a>). Хотя ядерные методы различных типов появились в машинном обучении в 1960-х, именно с изобретением SVM они заняли центральное место. </p><p>SVM оказались невероятно мощными. В начале 2000-х они искали сходства различных белковых последовательностей и предсказывали функции белков, работали в машинном зрении и распознавании рукописного текста.</p><p>Методы опорных векторов доминировали в ML, пока с появлением AlexNet не настало время глубоких нейронных сетей. Сообщество машинного обучения переключилось на ИНС и SVM оказались в затруднительном положении, однако ядерные методы остаются мощными моделями. Они способны на большее, чем трюк с ядром.</p><blockquote><p>«Если у вас есть мощное ядро, то вы отображаете данные в пространство этого ядра, как бы бесконечномерное и очень мощное, — рассказывает <a href="https://pluskid.org/">Чиюань Чжан</a>, научный сотрудник группы по изучению мозга в Google Research. — Вы всегда можете найти линейный разделитель в этом мощном скрытом пространстве, разделяющем данные, а возможных решений бесконечно много». </p></blockquote><p>Ограничивая пространство поиска решений, теория ядер позволяет выбрать не просто произвольный линейный разделитель, а в определённом смысле лучший из возможных. Это сродни регуляризации — уменьшению количества параметров в модели, чтобы предотвратить переобучение. Чжан задался вопросом, могут ли глубокие нейронные сети делать что-то подобное.</p><p>Глубокие нейронные сети имеют входной слой, выходной слой и по крайней мере один скрытый слой между ними. Чем больше слоёв, тем глубже сеть. Параметры сети представляют собой силу связей между нейронами. </p><p>Обучение сети для распознавания изображений — это предъявление сети ранее классифицированных изображений и определение значений её параметров, которые помогают правильно характеризовать изображения. После обучения ИНС представляет собой модель преобразования поданного изображения в выходной сигнал, то есть в метку или категорию.</p><p>Чтобы выяснить, уменьшают ли нейросети количество настраиваемых параметров, тем самым неявно выполняя регуляризацию, в 2017 году Чжан и его коллеги провели серию <a href="https://openreview.net/pdf?id=Sy8gdB9xx">эмпирических тестов</a> на AlexNet и VGG. Другими словами учёные выясняли, влияет ли режим обучения на саму возможность переобучения. Команда обнаружила, что этого не происходит. </p><p>Оказалось, что AlexNet и другие подобные ИНС действительно могут переобучиться и не обобщать данные. Но когда тем же сетям, обученным по тому же алгоритму, давали данные без изменений, они не переобучались. Напротив, они хорошо обобщали данные, когда им давали данные без изменений. </p><p>Такая неявная регуляризация не может быть ответом на загадку, а вывод требует «лучшего объяснения, чтобы характеризовать обобщение в глубоких нейронных сетях», — рассказывает Чжан.</p><h3>Бесконечные нейроны</h3><p><a href="https://arxiv.org/abs/1412.6614">Исследования</a> показывали, что нейронные сети с бо́льшим количеством нейронов в одном слое в прогнозах так же сильны или <a href="https://www.youtube.com/watch?v=V8iZOpY28_E&amp;t=223s">превосходят</a> свои аналоги с меньшим числом нейронов на один слой. </p><p>Некоторые учёные восприняли это как намёк: возможно, ИНС можно понять при помощи стратегии из физики, где «изучение предельных случаев иногда может упростить проблему», — сказал <a href="https://yasamanb.github.io/">Ясаман Бахри</a>, учёный-исследователь из команды изучения мозга Google Research. </p><p>Физики часто упрощают задачу, рассматривая крайние случаи. Что происходит, когда, например, число частиц в системе стремится к бесконечности? «В таких границах статистические эффекты могут стать проще», — рассказывает Бахри. </p><blockquote><p>Что произойдёт с нейронной сетью с математической точки зрения при бесконечном количестве слоёв?</p></blockquote><p>В 1994 году почётный профессор Университета Торонто Рэдфорд Нил показал, что если веса этой сети заданы (инициализированы) с определёнными статистическими свойствами, то до обучения сеть <a href="https://www.cs.toronto.edu/~radford/ftp/pin.pdf">математически эквивалентна</a> знаменитой ядерной функции, которая известна как гауссовский процесс. То же самое справедливо и для идеализированных глубоких нейронных сетей с бесконечным числом нейронов, где скрытых слоёв больше. Этот факт в 2017 году <a href="https://arxiv.org/abs/1711.00165">показали</a> две группы учёных, включая группу Бахри.</p><blockquote><p>«Известно, что по крайне мере иногда нейронные сети могут вести себя как ядерные методы». Артур Жако, Швейцарский федеральный технологический институт Лозанны</p></blockquote><p>Последствия поражают. Обычно аналитическое математическое выражение для прогнозирования новых данных использовать нельзя даже после обучения глубокой сети. </p><p>Это что-то вроде чёрного ящика: чтобы увидеть его содержимое, вы должны просто запустить сеть; но в идеализированном сценарии при инициализации сеть эквивалентна гауссовскому процессу. Иными словами, можно обучить ядерный метод, для которого существуют математические выражения, а нейронную сеть просто отбросить.</p><blockquote><p>«Как только вы сопоставите сеть с гауссовским процессом &lt;…> сможете аналитически рассчитать, каким должно быть предсказание», — рассказывает Бахри.</p></blockquote><p>Это уже знаковый результат, но он математически не описывал то, что происходит во время самой распространённой формы обучения: было неясно, как решение может обобщаться настолько хорошо.</p><h3>Градиентный спуск</h3><p>Часть загадки связана с процессом обучения глубоких нейросетей, где работает алгоритм <a href="https://www.quantamagazine.org/computer-scientists-discover-limits-of-major-research-algorithm-20210817/">градиентного спуска</a>. Сеть проходит по полному холмов и долин ландшафту высокой размерности, всё ниже спускаясь по ландшафту собственных ошибок, которые модель допустила на предоставленном наборе значений параметров. </p><p>После настройки параметров сеть достигает глобального минимума, то есть максимально приближается к точной классификации обучающих данных. Обучение сети — это, по сути, задача оптимизации, поиска глобального минимума. Обученная сеть — это почти оптимальная функция, которая отображает входные значения на выходные. Процесс обучения тяжело поддаётся анализу.</p><p><a href="https://www.cs.washington.edu/people/faculty/ssdu">Симон Ду</a>, эксперт по машинному обучению из Университета Вашингтона в Сиэтле, рассказывает:</p><blockquote><p>«Ни одна существующая теория не может гарантировать, что если вы примените какой-либо широко используемый алгоритм, например градиентный спуск, то [ИНС] сможет сойтись к глобальному минимуму». </p></blockquote><p>Понимать причину мы начали к концу 2018 года, когда к возможному ответу пришли несколько групп учёных одновременно. Это было сделано на основании математического анализа бесконечных сетей и анализа их связи с более понятными ядерными методами. </p><p>Примерно в то же время, когда свои работы опубликовала группа Ду, на главной в этой области конференции NeurIPS 2018 представил работу своей <a href="https://arxiv.org/abs/1806.07572">группы</a> молодой швейцарский аспирант — <a href="https://people.epfl.ch/arthur.jacot?lang=en">Артур Жако</a>. Статьи отличались в деталях, но суть заключалась в следующем: </p><blockquote><p>Бесконечные глубокие нейронные сети, веса которых инициализируются с учётом определённых статистических свойств, в точности эквивалентны ядрам не только при инициализации, но и на протяжении всего обучения. </p></blockquote><p>Ключевое предположение о весах состоит в том, что они очень мало изменяются по отдельности в процессе обучения, хотя чистый эффект от бесконечного числа малых изменений оказывается значительным. </p><p>С учётом таких предположений, Жако и его коллеги показали, что бесконечная глубокая нейронная сеть всегда эквивалентна ядру, которое в процессе обучения никогда не меняется и даже не зависит от обучающих данных. </p><p>Функция ядра зависит только от архитектуры нейронной сети, такой как её глубина и тип связности. Собственное ядро на основании некоторых геометрических свойств команда назвала нейронным касательным ядром.</p><blockquote><p>«Мы знаем, что по крайней мере иногда нейронные сети могут вести себя как ядерные методы, — говорит Жако. — Это первый шаг к тому, чтобы попытаться сравнить методы &lt;...>».</p></blockquote><h3>Переход ко всем ИНС</h3><p>Вот самое важное в этом результате: он объясняет, почему глубокие нейронные сети сходятся к решению, по крайней мере в идеализированном сценарии. </p><p>Если рассматривать ИНС с точки зрения параметров и сложного ландшафта потерь, то эту сходимость трудно доказать математически. </p><p>Идеализированная глубокая сеть эквивалентна ядерному методу, поэтому мы можем использовать обучающие данные для обучения либо глубокой сети, либо ядерного метода. Оба решения найдут почти оптимальную функцию отображения входных данных на выходные.</p><blockquote><p>«Если понять, что происходит с ядерными методами, я думаю, это даст ключ к открытию волшебной шкатулки [нейронных сетей]». Михаил Белкин, Калифорнийский университет, Сан-Диего.</p></blockquote><p>Во время обучения эволюция функции, представленной бесконечной нейронной сетью, совпадает с эволюцией функции, представленной ядерным методом. Если смотреть на это в пространстве функций, то нейронная сеть и эквивалентный ей ядерный метод катятся по простому, чашеобразному ландшафту в некотором сверх размерном пространстве. </p><p>Легко доказать, что градиентный спуск приведёт к глобальному минимуму, на дно чаши. По крайней мере в случае <a href="https://arxiv.org/abs/1810.02054">этого идеализированного сценария</a> «можно доказать глобальную конвергенцию, — рассказал Ду. — Вот почему люди из сообщества теории обучения очень взволнованы».</p><p>Не все убеждены, что эквивалентность ядер и нейросетей будет справедлива для применяемых на практике сете с конечным числом нейронов, параметры которых в процессе обучения могут резко меняться. </p><blockquote><p>«Я думаю, что есть ещё части головоломки, которые предстоит соединить», — сказал Чжан. </p></blockquote><p>Есть и психологический аспект: нейронные сети обладают некой таинственностью, и сведение их к ядерным методам вызывает у Чжана разочарование. </p><blockquote><p>«Отчасти я надеюсь, что эквивалентность — не ответ на загадку [успеха нейросетей], потому что тогда работает старая теория, а значит, загадка не так интересна».</p></blockquote><p>Но другие учёные взволнованы. Белкин считает, что, даже если ядерные методы — это старая теория, они до сих пор не понятны до конца. Его команда эмпирически <a href="https://arxiv.org/abs/1802.01396">продемонстрировала</a>, что ядерные методы не переобучаются и хорошо обобщают данные тестового набора без необходимости регуляризации, подобно нейронным сетям и вопреки тому, чего мы ожидаем от традиционной теории обучения. </p><blockquote><p>«Если мы поймём, что происходит с ядерными методами, я думаю, это действительно даст нам ключ к волшебной шкатулке [нейронных сетей]», — сказал Белкин.</p></blockquote><p>Дело не только в том, что исследователи лучше понимают математику ядер и это облегчает работу с ними в изучении нейронных сетей. С ядрами легче работать эмпирически. Они намного проще, не требуют случайной инициализации параметров, а их производительность более воспроизводима. </p><p>Исследователи начали выяснять связи между реальными работающими сетями и ядрами. Учёные рады видеть, насколько далеко может зайти это новое видение.</p><blockquote><p>«Если мы устанавливаем абсолютную, полную эквивалентность, я думаю, это может изменить всё», — утверждает Белкин.</p></blockquote><p>Пока учёные упрощают работу с нейросетями, вы можете обратить внимание на наши курсы, чтобы научиться с их помощью решать проблемы бизнеса и изменить карьеру:</p><ul><li><p><a href="https://skillfactory.ru/data-scientist-pro?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=data-science_dspr_181021&amp;utm_term=conc">Профессия Data Scientist (24 месяца)</a></p></li><li><p><a href="https://skillfactory.ru/machine-learning-i-deep-learning?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=data-science_mldl_181021&amp;utm_term=conc">Курс «Machine Learning и Deep Learning» (6 месяцев)</a></p></li></ul><p>Также вы можете перейти на страницы <a href="https://skillfactory.ru/catalogue?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=sf_allcourses_181021&amp;utm_term=conc">из каталога</a>, чтобы узнать, как мы готовим специалистов в других направлениях.</p><figure class="full-width "><img src="/img/image-loader.svg" height="200" data-src="https://habrastorage.org/getpro/habr/upload_files/e01/b89/876/e01b89876f6a2e7e499e544a77e72bba.png" data-width="1000"/><figcaption></figcaption></figure><details class="spoiler"><summary>Профессии и курсы</summary><div class="spoiler__content"><p><strong>Data Science и Machine Learning</strong></p><ul><li><p><a href="https://skillfactory.ru/data-scientist-pro?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=data-science_dspr_181021&amp;utm_term=cat">Профессия Data Scientist</a></p></li><li><p><a href="https://skillfactory.ru/data-analyst-pro?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=analytics_dapr_181021&amp;utm_term=cat">Профессия Data Analyst</a></p></li><li><p><a href="https://skillfactory.ru/matematika-dlya-data-science#syllabus?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=data-science_mat_181021&amp;utm_term=cat">Курс «Математика для Data Science»</a></p></li><li><p><a href="https://skillfactory.ru/matematika-i-machine-learning-dlya-data-science?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=data-science_matml_181021&amp;utm_term=cat">Курс «Математика и Machine Learning для Data Science»</a></p></li><li><p><a href="https://skillfactory.ru/data-engineer?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=data-science_dea_181021&amp;utm_term=cat">Курс по Data Engineering</a></p></li><li><p><a href="https://skillfactory.ru/machine-learning-i-deep-learning?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=data-science_mldl_181021&amp;utm_term=cat">Курс «Machine Learning и Deep Learning»</a></p></li><li><p><a href="https://skillfactory.ru/machine-learning?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=data-science_ml_181021&amp;utm_term=cat">Курс по Machine Learning</a></p></li></ul><p><strong>Python, веб-разработка</strong></p><ul><li><p><a href="https://skillfactory.ru/python-fullstack-web-developer?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_fpw_181021&amp;utm_term=cat">Профессия Fullstack-разработчик на Python</a></p></li><li><p><a href="https://skillfactory.ru/python-for-web-developers?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_pws_181021&amp;utm_term=cat">Курс «Python для веб-разработки»</a></p></li><li><p><a href="https://skillfactory.ru/frontend-razrabotchik?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_fr_181021&amp;utm_term=cat">Профессия Frontend-разработчик</a></p></li><li><p><a href="https://skillfactory.ru/webdev?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_webdev_181021&amp;utm_term=cat">Профессия Веб-разработчик</a></p></li></ul><p><strong>Мобильная разработка</strong></p><ul><li><p><a href="https://skillfactory.ru/ios-razrabotchik-s-nulya?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_ios_181021&amp;utm_term=cat">Профессия iOS-разработчик</a></p></li><li><p><a href="https://skillfactory.ru/android-razrabotchik?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_andr_181021&amp;utm_term=cat">Профессия Android-разработчик</a></p></li></ul><p><strong>Java и C#</strong></p><ul><li><p><a href="https://skillfactory.ru/java-razrabotchik?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_java_181021&amp;utm_term=cat">Профессия Java-разработчик</a></p></li><li><p><a href="https://skillfactory.ru/java-qa-engineer-testirovshik-po?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_qaja_181021&amp;utm_term=cat">Профессия QA-инженер на JAVA</a></p></li><li><p><a href="https://skillfactory.ru/c-sharp-razrabotchik?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_cdev_181021&amp;utm_term=cat">Профессия C#-разработчик</a></p></li><li><p><a href="https://skillfactory.ru/game-razrabotchik-na-unity-i-c-sharp?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_gamedev_181021&amp;utm_term=cat">Профессия Разработчик игр на Unity</a></p></li></ul><p><strong>От основ — в глубину</strong></p><ul><li><p><a href="https://skillfactory.ru/algoritmy-i-struktury-dannyh?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_algo_181021&amp;utm_term=cat">Курс «Алгоритмы и структуры данных»</a></p></li><li><p><a href="https://skillfactory.ru/c-plus-plus-razrabotchik?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_cplus_181021&amp;utm_term=cat">Профессия C++ разработчик</a></p></li><li><p><a href="https://skillfactory.ru/cyber-security-etichnij-haker?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_hacker_181021&amp;utm_term=cat">Профессия Этичный хакер</a></p></li></ul><p><strong>А также:</strong></p><ul><li><p><a href="https://skillfactory.ru/devops-ingineer?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_devops_181021&amp;utm_term=cat">Курс по DevOps</a></p></li></ul></div></details></div></div> <!----> <!----></div> <div class="tm-article-presenter__meta"><div class="tm-separated-list tm-article-presenter__meta-list"><span class="tm-separated-list__title">Теги:</span> <ul class="tm-separated-list__list"><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bskillfactory%5D" class="tm-tags-list__link">skillfactory</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B0%5D" class="tm-tags-list__link">математика</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D0%B3%D0%B0%D1%83%D1%81%D1%81%5D" class="tm-tags-list__link">гаусс</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D1%8F%D0%B4%D1%80%D0%BE%5D" class="tm-tags-list__link">ядро</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%B0%20%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85%20%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2%5D" class="tm-tags-list__link">машина опорных векторов</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D0%BE%D0%B1%D0%BE%D0%B1%D1%89%D0%B5%D0%BD%D0%B8%D1%8F%5D" class="tm-tags-list__link">обобщения</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D0%B1%D0%B5%D1%81%D0%BA%D0%BE%D0%BD%D0%B5%D1%87%D0%BD%D0%BE%D1%81%D1%82%D1%8C%5D" class="tm-tags-list__link">бесконечность</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D0%B8%D0%BD%D1%81%5D" class="tm-tags-list__link">инс</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D1%81%D0%B5%D1%82%D0%B8%5D" class="tm-tags-list__link">нейросети</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D1%83%D0%BF%D1%80%D0%BE%D1%89%D0%B5%D0%BD%D0%B8%D0%B5%5D" class="tm-tags-list__link">упрощение</a></li></ul></div> <div class="tm-separated-list tm-article-presenter__meta-list"><span class="tm-separated-list__title">Хабы:</span> <ul class="tm-separated-list__list"><li class="tm-separated-list__item"><a href="/ru/company/skillfactory/blog/" class="tm-hubs-list__link router-link-active">
    Блог компании SkillFactory
  </a></li><li class="tm-separated-list__item"><a href="/ru/hub/maths/" class="tm-hubs-list__link">
    Математика
  </a></li><li class="tm-separated-list__item"><a href="/ru/hub/read/" class="tm-hubs-list__link">
    Читальный зал
  </a></li><li class="tm-separated-list__item"><a href="/ru/hub/popular_science/" class="tm-hubs-list__link">
    Научно-популярное
  </a></li><li class="tm-separated-list__item"><a href="/ru/hub/artificial_intelligence/" class="tm-hubs-list__link">
    Искусственный интеллект
  </a></li></ul></div></div></article></div> <!----></div> <div class="tm-article-sticky-panel"><div class="tm-data-icons tm-article-sticky-panel__icons"><div class="tm-article-rating tm-data-icons__item"><div class="tm-votes-meter tm-article-rating__votes-switcher"><svg height="16" width="16" class="tm-svg-img tm-votes-meter__icon tm-votes-meter__icon_medium"><title>Всего голосов 37: ↑29 и ↓8</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-rating"></use></svg> <span title="Всего голосов 37: ↑29 и ↓8" class="tm-votes-meter__value tm-votes-meter__value_positive tm-votes-meter__value_medium">+21</span></div> <DIV class="v-portal" style="display:none;"></DIV></div> <!----> <span title="Количество просмотров" class="tm-icon-counter tm-data-icons__item"><svg height="16" width="16" class="tm-svg-img tm-icon-counter__icon"><title>Просмотры</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-views"></use></svg> <span class="tm-icon-counter__value">15K</span></span> <button title="Добавить в закладки" type="button" class="bookmarks-button tm-data-icons__item"><span title="Добавить в закладки" class="tm-svg-icon__wrapper bookmarks-button__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Добавить в закладки</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-favorite"></use></svg></span> <span title="Количество пользователей, добавивших публикацию в закладки" class="bookmarks-button__counter">
    70
  </span></button> <!----> <div title="Поделиться" class="tm-sharing tm-data-icons__item"><button type="button" class="tm-sharing__button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="tm-sharing__icon"><path fill="currentColor" d="M10.33.275l9.047 7.572a.2.2 0 010 .306l-9.048 7.572a.2.2 0 01-.328-.153V11c-8 0-9.94 6-9.94 6S-1 5 10 5V.428a.2.2 0 01.328-.153z"></path></svg></button> <!----></div> <DIV class="v-portal" style="display:none;"></DIV></div> </div></div> <!----> <!----> <div class="tm-article-presenter__footer"><div class="tm-article-blocks"><!----> <section class="tm-block tm-block_spacing-bottom"><!----> <div class="tm-block__body tm-block__body_variant-balanced"><div class="tm-article-author"><div class="tm-article-author__company"><div class="tm-article-author__company-card"><div class="tm-company-snippet"><a href="/ru/company/skillfactory/profile/" class="tm-company-snippet__logo-link"><div class="tm-entity-image"><img alt="" height="40" src="//habrastorage.org/getpro/habr/company/9bb/5f7/d50/9bb5f7d50871bf0981f64b1970f22fe1.png" width="40" class="tm-entity-image__pic"></div></a> <div class="tm-company-snippet__info"><a href="/ru/company/skillfactory/profile/" class="tm-company-snippet__title">SkillFactory</a> <div class="tm-company-snippet__description">Школа Computer Science. Скидка 45% по коду HABR</div></div></div> <div class="tm-article-author__buttons"><!----> <!----></div></div> <div class="tm-article-author__company-contacts"><a href="http://www.skillfactory.ru/" rel="noopener" target="_blank" class="tm-article-author__contact">
      Сайт
    </a></div> <div class="tm-article-author__separator"></div></div> <div class="tm-user-card tm-article-author__user-card tm-user-card_variant-article"><div class="tm-user-card__info-container"><div class="tm-user-card__header"><div class="tm-user-card__header-data"><a href="/ru/users/honyaki/" class="tm-user-card__userpic tm-user-card__userpic_size-40"><div class="tm-entity-image"><img alt="" src="//habrastorage.org/getpro/habr/avatars/96e/81d/bc0/96e81dbc01247c6c855b1806b355efa3.jpg" class="tm-entity-image__pic"></div></a> <div class="tm-user-card__meta"><div title=" 106 голосов " class="tm-karma tm-user-card__karma"><div class="tm-karma__votes tm-karma__votes_positive">
    20
  </div> <div class="tm-karma__text">
    Карма
  </div></div> <div title="Рейтинг пользователя" class="tm-rating tm-user-card__rating"><div class="tm-rating__header"> <div class="tm-rating__counter">22.4</div></div> <div class="tm-rating__text">
    Рейтинг
  </div></div></div></div></div> <div class="tm-user-card__info tm-user-card__info_variant-article"><div class="tm-user-card__title tm-user-card__title_variant-article"><!----> <a href="/ru/users/honyaki/" class="tm-user-card__nickname tm-user-card__nickname_variant-article">
          @honyaki
        </a> <!----></div> <p class="tm-user-card__short-info tm-user-card__short-info_variant-article">Пользователь</p></div></div> <div class="tm-user-card__buttons tm-user-card__buttons_variant-article"><!----> <!----> <!----> <!----> <!----></div></div> <!----></div> <DIV class="v-portal" style="display:none;"></DIV></div> <!----></section> <div class="tm-article-blocks__comments"><div class="tm-article-page-comments"><div class="tm-article-comments-counter-link tm-article-comments-counter-button"><a href="/ru/company/skillfactory/blog/584170/comments/" class="tm-article-comments-counter-link__link tm-article-comments-counter-link__link_button-style"><svg height="16" width="16" class="tm-svg-img tm-article-comments-counter-link__icon tm-article-comments-counter-link__icon_contrasted"><title>Комментарии</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-comments"></use></svg> <span class="tm-article-comments-counter-link__value tm-article-comments-counter-link__value_contrasted">
       Комментарии 6 
    </span></a> <!----></div></div></div>  <!---->  <!----> <!----></div></div></div></div></div> <div class="tm-page__sidebar"><div class="tm-layout-sidebar"><div class="tm-layout-sidebar__placeholder_initial"></div> <div class="tm-sexy-sidebar tm-sexy-sidebar_initial" style="margin-top:0px;"><!----> <section class="tm-block tm-block_spacing-bottom"><header class="tm-block__header"><h2 class="tm-block__title">Информация</h2> <!----></header> <div class="tm-block__body"><div class="tm-company-basic-info"><dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Дата основания</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap"><time datetime="2012-10-31T20:00:00.000Z" title="2012-11-01, 00:00">1  ноября  2012</time></dd></dl> <dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Местоположение</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap">
    Россия
  </dd></dl> <dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Сайт</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap"><a href="http://www.skillfactory.ru/" target="_blank" class="tm-company-basic-info__link">
      www.skillfactory.ru
    </a></dd></dl> <dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Численность</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap">
    201–500 человек
  </dd></dl> <dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Дата регистрации</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap"><time datetime="2012-12-14T13:25:37.000Z" title="2012-12-14, 17:25">14  декабря  2012</time></dd></dl> <dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Представитель</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap"><a href="/ru/users/skillfactory_school/" class="tm-company-basic-info__link">
      Skillfactory School
    </a></dd></dl></div></div> <!----></section> <div class="tm-company-widgets"></div> <!----></div></div></div></div></div></div></main> <!----></div> <div class="tm-footer-menu"><div class="tm-page-width"><div class="tm-footer-menu__container"><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Ваш аккаунт
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/kek/v1/auth/habrahabr/?back=/ru/company/skillfactory/blog/584170/&amp;hl=ru" rel="nofollow" target="_self">
                Войти
              </a></li><li class="tm-footer-menu__list-item"><a href="/kek/v1/auth/habrahabr-register/?back=/ru/company/skillfactory/blog/584170/&amp;hl=ru" rel="nofollow" target="_self">
                Регистрация
              </a></li></ul></div></div><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Разделы
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/ru/" class="footer-menu__item-link router-link-active">
                Публикации
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/news/" class="footer-menu__item-link">
                Новости
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/hubs/" class="footer-menu__item-link">
                Хабы
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/companies/" class="footer-menu__item-link">
                Компании
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/users/" class="footer-menu__item-link">
                Авторы
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/sandbox/" class="footer-menu__item-link">
                Песочница
              </a></li></ul></div></div><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Информация
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/ru/docs/help/" class="footer-menu__item-link">
                Устройство сайта
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/authors/codex/" class="footer-menu__item-link">
                Для авторов
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/companies/corpblogs/" class="footer-menu__item-link">
                Для компаний
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/docs/transparency/" class="footer-menu__item-link">
                Документы
              </a></li><li class="tm-footer-menu__list-item"><a href="https://account.habr.com/info/agreement" target="_blank">
                Соглашение
              </a></li><li class="tm-footer-menu__list-item"><a href="https://account.habr.com/info/confidential/" target="_blank">
                Конфиденциальность
              </a></li></ul></div></div><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Услуги
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="https://docs.google.com/presentation/d/e/2PACX-1vQLwRfQmXibiUlWaRg-BAc38s7oM3lJiaPju7qmdJsp8ysIvZ_G-Npem0njJLMozE2bPHMpDqiI5hhy/pub?start=false&amp;loop=false&amp;delayms=60000&amp;slide=id.g91a03369cd_4_297" target="_blank">
                Реклама
              </a></li><li class="tm-footer-menu__list-item"><a href="https://habrastorage.org/storage/stuff/habr/service_price.pdf" target="_blank">
                Тарифы
              </a></li><li class="tm-footer-menu__list-item"><a href="https://docs.google.com/presentation/d/e/2PACX-1vQJJds8-Di7BQSP_guHxICN7woVYoN5NP_22ra-BIo4bqnTT9FR6fB-Ku2P0AoRpX0Ds-LRkDeAoD8F/pub?start=false&amp;loop=false&amp;delayms=60000" target="_blank">
                Контент
              </a></li><li class="tm-footer-menu__list-item"><a href="https://tmtm.timepad.ru/" target="_blank">
                Семинары
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/megaprojects/" class="footer-menu__item-link">
                Мегапроекты
              </a></li></ul></div></div></div></div></div> <div class="tm-footer"><div class="tm-page-width"><div class="tm-footer__container"><!----> <div class="tm-footer__social"><a href="https://www.facebook.com/habrahabr.ru" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Facebook</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-facebook"></use></svg></a><a href="https://twitter.com/habr_com" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Twitter</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-twitter"></use></svg></a><a href="https://vk.com/habr" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>VK</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-vkontakte"></use></svg></a><a href="https://telegram.me/habr_com" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Telegram</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-telegram"></use></svg></a><a href="https://www.youtube.com/channel/UCd_sTwKqVrweTt4oAKY5y4w" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Youtube</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-youtube"></use></svg></a><a href="https://zen.yandex.ru/habr" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Яндекс Дзен</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-zen"></use></svg></a></div> <DIV class="v-portal" style="display:none;"></DIV> <button class="tm-footer__link"><!---->
        Настройка языка
      </button> <a href="/ru/about" class="tm-footer__link">
        О сайте
      </a> <a href="/ru/feedback/" class="tm-footer__link">
        Техническая поддержка
      </a> <!----> <a href="/berserk-mode-nope" class="tm-footer__link">
        Вернуться на старую версию
      </a> <div class="tm-footer-copyright"><span class="tm-copyright"><span class="tm-copyright__years">© 2006–2021 </span> <span class="tm-copyright__name">«<a href="https://company.habr.com/" rel="noopener" target="_blank" class="tm-copyright__link">Habr</a>»</span></span></div></div></div></div> <!----> <!----></div> <div class="vue-portal-target"></div></div>
<script>window.__INITIAL_STATE__={"adblock":{"hasAcceptableAdsFilter":false,"hasAdblock":false},"articlesList":{"articlesList":{"584170":{"id":"584170","timePublished":"2021-10-18T19:06:26+00:00","isCorporative":true,"lang":"ru","titleHtml":"Нейросети могут оказаться проще, чем принято считать","leadData":{"textHtml":"\u003Cp\u003EНейросети отчасти будто подрывают традиционную теорию машинного обучения, которая сильно опирается на идеи теории вероятности и статистики. В чём же заключается загадка их успеха?\u003C\u002Fp\u003E\u003Cp\u003EИсследователи показывают, что сети с бесконечным числом нейронов математически эквивалентны более простым моделям машинного обучения — ядерным методам. Поразительные результаты можно объяснить, если эта эквивалентность простирается дальше «идеальных» нейронных сетей. Подробности рассказываем к старту нашего флагманского \u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fdata-scientist-pro?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=data-science_dspr_181021&amp;utm_term=lead\"\u003Eкурса по Data Science\u003C\u002Fa\u003E.\u003C\u002Fp\u003E","imageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F775\u002Fa09\u002F173\u002F775a091730993f28871abea6cc2ca62e.jpg","buttonTextHtml":"Читать далее","image":{"url":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F775\u002Fa09\u002F173\u002F775a091730993f28871abea6cc2ca62e.jpg","fit":"cover","positionY":0,"positionX":0}},"editorVersion":"2.0","postType":"article","postLabels":[{"type":"translation","data":{"originalAuthorName":"Anil Ananthaswamy","originalUrl":"https:\u002F\u002Fwww.quantamagazine.org\u002Fa-new-link-to-an-old-model-could-crack-the-mystery-of-deep-learning-20211011\u002F"}}],"author":{"scoreStats":{"score":20,"votesCount":106},"rating":22.4,"relatedData":null,"contacts":[],"authorContacts":[],"paymentDetails":{"paymentYandexMoney":null,"paymentPayPalMe":null,"paymentWebmoney":null},"id":"2534597","alias":"honyaki","fullname":null,"avatarUrl":"\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Favatars\u002F96e\u002F81d\u002Fbc0\u002F96e81dbc01247c6c855b1806b355efa3.jpg","speciality":null},"statistics":{"commentsCount":6,"favoritesCount":70,"readingCount":15336,"score":21,"votesCount":37},"hubs":[{"relatedData":null,"id":"17931","alias":"skillfactory","type":"corporative","title":"Блог компании SkillFactory","titleHtml":"Блог компании SkillFactory","isProfiled":false},{"relatedData":null,"id":"17812","alias":"maths","type":"collective","title":"Математика","titleHtml":"Математика","isProfiled":true},{"relatedData":null,"id":"20742","alias":"read","type":"collective","title":"Читальный зал","titleHtml":"Читальный зал","isProfiled":false},{"relatedData":null,"id":"21910","alias":"popular_science","type":"collective","title":"Научно-популярное","titleHtml":"Научно-популярное","isProfiled":false},{"relatedData":null,"id":"21922","alias":"artificial_intelligence","type":"collective","title":"Искусственный интеллект","titleHtml":"Искусственный интеллект","isProfiled":false}],"flows":[{"id":"1","alias":"develop","title":"Разработка"},{"id":"3","alias":"management","title":"Менеджмент"},{"id":"7","alias":"popsci","title":"Научпоп"}],"relatedData":null,"textHtml":"\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\"\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F8a6\u002F371\u002F45e\u002F8a637145e56ff345a069bf048d528c13.jpg\" width=\"780\" height=\"440\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F8a6\u002F371\u002F45e\u002F8a637145e56ff345a069bf048d528c13.jpg\" data-blurred=\"true\"\u002F\u003E\u003Cfigcaption\u003E\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EНейросети отчасти будто подрывают традиционную теорию машинного обучения, которая сильно опирается на идеи теории вероятности и статистики. В чём же заключается загадка их успеха?\u003C\u002Fp\u003E\u003Cp\u003EИсследователи показывают, что сети с бесконечным числом нейронов математически эквивалентны более простым моделям машинного обучения — ядерным методам. Поразительные результаты можно объяснить, если эта эквивалентность простирается дальше «идеальных» нейронных сетей. Подробности рассказываем к старту нашего флагманского \u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fdata-scientist-pro?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=data-science_dspr_181021&amp;utm_term=lead\"\u003Eкурса по Data Science\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Cp\u003EОбычно считается, что модели ML работают лучше, когда имеют подходящее число параметров. Если параметров слишком мало, модель может оказаться слишком простой и не улавливать все нюансы. Слишком много параметров — и модель усложняется, изучая настолько тонкие детали, что затем не может обобщать. Это и называется переобучением. \u003C\u002Fp\u003E\u003Cblockquote\u003E\u003Cp\u003E«Это баланс между тем, чтобы слишком хорошо обучиться на данных, и тем, чтобы не обучиться вообще. Хочется быть посередине», — рассказывает взволнованный новыми перспективами \u003Ca href=\"https:\u002F\u002Fdatascience.ucsd.edu\u002Fabout\u002Four-team\u002Fname\u002Fmikhail-belkin\u002F\"\u003EМихаил Белкин\u003C\u002Fa\u003E, исследователь машинного обучения из Калифорнийского университета в Сан-Диего.\u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Cp\u003EГлубокие нейронные сети, например VGG, по общему мнению, имеют слишком много параметров, а значит, их прогнозы должны страдать от переобучения. Но этого не происходит. Напротив,  такие сети обобщают новые данные с удивительным успехом. Почему? Ответа на этот вопрос не знал никто, хотя выяснить причину пытались. \u003C\u002Fp\u003E\u003Cp\u003EСпециалист в компьютерных науках и нейробиолог из Еврейского университета Иерусалима, Нафтали Тишби, утверждал, что глубокие нейронные сети сначала обучаются на данных, а затем \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1503.02406\"\u003Eпроходят\u003C\u002Fa\u003E через информационное бутылочное горлышко, отбрасывая нерелевантную информацию. Это и \u003Ca href=\"https:\u002F\u002Fwww.quantamagazine.org\u002Fnew-theory-cracks-open-the-black-box-of-deep-learning-20170921\u002F\"\u003Eпомогает \u003C\u002Fa\u003Eв обобщении. Другие учёные\u003Ca href=\"https:\u002F\u002Fopenreview.net\u002Fpdf?id=ry_WPG-A-\"\u003E считают\u003C\u002Fa\u003E, что это происходит не во всех сетях.\u003C\u002Fp\u003E\u003Cp\u003EМатематическая эквивалентность ядерных методов и идеализированных нейронных сетей даёт ключ к тому, почему и как сети с огромным числом параметров приходят к своим решениям.\u003C\u002Fp\u003E\u003Cp\u003EЯдерные методы — это алгоритмы, которые находят закономерности в данных через их проекцию на очень высокие измерения. Изучая более понятные эквиваленты ядер идеализированных нейронных сетей, исследователи могут узнать, почему сложные глубокие сети сходятся в процессе обучения к решениям, которые хорошо обобщаются на новые данные.\u003C\u002Fp\u003E\u003Cblockquote\u003E\u003Cp\u003E«Нейронная сеть немного похожа на машину Руба Голдберга. Неизвестно, что в этой машине по-настоящему важно, — утверждает Белкин. — Ядерные методы не настолько сложны. Думаю, что упрощение нейронных сетей до ядерных методов позволяет выделить движущую силу происходящего».\u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Ch3\u003EНемного истории\u003C\u002Fh3\u003E\u003Cp\u003EЯдерные методы полагаются на область математики с долгой историей. История эта восходит к немецкому математику XIX века Карлу Гауссу. Именно он придумал ядро, названное в его честь — гауссовым. Такое ядро отображает переменную \u003Cem\u003EX\u003C\u002Fem\u003E на знаменитую функцию распределения в форме колокола. \u003C\u002Fp\u003E\u003Cp\u003EВ начале XX века с помощью ядер английский математик Джеймс Мерсер решил интегральные уравнения, после этого работать с ядрами стали намного чаще, а к 1960 году их применили в машинном обучении, чтобы выполнять сложные классификации.\u003C\u002Fp\u003E\u003Cp\u003EПонимание методов ядра требуется уже в линейных классификаторах. Что такое линейный классификатор? Допустим, кошек и собак можно разделить на две группы при помощи только двух признаков, (иначе говоря — данных в двух измерениях). \u003C\u002Fp\u003E\u003Cp\u003EЧтобы отличить животных друг от друга, вам нужны две функции (к примеру, размер морды, который мы можем откладывать на оси \u003Cem\u003Ex\u003C\u002Fem\u003E, и размер ушей, — на оси \u003Cem\u003Ey\u003C\u002Fem\u003E. Если отобразить эти данные на плоскость, \u003Cem\u003EXY\u003C\u002Fem\u003E, кошки должны оказаться в одном кластере, а собаки — в другом.\u003C\u002Fp\u003E\u003Ch3\u003EПоиск прямой\u003C\u002Fh3\u003E\u003Cp\u003EЧтобы найти разделяющую кластеры прямую, то есть вычислить коэффициенты уравнения прямой [y = ax + b], можно обучить линейный классификатор на размеченных данных. После этого посмотрите, на какую сторону от линии попадают данные — это и будет прогнозом классификатора.\u003C\u002Fp\u003E\u003Cp\u003EЛюбители собак и кошек могут возмутиться: реальные данные о мордах и ушах многочисленных видов кошек и собак почти наверняка нельзя разделить линейно; но тогда их можно преобразовать или спроецировать в пространство более высокой размерности. \u003C\u002Fp\u003E\u003Cp\u003EПростой способ этого добиться — умножить значение двух признаков, чтобы получить третий. Возможно, есть нечто, что отличает собак от кошек по соотношению размеров морды и ушей.\u003C\u002Fp\u003E\u003Cp\u003EЛинейный разделитель (гиперплоскость) легче найти в обобщённом случае, когда пространство имеет более трёх измерений. Проецируясь обратно в нижние измерения, гиперплоскость принимает форму нелинейной функции с кривыми и волнистыми линиями. Эти линии разделяют исходные данные в нижних измерениях на два кластера. \u003C\u002Fp\u003E\u003Cp\u003EЕсли вы работаете с реальными данными, поиск коэффициентов гиперплоскости в высоких измерениях часто оказывается вычислительно неэффективным, а иногда невозможным. Но это не касается ядерных методов.\u003C\u002Fp\u003E\u003Ch3\u003EЯдерные методы\u003C\u002Fh3\u003E\u003Cp\u003EМощь ядерных методов заключена в способности делать две вещи. Они: \u003C\u002Fp\u003E\u003Col\u003E\u003Cli\u003E\u003Cp\u003EСопоставляют каждую точку в наборе данных низкой размерности с точкой из более высоких измерений. Размерность этого гиперпространства может быть бесконечной в зависимости от отображения. \u003C\u002Fp\u003E\u003Cp\u003EКогда данные проецируются в бесконечное число измерений, нахождение коэффициентов разделяющей гиперплоскости предполагает вычисление внутреннего произведения для каждой пары признаков в высокой размерности становится сложным, когда данные проецируются в бесконечномерное пространство.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EПолучив две точки данных низкой размерности, они используют функцию ядра, чтобы вывести внутреннее произведение соответствующих признаков высокой размерности. \u003C\u002Fp\u003E\u003Cp\u003EОчень важно, что с помощью этого трюка алгоритм может найти коэффициенты гиперплоскости, не касаясь пространства высокой размерности.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cblockquote\u003E\u003Cp\u003E«Замечательная особенность трюка с ядром заключается в том, что все вычисления происходят в пространстве низкой размерности, а не в пространстве, где оно может иметь бесконечное число измерений», — рассказывает \u003Ca href=\"https:\u002F\u002Fwww2.eecs.berkeley.edu\u002FFaculty\u002FHomepages\u002Fboser.html\"\u003EБернхард Бозер\u003C\u002Fa\u003E, заслуженный профессор Калифорнийского университета в Беркли.\u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Cp\u003EВ конце 1980-х — начале 1990-x Бозер с коллегами \u003Ca href=\"https:\u002F\u002Fguyon.chalearn.org\u002F\"\u003EИзабель Гийон\u003C\u002Fa\u003E и \u003Ca href=\"https:\u002F\u002Fdatascience.columbia.edu\u002Fpeople\u002Fvladimir-vapnik\u002F\"\u003EВладимиром Вапником\u003C\u002Fa\u003E изобрёл класс ядерных методов — методы опорных векторов\u003Ca href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F130385.130401\"\u003E (SVM\u003C\u002Fa\u003E). Хотя ядерные методы различных типов появились в машинном обучении в 1960-х, именно с изобретением SVM они заняли центральное место. \u003C\u002Fp\u003E\u003Cp\u003ESVM оказались невероятно мощными. В начале 2000-х они искали сходства различных белковых последовательностей и предсказывали функции белков, работали в машинном зрении и распознавании рукописного текста.\u003C\u002Fp\u003E\u003Cp\u003EМетоды опорных векторов доминировали в ML, пока с появлением AlexNet не настало время глубоких нейронных сетей. Сообщество машинного обучения переключилось на ИНС и SVM оказались в затруднительном положении, однако ядерные методы остаются мощными моделями. Они способны на большее, чем трюк с ядром.\u003C\u002Fp\u003E\u003Cblockquote\u003E\u003Cp\u003E«Если у вас есть мощное ядро, то вы отображаете данные в пространство этого ядра, как бы бесконечномерное и очень мощное, — рассказывает \u003Ca href=\"https:\u002F\u002Fpluskid.org\u002F\"\u003EЧиюань Чжан\u003C\u002Fa\u003E, научный сотрудник группы по изучению мозга в Google Research. — Вы всегда можете найти линейный разделитель в этом мощном скрытом пространстве, разделяющем данные, а возможных решений бесконечно много». \u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Cp\u003EОграничивая пространство поиска решений, теория ядер позволяет выбрать не просто произвольный линейный разделитель, а в определённом смысле лучший из возможных. Это сродни регуляризации — уменьшению количества параметров в модели, чтобы предотвратить переобучение. Чжан задался вопросом, могут ли глубокие нейронные сети делать что-то подобное.\u003C\u002Fp\u003E\u003Cp\u003EГлубокие нейронные сети имеют входной слой, выходной слой и по крайней мере один скрытый слой между ними. Чем больше слоёв, тем глубже сеть. Параметры сети представляют собой силу связей между нейронами. \u003C\u002Fp\u003E\u003Cp\u003EОбучение сети для распознавания изображений — это предъявление сети ранее классифицированных изображений и определение значений её параметров, которые помогают правильно характеризовать изображения. После обучения ИНС представляет собой модель преобразования поданного изображения в выходной сигнал, то есть в метку или категорию.\u003C\u002Fp\u003E\u003Cp\u003EЧтобы выяснить, уменьшают ли нейросети количество настраиваемых параметров, тем самым неявно выполняя регуляризацию, в 2017 году Чжан и его коллеги провели серию \u003Ca href=\"https:\u002F\u002Fopenreview.net\u002Fpdf?id=Sy8gdB9xx\"\u003Eэмпирических тестов\u003C\u002Fa\u003E на AlexNet и VGG. Другими словами учёные выясняли, влияет ли режим обучения на саму возможность переобучения. Команда обнаружила, что этого не происходит. \u003C\u002Fp\u003E\u003Cp\u003EОказалось, что AlexNet и другие подобные ИНС действительно могут переобучиться и не обобщать данные. Но когда тем же сетям, обученным по тому же алгоритму, давали данные без изменений, они не переобучались. Напротив, они хорошо обобщали данные, когда им давали данные без изменений. \u003C\u002Fp\u003E\u003Cp\u003EТакая неявная регуляризация не может быть ответом на загадку, а вывод требует «лучшего объяснения, чтобы характеризовать обобщение в глубоких нейронных сетях», — рассказывает Чжан.\u003C\u002Fp\u003E\u003Ch3\u003EБесконечные нейроны\u003C\u002Fh3\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1412.6614\"\u003EИсследования\u003C\u002Fa\u003E показывали, что нейронные сети с бо́льшим количеством нейронов в одном слое в прогнозах так же сильны или \u003Ca href=\"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=V8iZOpY28_E&amp;t=223s\"\u003Eпревосходят\u003C\u002Fa\u003E свои аналоги с меньшим числом нейронов на один слой. \u003C\u002Fp\u003E\u003Cp\u003EНекоторые учёные восприняли это как намёк: возможно, ИНС можно понять при помощи стратегии из физики, где «изучение предельных случаев иногда может упростить проблему», — сказал \u003Ca href=\"https:\u002F\u002Fyasamanb.github.io\u002F\"\u003EЯсаман Бахри\u003C\u002Fa\u003E, учёный-исследователь из команды изучения мозга Google Research. \u003C\u002Fp\u003E\u003Cp\u003EФизики часто упрощают задачу, рассматривая крайние случаи. Что происходит, когда, например, число частиц в системе стремится к бесконечности? «В таких границах статистические эффекты могут стать проще», — рассказывает Бахри. \u003C\u002Fp\u003E\u003Cblockquote\u003E\u003Cp\u003EЧто произойдёт с нейронной сетью с математической точки зрения при бесконечном количестве слоёв?\u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Cp\u003EВ 1994 году почётный профессор Университета Торонто Рэдфорд Нил показал, что если веса этой сети заданы (инициализированы) с определёнными статистическими свойствами, то до обучения сеть \u003Ca href=\"https:\u002F\u002Fwww.cs.toronto.edu\u002F~radford\u002Fftp\u002Fpin.pdf\"\u003Eматематически эквивалентна\u003C\u002Fa\u003E знаменитой ядерной функции, которая известна как гауссовский процесс. То же самое справедливо и для идеализированных глубоких нейронных сетей с бесконечным числом нейронов, где скрытых слоёв больше. Этот факт в 2017 году \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1711.00165\"\u003Eпоказали\u003C\u002Fa\u003E две группы учёных, включая группу Бахри.\u003C\u002Fp\u003E\u003Cblockquote\u003E\u003Cp\u003E«Известно, что по крайне мере иногда нейронные сети могут вести себя как ядерные методы». Артур Жако, Швейцарский федеральный технологический институт Лозанны\u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Cp\u003EПоследствия поражают. Обычно аналитическое математическое выражение для прогнозирования новых данных использовать нельзя даже после обучения глубокой сети. \u003C\u002Fp\u003E\u003Cp\u003EЭто что-то вроде чёрного ящика: чтобы увидеть его содержимое, вы должны просто запустить сеть; но в идеализированном сценарии при инициализации сеть эквивалентна гауссовскому процессу. Иными словами, можно обучить ядерный метод, для которого существуют математические выражения, а нейронную сеть просто отбросить.\u003C\u002Fp\u003E\u003Cblockquote\u003E\u003Cp\u003E«Как только вы сопоставите сеть с гауссовским процессом &lt;…\u003E сможете аналитически рассчитать, каким должно быть предсказание», — рассказывает Бахри.\u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Cp\u003EЭто уже знаковый результат, но он математически не описывал то, что происходит во время самой распространённой формы обучения: было неясно, как решение может обобщаться настолько хорошо.\u003C\u002Fp\u003E\u003Ch3\u003EГрадиентный спуск\u003C\u002Fh3\u003E\u003Cp\u003EЧасть загадки связана с процессом обучения глубоких нейросетей, где работает алгоритм \u003Ca href=\"https:\u002F\u002Fwww.quantamagazine.org\u002Fcomputer-scientists-discover-limits-of-major-research-algorithm-20210817\u002F\"\u003Eградиентного спуска\u003C\u002Fa\u003E. Сеть проходит по полному холмов и долин ландшафту высокой размерности, всё ниже спускаясь по ландшафту собственных ошибок, которые модель допустила на предоставленном наборе значений параметров. \u003C\u002Fp\u003E\u003Cp\u003EПосле настройки параметров сеть достигает глобального минимума, то есть максимально приближается к точной классификации обучающих данных. Обучение сети — это, по сути, задача оптимизации, поиска глобального минимума. Обученная сеть — это почти оптимальная функция, которая отображает входные значения на выходные. Процесс обучения тяжело поддаётся анализу.\u003C\u002Fp\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fwww.cs.washington.edu\u002Fpeople\u002Ffaculty\u002Fssdu\"\u003EСимон Ду\u003C\u002Fa\u003E, эксперт по машинному обучению из Университета Вашингтона в Сиэтле, рассказывает:\u003C\u002Fp\u003E\u003Cblockquote\u003E\u003Cp\u003E«Ни одна существующая теория не может гарантировать, что если вы примените какой-либо широко используемый алгоритм, например градиентный спуск, то [ИНС] сможет сойтись к глобальному минимуму». \u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Cp\u003EПонимать причину мы начали к концу 2018 года, когда к возможному ответу пришли несколько групп учёных одновременно. Это было сделано на основании математического анализа бесконечных сетей и анализа их связи с более понятными ядерными методами. \u003C\u002Fp\u003E\u003Cp\u003EПримерно в то же время, когда свои работы опубликовала группа Ду, на главной в этой области конференции NeurIPS 2018 представил работу своей \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1806.07572\"\u003Eгруппы\u003C\u002Fa\u003E молодой швейцарский аспирант — \u003Ca href=\"https:\u002F\u002Fpeople.epfl.ch\u002Farthur.jacot?lang=en\"\u003EАртур Жако\u003C\u002Fa\u003E. Статьи отличались в деталях, но суть заключалась в следующем: \u003C\u002Fp\u003E\u003Cblockquote\u003E\u003Cp\u003EБесконечные глубокие нейронные сети, веса которых инициализируются с учётом определённых статистических свойств, в точности эквивалентны ядрам не только при инициализации, но и на протяжении всего обучения. \u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Cp\u003EКлючевое предположение о весах состоит в том, что они очень мало изменяются по отдельности в процессе обучения, хотя чистый эффект от бесконечного числа малых изменений оказывается значительным. \u003C\u002Fp\u003E\u003Cp\u003EС учётом таких предположений, Жако и его коллеги показали, что бесконечная глубокая нейронная сеть всегда эквивалентна ядру, которое в процессе обучения никогда не меняется и даже не зависит от обучающих данных. \u003C\u002Fp\u003E\u003Cp\u003EФункция ядра зависит только от архитектуры нейронной сети, такой как её глубина и тип связности. Собственное ядро на основании некоторых геометрических свойств команда назвала нейронным касательным ядром.\u003C\u002Fp\u003E\u003Cblockquote\u003E\u003Cp\u003E«Мы знаем, что по крайней мере иногда нейронные сети могут вести себя как ядерные методы, — говорит Жако. — Это первый шаг к тому, чтобы попытаться сравнить методы &lt;...\u003E».\u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Ch3\u003EПереход ко всем ИНС\u003C\u002Fh3\u003E\u003Cp\u003EВот самое важное в этом результате: он объясняет, почему глубокие нейронные сети сходятся к решению, по крайней мере в идеализированном сценарии. \u003C\u002Fp\u003E\u003Cp\u003EЕсли рассматривать ИНС с точки зрения параметров и сложного ландшафта потерь, то эту сходимость трудно доказать математически. \u003C\u002Fp\u003E\u003Cp\u003EИдеализированная глубокая сеть эквивалентна ядерному методу, поэтому мы можем использовать обучающие данные для обучения либо глубокой сети, либо ядерного метода. Оба решения найдут почти оптимальную функцию отображения входных данных на выходные.\u003C\u002Fp\u003E\u003Cblockquote\u003E\u003Cp\u003E«Если понять, что происходит с ядерными методами, я думаю, это даст ключ к открытию волшебной шкатулки [нейронных сетей]». Михаил Белкин, Калифорнийский университет, Сан-Диего.\u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Cp\u003EВо время обучения эволюция функции, представленной бесконечной нейронной сетью, совпадает с эволюцией функции, представленной ядерным методом. Если смотреть на это в пространстве функций, то нейронная сеть и эквивалентный ей ядерный метод катятся по простому, чашеобразному ландшафту в некотором сверх размерном пространстве. \u003C\u002Fp\u003E\u003Cp\u003EЛегко доказать, что градиентный спуск приведёт к глобальному минимуму, на дно чаши. По крайней мере в случае \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1810.02054\"\u003Eэтого идеализированного сценария\u003C\u002Fa\u003E «можно доказать глобальную конвергенцию, — рассказал Ду. — Вот почему люди из сообщества теории обучения очень взволнованы».\u003C\u002Fp\u003E\u003Cp\u003EНе все убеждены, что эквивалентность ядер и нейросетей будет справедлива для применяемых на практике сете с конечным числом нейронов, параметры которых в процессе обучения могут резко меняться. \u003C\u002Fp\u003E\u003Cblockquote\u003E\u003Cp\u003E«Я думаю, что есть ещё части головоломки, которые предстоит соединить», — сказал Чжан. \u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Cp\u003EЕсть и психологический аспект: нейронные сети обладают некой таинственностью, и сведение их к ядерным методам вызывает у Чжана разочарование. \u003C\u002Fp\u003E\u003Cblockquote\u003E\u003Cp\u003E«Отчасти я надеюсь, что эквивалентность — не ответ на загадку [успеха нейросетей], потому что тогда работает старая теория, а значит, загадка не так интересна».\u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Cp\u003EНо другие учёные взволнованы. Белкин считает, что, даже если ядерные методы — это старая теория, они до сих пор не понятны до конца. Его команда эмпирически \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1802.01396\"\u003Eпродемонстрировала\u003C\u002Fa\u003E, что ядерные методы не переобучаются и хорошо обобщают данные тестового набора без необходимости регуляризации, подобно нейронным сетям и вопреки тому, чего мы ожидаем от традиционной теории обучения. \u003C\u002Fp\u003E\u003Cblockquote\u003E\u003Cp\u003E«Если мы поймём, что происходит с ядерными методами, я думаю, это действительно даст нам ключ к волшебной шкатулке [нейронных сетей]», — сказал Белкин.\u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Cp\u003EДело не только в том, что исследователи лучше понимают математику ядер и это облегчает работу с ними в изучении нейронных сетей. С ядрами легче работать эмпирически. Они намного проще, не требуют случайной инициализации параметров, а их производительность более воспроизводима. \u003C\u002Fp\u003E\u003Cp\u003EИсследователи начали выяснять связи между реальными работающими сетями и ядрами. Учёные рады видеть, насколько далеко может зайти это новое видение.\u003C\u002Fp\u003E\u003Cblockquote\u003E\u003Cp\u003E«Если мы устанавливаем абсолютную, полную эквивалентность, я думаю, это может изменить всё», — утверждает Белкин.\u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Cp\u003EПока учёные упрощают работу с нейросетями, вы можете обратить внимание на наши курсы, чтобы научиться с их помощью решать проблемы бизнеса и изменить карьеру:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fdata-scientist-pro?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=data-science_dspr_181021&amp;utm_term=conc\"\u003EПрофессия Data Scientist (24 месяца)\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fmachine-learning-i-deep-learning?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=data-science_mldl_181021&amp;utm_term=conc\"\u003EКурс «Machine Learning и Deep Learning» (6 месяцев)\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EТакже вы можете перейти на страницы \u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fcatalogue?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=sf_allcourses_181021&amp;utm_term=conc\"\u003Eиз каталога\u003C\u002Fa\u003E, чтобы узнать, как мы готовим специалистов в других направлениях.\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" height=\"200\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fe01\u002Fb89\u002F876\u002Fe01b89876f6a2e7e499e544a77e72bba.png\" data-width=\"1000\"\u002F\u003E\u003Cfigcaption\u003E\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cdetails class=\"spoiler\"\u003E\u003Csummary\u003EПрофессии и курсы\u003C\u002Fsummary\u003E\u003Cdiv class=\"spoiler__content\"\u003E\u003Cp\u003E\u003Cstrong\u003EData Science и Machine Learning\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fdata-scientist-pro?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=data-science_dspr_181021&amp;utm_term=cat\"\u003EПрофессия Data Scientist\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fdata-analyst-pro?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=analytics_dapr_181021&amp;utm_term=cat\"\u003EПрофессия Data Analyst\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fmatematika-dlya-data-science#syllabus?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=data-science_mat_181021&amp;utm_term=cat\"\u003EКурс «Математика для Data Science»\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fmatematika-i-machine-learning-dlya-data-science?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=data-science_matml_181021&amp;utm_term=cat\"\u003EКурс «Математика и Machine Learning для Data Science»\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fdata-engineer?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=data-science_dea_181021&amp;utm_term=cat\"\u003EКурс по Data Engineering\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fmachine-learning-i-deep-learning?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=data-science_mldl_181021&amp;utm_term=cat\"\u003EКурс «Machine Learning и Deep Learning»\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fmachine-learning?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=data-science_ml_181021&amp;utm_term=cat\"\u003EКурс по Machine Learning\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E\u003Cstrong\u003EPython, веб-разработка\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fpython-fullstack-web-developer?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_fpw_181021&amp;utm_term=cat\"\u003EПрофессия Fullstack-разработчик на Python\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fpython-for-web-developers?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_pws_181021&amp;utm_term=cat\"\u003EКурс «Python для веб-разработки»\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Ffrontend-razrabotchik?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_fr_181021&amp;utm_term=cat\"\u003EПрофессия Frontend-разработчик\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fwebdev?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_webdev_181021&amp;utm_term=cat\"\u003EПрофессия Веб-разработчик\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E\u003Cstrong\u003EМобильная разработка\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fios-razrabotchik-s-nulya?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_ios_181021&amp;utm_term=cat\"\u003EПрофессия iOS-разработчик\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fandroid-razrabotchik?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_andr_181021&amp;utm_term=cat\"\u003EПрофессия Android-разработчик\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E\u003Cstrong\u003EJava и C#\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fjava-razrabotchik?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_java_181021&amp;utm_term=cat\"\u003EПрофессия Java-разработчик\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fjava-qa-engineer-testirovshik-po?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_qaja_181021&amp;utm_term=cat\"\u003EПрофессия QA-инженер на JAVA\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fc-sharp-razrabotchik?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_cdev_181021&amp;utm_term=cat\"\u003EПрофессия C#-разработчик\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fgame-razrabotchik-na-unity-i-c-sharp?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_gamedev_181021&amp;utm_term=cat\"\u003EПрофессия Разработчик игр на Unity\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E\u003Cstrong\u003EОт основ — в глубину\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Falgoritmy-i-struktury-dannyh?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_algo_181021&amp;utm_term=cat\"\u003EКурс «Алгоритмы и структуры данных»\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fc-plus-plus-razrabotchik?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_cplus_181021&amp;utm_term=cat\"\u003EПрофессия C++ разработчик\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fcyber-security-etichnij-haker?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_hacker_181021&amp;utm_term=cat\"\u003EПрофессия Этичный хакер\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E\u003Cstrong\u003EА также:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fskillfactory.ru\u002Fdevops-ingineer?utm_source=habr&amp;utm_medium=habr&amp;utm_campaign=article&amp;utm_content=coding_devops_181021&amp;utm_term=cat\"\u003EКурс по DevOps\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdetails\u003E\u003C\u002Fdiv\u003E","tags":[{"titleHtml":"skillfactory"},{"titleHtml":"математика"},{"titleHtml":"гаусс"},{"titleHtml":"ядро"},{"titleHtml":"машина опорных векторов"},{"titleHtml":"обобщения"},{"titleHtml":"бесконечность"},{"titleHtml":"инс"},{"titleHtml":"нейросети"},{"titleHtml":"упрощение"}],"metadata":{"stylesUrls":[],"scriptUrls":[],"shareImageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F775\u002Fa09\u002F173\u002F775a091730993f28871abea6cc2ca62e.jpg","shareImageWidth":1200,"shareImageHeight":630,"vkShareImageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F775\u002Fa09\u002F173\u002F775a091730993f28871abea6cc2ca62e.jpg","schemaJsonLd":"{\"@context\":\"http:\\\u002F\\\u002Fschema.org\",\"@type\":\"Article\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https:\\\u002F\\\u002Fhabr.com\\\u002Fru\\\u002Fcompany\\\u002Fskillfactory\\\u002Fblog\\\u002F584170\\\u002F\"},\"headline\":\"Нейросети могут оказаться проще, чем принято считать\",\"datePublished\":\"2021-10-18T22:06:26+03:00\",\"dateModified\":\"2021-10-22T18:02:20+03:00\",\"author\":{\"@type\":\"Person\",\"name\":\"honyaki\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Habr\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fa_\\\u002Flk\\\u002F9m\\\u002Fa_lk9mjkccjox-zccjrpfolmkmq.png\"}},\"description\":\"Нейросети отчасти будто подрывают традиционную теорию машинного обучения, которая сильно опирается на идеи теории вероятности и статистики. В чём же заключается...\",\"url\":\"https:\\\u002F\\\u002Fhabr.com\\\u002Fru\\\u002Fcompany\\\u002Fskillfactory\\\u002Fblog\\\u002F584170\\\u002F#post-content-body\",\"about\":[\"c_skillfactory\",\"h_maths\",\"h_read\",\"h_popular_science\",\"h_artificial_intelligence\",\"f_develop\",\"f_management\",\"f_popsci\"],\"image\":[\"https:\\\u002F\\\u002Fhabr.com\\\u002Fshare\\\u002Fpublication\\\u002F584170\\\u002F3740c1e31006d6fca3a1d5842c7f5c25\\\u002F\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F8a6\\\u002F371\\\u002F45e\\\u002F8a637145e56ff345a069bf048d528c13.jpg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Fe01\\\u002Fb89\\\u002F876\\\u002Fe01b89876f6a2e7e499e544a77e72bba.png\"]}","metaDescription":"Нейросети отчасти будто подрывают традиционную теорию машинного обучения, которая сильно опирается на идеи теории вероятности и статистики. В чём же заключается загадка их успеха?Исследователи...","mainImageUrl":null,"amp":true},"polls":[],"commentsEnabled":true,"rulesRemindEnabled":false,"votesEnabled":true,"status":"published","plannedPublishTime":null,"checked":null,"isEditorial":false}},"articlesIds":{},"isLoading":false,"pagesCount":{},"route":{},"reasonsList":null,"view":"cards","lastVisitedRoute":{},"ssrCommentsArticleIds":[""],"karma":{}},"authorContribution":{"authors":{}},"betaTest":{"currentAnnouncement":null,"announcements":{},"announcementCards":null,"announcementComments":{},"announcementCommentThreads":{},"announcementCommentingStatuses":{},"archivedList":[]},"authorStatistics":{"articleRefs":{},"articleIds":{},"pagesCount":{},"route":{},"viewsCount":[],"maxStatsCount":{}},"career":{"seoLandings":[],"hubs":""},"comments":{"articleComments":{},"searchCommentsResults":null,"previewComment":null,"pagesCount":null,"commentAccess":{},"scrollParents":{},"pageArticleComments":{"lastViewedComment":0,"postId":null,"lastCommentTimestamp":"","moderated":[],"moderatedIds":[],"commentRoute":""}},"companies":{"companyRefs":{"skillfactory":{"alias":"skillfactory","imageUrl":"\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fcompany\u002F9bb\u002F5f7\u002Fd50\u002F9bb5f7d50871bf0981f64b1970f22fe1.png","titleHtml":"SkillFactory","descriptionHtml":"Школа Computer Science. Скидка 45% по коду HABR","relatedData":null,"statistics":{"postsCount":538,"newsCount":3,"vacanciesCount":4,"employeesCount":27,"careerRating":null,"subscribersCount":2670,"rating":146.01,"invest":null},"foundationDate":{"year":"2012","month":"11","day":"01"},"location":{"city":{"id":"447159","title":"Москва"},"region":{"id":"1885","title":"Москва и Московская обл."},"country":{"id":"168","title":"Россия"}},"siteUrl":"http:\u002F\u002Fwww.skillfactory.ru\u002F","staffNumber":"201–500 человек","registrationDate":"2012-12-14T13:25:37+00:00","representativeUser":{"alias":"skillfactory_school","fullname":"Skillfactory School"},"contacts":[{"title":"Сайт","url":"http:\u002F\u002Fwww.skillfactory.ru\u002F"}],"settings":{"analyticsSettings":[{"type":"ym","trackingId":"38813825"}],"branding":null,"status":"active"},"metadata":{"titleHtml":"SkillFactory, Москва - Школа Computer Science. Скидка 45% по коду HABR с 1 ноября 2012 г.","title":"SkillFactory, Москва - Школа Computer Science. Скидка 45% по коду HABR с 1 ноября 2012 г.","keywords":["Программирование","Python","Машинное обучение","Читальный зал","Научно-популярное"],"descriptionHtml":"538 статей от авторов компании SkillFactory","description":"538 статей от авторов компании SkillFactory"},"aDeskSettings":null,"careerAlias":"skillfactory","maxCustomTrackerLinks":0}},"companyIds":{},"companyTopIds":{},"pagesCount":{},"companyProfiles":{},"companiesCategories":[],"companiesCategoriesTotalCount":0,"companiesWidgets":{},"companiesWorkers":{},"companiesFans":{},"route":{},"isLoading":false,"companyWorkersLoading":false,"companyFansLoading":false,"vacancies":{}},"companiesContribution":{"hubs":{},"flows":{},"companyRefs":{}},"companyHubsContribution":{"contributionRefs":{"hubRefs":{},"hubIds":{}}},"conversation":{"messages":[],"respondent":null,"isLoadMore":false},"conversations":{"conversations":[],"unreadCount":0,"pagesCount":0,"isLoadMore":false},"desktopState":{"desktopFl":null,"desktopHl":null,"isChecked":false,"isLoginDemanded":false},"dfp":{"slotsDict":{}},"docs":{"menu":{},"articles":{},"mainMenu":[],"loading":{"main":false,"dropdown":false,"article":false}},"feature":{"isProbablyVisible":"true"},"flows":{"flows":[{"alias":"develop","id":1,"route":{"name":"FLOW_PAGE","params":{"flowName":"develop"}}},{"alias":"admin","id":6,"route":{"name":"FLOW_PAGE","params":{"flowName":"admin"}}},{"alias":"design","id":2,"route":{"name":"FLOW_PAGE","params":{"flowName":"design"}}},{"alias":"management","id":3,"route":{"name":"FLOW_PAGE","params":{"flowName":"management"}}},{"alias":"marketing","id":4,"route":{"name":"FLOW_PAGE","params":{"flowName":"marketing"}}},{"alias":"popsci","id":7,"route":{"name":"FLOW_PAGE","params":{"flowName":"popsci"}}}]},"global":{"isPwa":false,"device":"desktop","isHabrCom":true},"hubs":{"hubRefs":{},"hubIds":{},"pagesCount":{},"isLoading":false,"route":{}},"hubsBlock":{"hubRefs":{},"hubIds":{}},"i18n":{"fl":"ru","hl":"ru"},"info":{"infoPage":{},"isLoading":true},"location":{"urlStruct":{"protocol":null,"slashes":null,"auth":null,"host":null,"port":null,"hostname":null,"hash":null,"search":null,"query":{},"pathname":null,"path":null,"href":""},"searchQuery":null},"me":{"user":null,"ppgDemanded":false,"karmaResetInfo":{"canReincarnate":null,"wasReincarnated":null,"currentScore":null},"notes":null},"mostReadingList":{"mostReadingListIds":[],"mostReadingListRefs":null,"promoPost":null},"pinnedPost":{"pinnedPost":null},"ppa":{"articles":{},"card":null,"transactions":null,"totalTransactions":null,"isAccessible":null},"projectsBlocks":{"activeBlocks":{}},"pullRefresh":{"shouldRefresh":false},"sandbox":{"articleIds":[],"articleRefs":{},"pagesCount":null,"route":{},"lastVisitedRoute":{},"isLoading":false},"settingsOther":{"inputs":{"uiLang":{"errors":[],"ref":null,"value":""},"articlesLangEnglish":{"errors":[],"ref":null,"value":false},"articlesLangRussian":{"errors":[],"ref":null,"value":false},"agreement":{"errors":[],"ref":null,"value":false},"email":{"errors":[],"ref":null,"value":true},"digest":{"errors":[],"ref":null,"value":true}}},"similarList":{"similarListIds":[],"similarListRefs":null},"ssr":{"error":null,"isDataLoaded":false,"isDataLoading":false,"isHydrationFailed":false,"isServer":false},"userHubsContribution":{"contributionRefs":{"hubRefs":{},"hubIds":{}}},"userInvites":{"availableInvites":0,"usedInvitesIds":[],"usedInvitesRefs":{},"usedInvitesPagesCount":0,"unusedInvitesIds":[],"unusedInvitesRefs":{},"unusedInvitesPagesCount":0},"users":{"authorRefs":{},"authorIds":{},"pagesCount":{},"authorProfiles":{},"userHubs":{},"userInvitations":{},"authorFollowers":{},"authorFollowed":{},"karmaStats":[],"statistics":null,"isLoading":false,"authorFollowersLoading":false,"authorFollowedLoading":false,"userHubsLoading":false,"userInvitationsLoading":false,"route":{}},"viewport":{"prevScrollY":{},"scrollY":0,"width":0},"tracker":{"items":{},"pagesCache":{},"markedViewedSilently":{},"markedRead":{},"unreadCounters":{"applications":null,"system":null,"mentions":null,"subscribers":null,"posts_and_comments":null},"unviewedCounters":{"applications":null,"system":null,"mentions":null,"subscribers":null,"posts_and_comments":null}}};(function(){var s;(s=document.currentScript||document.scripts[document.scripts.length-1]).parentNode.removeChild(s);}());</script>
<script src="https://assets.habr.com/habr-web/js/chunk-vendors.c2c3fc9a.js" defer></script><script src="https://assets.habr.com/habr-web/js/app.c0af73e7.js" defer></script>



    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    </script>
  
  <script type="text/javascript" >
    (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
    m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
    (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

    ym(24049213, "init", {
      defer:true,
      trackLinks:true,
      accurateTrackBounce:true,
      webvisor:false,
    });
  </script>
  <noscript>
    <div>
      <img src="https://mc.yandex.ru/watch/24049213" style="position:absolute; left:-9999px;" alt="" />
    </div>
  </noscript>
  
    <script type="text/javascript">
      window.addEventListener('load', function () {
        setTimeout(() => {
          const img = new Image();
          img.src = 'https://vk.com/rtrg?p=VK-RTRG-421343-57vKE';
        }, 0);
      });
    </script>
  
</body>
</html>
