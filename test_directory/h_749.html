<!DOCTYPE html>
<html lang="ru" data-vue-meta="%7B%22lang%22:%7B%22ssr%22:%22ru%22%7D%7D">
<head >
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0,viewport-fit=cover">
  <title>Отдача от глубокого обучения снижается. Что с этим делать / Хабр</title>
  <style>
    /* cyrillic-ext */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveSxf6TF0.woff2) format('woff2');
      unicode-range: U+0460-052F, U+1C80-1C88, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;
    }

    /* cyrillic */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveQhf6TF0.woff2) format('woff2');
      unicode-range: U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;
    }

    /* latin-ext */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveSBf6TF0.woff2) format('woff2');
      unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
    }

    /* latin */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveRhf6.woff2) format('woff2');
      unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
    }
  </style>
  <link rel="preload" href="https://assets.habr.com/habr-web/css/chunk-vendors.e7843cc0.css" as="style"><link rel="preload" href="https://assets.habr.com/habr-web/js/chunk-vendors.c2c3fc9a.js" as="script"><link rel="preload" href="https://assets.habr.com/habr-web/css/app.02ee25a4.css" as="style"><link rel="preload" href="https://assets.habr.com/habr-web/js/app.c0af73e7.js" as="script">
  <link rel="stylesheet" href="https://assets.habr.com/habr-web/css/chunk-vendors.e7843cc0.css"><link rel="stylesheet" href="https://assets.habr.com/habr-web/css/app.02ee25a4.css">
  <script>window.i18nFetch = new Promise((res, rej) => {
          const xhr = new XMLHttpRequest();
          xhr.open('GET', '/js/i18n/ru-compiled.85eb77f0b17c8235e7b64b9f81ea5ec2.json');
          xhr.responseType = 'json';
          xhr.onload = function(e) {
            if (this.status === 200) {
              res({ru: xhr.response});
            } else {
              rej(e);
            }
          };
          xhr.send();
        });</script>
  
  <script data-vue-meta="ssr" src="/js/ads.js" onload="window['zhY4i4nJ9K'] = true" data-vmid="checkad"></script><script data-vue-meta="ssr" type="application/ld+json" data-vmid="ldjson-schema">{"@context":"http:\/\/schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/habr.com\/ru\/post\/582646\/"},"headline":"Отдача от глубокого обучения снижается. Что с этим делать","datePublished":"2021-10-10T19:45:39+03:00","dateModified":"2021-10-10T22:57:08+03:00","author":{"@type":"Person","name":"Sivchenko_translate"},"publisher":{"@type":"Organization","name":"Habr","logo":{"@type":"ImageObject","url":"https:\/\/habrastorage.org\/webt\/a_\/lk\/9m\/a_lk9mjkccjox-zccjrpfolmkmq.png"}},"description":"В настоящее время&nbsp;глубокое обучение используется для перевода, прогнозирования&nbsp;укладки белков,&nbsp;анализа рентгеновских и других медицинских снимков , а также для&nbsp;и...","url":"https:\/\/habr.com\/ru\/post\/582646\/#post-content-body","about":["h_machine_learning","h_artificial_intelligence","f_develop","f_popsci"],"image":["https:\/\/habr.com\/share\/publication\/582646\/2958b1d2633ee075daf1a61fddeb04f5\/","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/041\/60b\/cc8\/04160bcc879bab8464ecd384700e7ded.png"]}</script>
  <script src="//www.googletagservices.com/tag/js/gpt.js" async></script>
  <style>.grecaptcha-badge{visibility: hidden;}</style>
  <meta name="habr-version" content="2.49.0">
  
  <meta data-vue-meta="ssr" property="fb:app_id" content="444736788986613"><meta data-vue-meta="ssr" property="fb:pages" content="472597926099084"><meta data-vue-meta="ssr" name="twitter:card" content="summary_large_image"><meta data-vue-meta="ssr" name="twitter:site" content="@habr_eng"><meta data-vue-meta="ssr" property="og:title" content="Отдача от глубокого обучения снижается. Что с этим делать" data-vmid="og:title"><meta data-vue-meta="ssr" name="twitter:title" content="Отдача от глубокого обучения снижается. Что с этим делать" data-vmid="twitter:title"><meta data-vue-meta="ssr" name="aiturec:title" content="Отдача от глубокого обучения снижается. Что с этим делать" data-vmid="aiturec:title"><meta data-vue-meta="ssr" name="description" content="В настоящее время&amp;nbsp;глубокое обучение используется для перевода, прогнозирования&amp;nbsp;укладки белков,&amp;nbsp;анализа рентгеновских и других медицинских снимков , а также для&amp;nbsp;игр, столь сложных..." data-vmid="description"><meta data-vue-meta="ssr" itemprop="description" content="В настоящее время&amp;nbsp;глубокое обучение используется для перевода, прогнозирования&amp;nbsp;укладки белков,&amp;nbsp;анализа рентгеновских и других медицинских снимков , а также для&amp;nbsp;игр, столь сложных..." data-vmid="description:itemprop"><meta data-vue-meta="ssr" property="og:description" content="В настоящее время&amp;nbsp;глубокое обучение используется для перевода, прогнозирования&amp;nbsp;укладки белков,&amp;nbsp;анализа рентгеновских и других медицинских снимков , а также для&amp;nbsp;игр, столь сложных..." data-vmid="og:description"><meta data-vue-meta="ssr" name="twitter:description" content="В настоящее время&amp;nbsp;глубокое обучение используется для перевода, прогнозирования&amp;nbsp;укладки белков,&amp;nbsp;анализа рентгеновских и других медицинских снимков , а также для&amp;nbsp;игр, столь сложных..." data-vmid="twitter:description"><meta data-vue-meta="ssr" property="aiturec:description" content="В настоящее время&amp;nbsp;глубокое обучение используется для перевода, прогнозирования&amp;nbsp;укладки белков,&amp;nbsp;анализа рентгеновских и других медицинских снимков , а также для&amp;nbsp;игр, столь сложных..." data-vmid="aiturec:description"><meta data-vue-meta="ssr" itemprop="image" content="https://habrastorage.org/getpro/habr/upload_files/c6e/34c/13f/c6e34c13f2b2d9ca7c1989b27056cd75.jpg" data-vmid="image:itemprop"><meta data-vue-meta="ssr" property="og:image" content="https://habrastorage.org/getpro/habr/upload_files/c6e/34c/13f/c6e34c13f2b2d9ca7c1989b27056cd75.jpg" data-vmid="og:image"><meta data-vue-meta="ssr" property="aiturec:image" content="https://habrastorage.org/getpro/habr/upload_files/c6e/34c/13f/c6e34c13f2b2d9ca7c1989b27056cd75.jpg" data-vmid="aiturec:image"><meta data-vue-meta="ssr" name="twitter:image" content="https://habrastorage.org/getpro/habr/upload_files/c6e/34c/13f/c6e34c13f2b2d9ca7c1989b27056cd75.jpg" data-vmid="twitter:image"><meta data-vue-meta="ssr" property="vk:image" content="https://habrastorage.org/getpro/habr/upload_files/c6e/34c/13f/c6e34c13f2b2d9ca7c1989b27056cd75.jpg" data-vmid="vk:image"><meta data-vue-meta="ssr" property="aiturec:item_id" content="582646" data-vmid="aiturec:item_id"><meta data-vue-meta="ssr" property="aiturec:datetime" content="2021-10-10T16:45:39.000Z" data-vmid="aiturec:datetime"><meta data-vue-meta="ssr" property="og:type" content="article" data-vmid="og:type"><meta data-vue-meta="ssr" property="og:locale" content="ru_RU" data-vmid="og:locale"><meta data-vue-meta="ssr" property="og:image:width" content="1200" data-vmid="og:image:width"><meta data-vue-meta="ssr" property="og:image:height" content="630" data-vmid="og:image:height">
  <link data-vue-meta="ssr" href="https://habr.com/ru/rss/post/582646/?fl=ru" type="application/rss+xml" title="" rel="alternate" name="rss"><link data-vue-meta="ssr" href="https://habr.com/ru/post/582646/" rel="canonical" data-vmid="canonical"><link data-vue-meta="ssr" data-vmid="hreflang"><link data-vue-meta="ssr" image_src="image" href="https://habrastorage.org/getpro/habr/upload_files/c6e/34c/13f/c6e34c13f2b2d9ca7c1989b27056cd75.jpg" data-vmid="image:href"><link data-vue-meta="ssr" rel="amphtml" href="https://habr.com/ru/amp/post/582646/">
  <meta name="apple-mobile-web-app-status-bar-style" content="#303b44">
  <meta name="msapplication-TileColor" content="#629FBC">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="mobile-web-app-capable" content="yes">
  <link
    rel="shortcut icon"
    type="image/png"
    sizes="16x16"
    href="https://assets.habr.com/habr-web/img/favicons/favicon-16.png"
  >
  <link
    rel="shortcut icon"
    type="image/png"
    sizes="32x32"
    href="https://assets.habr.com/habr-web/img/favicons/favicon-32.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="76x76"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-76.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="120x120"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-120.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="152x152"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-152.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="180x180"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-180.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="256x256"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-256.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1136x640.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2436x1125.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1792x828.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_828x1792.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1334x750.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1242x2668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2208x1242.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1125x2436.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1242x2208.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2732x2048.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2688x1242.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2224x1668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_750x1334.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2048x2732.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2388x1668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1668x2224.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_640x1136.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1668x2388.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2048x1536.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1536x2048.png"
  >
  <link
    rel="mask-icon"
    color="#77a2b6"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-120.svg"
  >
  <link
    crossorigin="use-credentials"
    href="/manifest.webmanifest"
    rel="manifest"
  >
</head>
<body>


<div id="app" data-server-rendered="true" data-async-called="true"><div class="tm-layout__wrapper"><!----> <div></div> <!----> <header class="tm-header"><div class="tm-page-width"><div class="tm-header__container"><!----> <span class="tm-header__logo-wrap"><a href="/ru/" class="tm-header__logo tm-header__logo_ru"><svg height="16" width="16" class="tm-svg-img tm-header__icon"><title>Хабр</title> <use xlink:href="/img/habr-logo-ru.svg#logo"></use></svg></a> <span class="tm-header__beta-sign" style="display:none;">β</span></span> <div class="tm-dropdown tm-header__projects"><div class="tm-dropdown__head"><button class="tm-header__dropdown-toggle"><svg height="16" width="16" class="tm-svg-img tm-header__icon tm-header__icon_dropdown"><title>Открыть список</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#arrow-down"></use></svg></button></div> <!----></div> <a href="/ru/sandbox/start/" class="tm-header__become-author-btn">
            Как стать автором
          </a> <div class="tm-feature tm-header__feature tm-feature_variant-inline"><!----></div> <!----> <!----></div></div></header> <div class="tm-layout"><div class="tm-page-progress-bar"></div> <div data-menu-sticky="true" class="tm-base-layout__header tm-base-layout__header_is-sticky"><div class="tm-page-width"><div class="tm-base-layout__header-wrapper"><div class="tm-main-menu"><div class="tm-main-menu__section"><nav class="tm-main-menu__section-content"><!----> <a href="/ru/all/" class="tm-main-menu__item">
        Все потоки
      </a> <a href="/ru/flows/develop/" class="tm-main-menu__item">
          Разработка
        </a><a href="/ru/flows/admin/" class="tm-main-menu__item">
          Администрирование
        </a><a href="/ru/flows/design/" class="tm-main-menu__item">
          Дизайн
        </a><a href="/ru/flows/management/" class="tm-main-menu__item">
          Менеджмент
        </a><a href="/ru/flows/marketing/" class="tm-main-menu__item">
          Маркетинг
        </a><a href="/ru/flows/popsci/" class="tm-main-menu__item">
          Научпоп
        </a></nav></div></div> <div class="tm-header-user-menu tm-base-layout__user-menu"><a href="/ru/search/" class="tm-header-user-menu__item tm-header-user-menu__search"><svg height="24" width="24" class="tm-svg-img tm-header-user-menu__icon tm-header-user-menu__icon_search tm-header-user-menu__icon_dark"><title>Поиск</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#search"></use></svg></a> <!----> <!----> <!----> <div class="tm-header-user-menu__item tm-header-user-menu__user_desktop"><div class="tm-dropdown"><div class="tm-dropdown__head"><svg height="24" width="24" data-test-id="menu-toggle-guest" class="tm-svg-img tm-header-user-menu__icon"><title>Профиль</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#header-user"></use></svg> <!----></div> <!----></div> <!----></div> <!----></div></div></div></div> <!----> <div class="tm-page-width"></div> <main class="tm-layout__container"><div hl="ru" data-async-called="true" class="tm-page"><div class="tm-page-width"><!----> <div class="tm-page__wrapper"><div class="tm-page__main tm-page__main_has-sidebar"><div class="pull-down"><div class="pull-down__header" style="height:0px;"><div class="pull-down__content" style="bottom:10px;"><svg height="24" width="24" class="tm-svg-img pull-down__arrow"><title>Обновить</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#pull-arrow"></use></svg></div></div> <div class="tm-article-presenter"> <div class="tm-article-presenter__body"><div class="tm-misprint-area"><div class="tm-misprint-area__wrapper"><article class="tm-article-presenter__content tm-article-presenter__content_narrow"><div class="tm-article-presenter__header"> <div class="tm-article-snippet tm-article-presenter__snippet"><div class="tm-article-snippet__meta-container"><div class="tm-article-snippet__meta"><span class="tm-user-info tm-article-snippet__author"><a href="/ru/users/Sivchenko_translate/" title="Sivchenko_translate" class="tm-user-info__userpic"><div class="tm-entity-image"><svg height="24" width="24" class="tm-svg-img tm-image-placeholder tm-image-placeholder_green"><!----> <use xlink:href="/img/megazord-v24.ce74655c.svg#placeholder-user"></use></svg></div></a> <span class="tm-user-info__user"><a href="/ru/users/Sivchenko_translate/" class="tm-user-info__username">
      Sivchenko_translate
    </a> </span></span> <span class="tm-article-snippet__datetime-published"><time datetime="2021-10-10T16:45:39.000Z" title="2021-10-10, 19:45">10  октября   в 19:45</time></span></div> <!----></div> <h1 lang="ru" class="tm-article-snippet__title tm-article-snippet__title_h1"><span>Отдача от глубокого обучения снижается. Что с этим делать</span></h1> <div class="tm-article-snippet__hubs"><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/machine_learning/" class="tm-article-snippet__hubs-item-link"><span>Машинное обучение</span> <span title="Профильный хаб" class="tm-article-snippet__profiled-hub">*</span></a></span><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/artificial_intelligence/" class="tm-article-snippet__hubs-item-link"><span>Искусственный интеллект</span> <!----></a></span></div> <div class="tm-article-snippet__labels"><div class="tm-article-snippet__label"><span>
        Перевод
      </span></div></div> <!----> <!----></div></div> <div class="tm-article-presenter__origin"><a href="https://spectrum.ieee.org/deep-learning-computational-cost" target="_blank" class="tm-article-presenter__origin-link">
                Автор оригинала:
                <span>
                  NEIL C. THOMPSON KRISTJAN GREENEWALD KEEHEON LEE GABRIEL F. MANSO
                </span></a></div> <div data-gallery-root="" lang="ru" class="tm-article-body"><div id="post-content-body" class="article-formatted-body article-formatted-body_version-2"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>В настоящее время </strong>глубокое обучение используется для перевода, прогнозирования <a href="https://www.nature.com/articles/d41586-020-03348-4" rel="noopener noreferrer nofollow">укладки белков</a>,<a href="https://www.nature.com/articles/d41586-020-03157-9" rel="noopener noreferrer nofollow"> анализа рентгеновских и других медицинских снимков </a>, а также для <a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far" rel="noopener noreferrer nofollow">игр, столь сложных как го </a> - вот лишь некоторые варианты применения этой технологии, которая становится всепроникающей. Успех в этой и других отраслях привел технологию машинного обучения от безвестности в нулевые до доминирования сегодня. </p><p>Хотя, славные дни глубокого обучения начались сравнительно недавно, зародилась эта парадигма много лет назад. В 1958 году, когда компьютеры-мейнфреймы еще занимали целые залы и работали на электронно-лучевых трубках, <a href="https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon" rel="noopener noreferrer nofollow">Фрэнк Розенблатт из Корнельского университета</a>, исходя из знаний о том, как устроены связи между нейронами в мозге, спроектировал первую нейронную сеть, которую прозорливо описал как «устройство для распознавания образов». Но амбиции Розенблатта сильно опережали его время – и он об этом знал. Даже в своей инаугурационной статье он был вынужден признать, с каким волчьим аппетитом нейронная сеть жрет вычислительные ресурсы, сетуя на то, что «по мере того, как количество связей в сети растет… нагрузка на традиционный цифровой компьютер вскоре становится чрезмерной».  </p><p>К счастью для разработчиков таких искусственных нейронных сетей, вскоре влившихся в концепцию «глубокого обучения» и получивших дополнительные слои нейронов, десятилетия работы <a href="https://spectrum.ieee.org/the-multiple-lives-of-moores-law" rel="noopener noreferrer nofollow">закона Мура</a> и проистекающее из него улучшение аппаратного обеспечения позволили примерно в <a href="https://s3-us-west-2.amazonaws.com/ieeeshutpages/xplore/xplore-ie-notice.html?" rel="noopener noreferrer nofollow">10 миллионов раз</a> увеличить объем вычислений, выполняемых компьютером в секунду. Поэтому, когда исследователи вернулись к разработке глубокого обучения в конце нулевых, у них уже были орудия, соразмерные этому вызову.     </p><p>Новые компьютеры с принципиально возросшей мощностью позволили создавать сети, в которых было неизмеримо больше связей и нейронов, чем в прототипах; соответственно, они гораздо лучше подходили для моделирования сложных феноменов. Исследователи пользовались возможностью делать перерывы между записями, применяя алгоритмы глубокого обучения к решению новых задач.</p><p>Притом, что взлет глубокого обучения вышел аэрокосмическим, будущее его может оказаться тернистым. Как и Розенблатт полвека назад, нынешние исследователи глубокого обучения приближаются к пределу возможностей тех инструментов, которыми располагают. Чтобы понять, почему это переформатирует все машинное обучение, сначала давайте разберемся, почему глубокое обучение оказалось столь успешным, и чего стоит держать его на таком уровне.</p><p><strong>Глубокое обучение</strong> – это современное воплощение в давней тенденции развития искусственного интеллекта, а именно движения от четко очерченных экспертных систем, опирающихся на конкретные знания, к гибким статистическим моделям. Первые системы в истории ИИ были основаны на правилах, для вывода результатов в них применялись логика и экспертные знания. Позже в такие системы была добавлена возможность обучения для настраивания некоторых параметров, но обычно число таких параметров было невелико.</p><p>Нынешние нейронные сети тоже изучают значения параметров, но эти параметры входят в состав столь гибких компьютерных моделей, что – если эти модели достаточно велики – становятся универсальными аппроксиматорами функций, то есть, подходят для описания данных любого типа. Именно по причине такой неограниченной гибкости глубокое обучение применимо к столь разным предметным областям.</p><p>Гибкость нейронных сетей обусловлена тем, что модель может принять множество видов ввода, а сеть – скомбинировать эту информацию мириадами способов. Таким образом, вывод нейронной сети получается применением не простых формул, а неизмеримо сложных. </p><p>Например, когда ультрасовременная система распознавания образов <a href="https://paperswithcode.com/paper/self-training-with-noisy-student-improves" rel="noopener noreferrer nofollow">Noisy Student</a> преобразует пиксельные значения изображения в вероятности, описывающие, что же может быть на нем, это делается при помощи сети, содержащей 480 миллионов параметров. Еще примечательнее сам процесс обучения, удостоверяющий верность такого множества параметров, поскольку в нем использовалось всего 1,2 миллиона размеченных изображений. Это может смутить тех читателей, которые помнят из школьного курса алгебры, что уравнений больше, чем неизвестных. Оказывается, ключ технологии – в том, что это правило нарушается.</p><p>Модели глубокого обучения переполнены параметрами, то есть, параметров в них больше, чем точек данных, доступных для обучения. В классическом сценарии это должно приводить к переобучению, когда модель не только выучит общие тенденции, но и усвоит случайные закономерности, имеющиеся в учебных данных. При глубоком обучении такая ловушка обходится, поскольку параметры инициализируются случайным образом, а затем их множества уточняются итерация за итерацией для более полного соответствия данным. Такой метод называется «стохастический градиентный спуск». Удивительно, но (доказано) эта процедура гарантирует, что усвоенная модель хорошо обобщается.</p><p>Успех гибких моделей глубокого обучения хорошо просматривается на примере машинного перевода. Десятилетиями применяются программы для перевода текстов с языка на язык. На первых этапах эта задача решалась при помощи правил, которые формулировали эксперты по грамматике. Но чем больше становился доступный объем текстовых данных на каждом конкретном языке, тем успешнее они поддавались обработке при помощи статистических подходов – имеющих столь мудреные наименования, как «максимальная энтропия», «скрытые Марковские модели» и «условные случайные поля». </p><p>Исходно успешность этих подходов для обработки каждого языка зависела от объема данных, доступных на нем, и от грамматических свойств языка. Например, перевод на основе правил для таких языков, как урду, арабский и малайский поначалу превосходил по качеству статистические методы перевода. Сегодня глубокое обучение обставило все альтернативные подходы и показало превосходные результаты практически на любом материале, к которому применялось.</p><p>Итак, повод для оптимизма в том, что глубокое обучение обеспечивает чрезвычайную гибкость в работе. Повод для пессимизма – в том, что эта гибкость требует огромных вычислительных ресурсов. Эта темная сторона реальности имеет два аспекта.</p><figure class="full-width "><img src="/img/image-loader.svg" alt="Экстраполируя достижения последних лет, можно предположить, что к 2025 году уровень допущения ошибок в наилучших системах глубокого обучения, предназначенных для распознавания объектов из множества данных ImageNet должен сократиться всего до 5 процентов [вверху]. Но вычислительные ресурсы и энергия, которые потребуются для обучения такой системы будущего, также будут колоссальными, и приведут к такому же выбросу углекислого газа, какой дает за месяц весь город Нью-Йорк [внизу].ИСТОЧНИК: N.C. THOMPSON, K. GREENEWALD, K. LEE, G.F. MANSO" title="Экстраполируя достижения последних лет, можно предположить, что к 2025 году уровень допущения ошибок в наилучших системах глубокого обучения, предназначенных для распознавания объектов из множества данных ImageNet должен сократиться всего до 5 процентов [вверху]. Но вычислительные ресурсы и энергия, которые потребуются для обучения такой системы будущего, также будут колоссальными, и приведут к такому же выбросу углекислого газа, какой дает за месяц весь город Нью-Йорк [внизу].ИСТОЧНИК: N.C. THOMPSON, K. GREENEWALD, K. LEE, G.F. MANSO" height="684" data-src="https://habrastorage.org/getpro/habr/upload_files/041/60b/cc8/04160bcc879bab8464ecd384700e7ded.png" data-width="614"/><figcaption>Экстраполируя достижения последних лет, можно предположить, что к 2025 году уровень допущения ошибок в наилучших системах глубокого обучения, предназначенных для распознавания объектов из множества данных ImageNet должен сократиться всего до 5 процентов [вверху]. Но вычислительные ресурсы и энергия, которые потребуются для обучения такой системы будущего, также будут колоссальными, и приведут к такому же выбросу углекислого газа, какой дает за месяц весь город Нью-Йорк [внизу].ИСТОЧНИК: N.C. THOMPSON, K. GREENEWALD, K. LEE, G.F. MANSO</figcaption></figure><p>Первая часть актуальна для всех статистических моделей: чтобы улучшить производительность в <em>k </em>раз, для обучения модели должно применяться как минимум в <em>k</em><sup>2</sup> раз больше точек данных. Вторая часть вычислительных издержек связана непосредственно с чрезмерной параметризацией. Если учесть ее в полной мере, то для вышеуказанного улучшения требуемый рост издержек составит <em>как минимум k</em><sup>4</sup>. Эта четверка в степени дается очень дорого. Например, для десятикратного улучшения результата потребуется как минимум 10 000-кратное увеличение объема вычислений. </p><p>Чтобы еще выразительнее представить этот компромисс между гибкостью модели и объемом вычислений, рассмотрим такой сценарий: мы пытаемся спрогнозировать, покажет ли рентгенограмма пациента признаки рака. Далее предположим, что верный ответ может быть найден, если проанализировать 100 деталей рентгенограммы (такие детали зачастую называются «переменными» или «признаками»). Сложность в том, что заранее мы не знаем, какие переменные будут важны, и пул переменных, которые потребуется рассмотреть, может быть очень велик. </p><p>Если бы эта проблема решалась с позиции применения экспертных систем, то нам бы потребовались специалисты, разбирающиеся в радиологии и онкологии, чтобы они указали те переменные, которые считают важными, после чего системе будет разрешено проверять лишь эти переменные. Гибкий подход к обучению — это протестировать максимально возможное количество переменных и позволить системе самой вывести, какие из этих переменных важны. В таком случае системе потребуется гораздо больше данных для анализа, и этот процесс повлечет значительно более высокие вычислительные издержки.  </p><p>Модели, для которых эксперты установили релевантные переменные, могут быстро изучить, какие значения лучше всего работают с этими переменными, и для этого потребуется сравнительно небольшой объем вычислений. Именно поэтому как раз такие модели были наиболее популярны поначалу. Но обучение застопоривается, если эксперт не смог правильно задать все переменные, которые должны быть включены в модель. Напротив, гибкие модели, в частности, модели глубокого обучения, менее эффективны и требуют неизмеримо больше вычислений, чтобы потягаться в производительности с экспертными моделями. Но при наличии достаточного объема вычислений (и множества данных) гибкие модели могут обставить те, для которых эксперты попытались указать все релевантные переменные. </p><p><strong>Определенно, можно повысить </strong>производительность глубокого обучения, если задействовать больше вычислительных ресурсов для построения более крупных моделей и обучать их на более обширных данных. Но какова будет цена такого вычислительного бремени? Могут ли эти расходы вырасти настолько, что станут тормозить прогресс? </p><p>Чтобы конкретно ответить на эти вопросы, <a href="https://arxiv.org/abs/2007.05558" rel="noopener noreferrer nofollow">недавно были собраны данные</a> из более чем 1000 исследовательских статей по глубокому обучению, охватывающих такие предметные области, как классификация изображений, обнаружение объектов, ответы на вопросы, распознавание именованных сущностей и машинный перевод. Здесь мы подробно обсудим только классфикацию изображений, но описанные ниже закономерности применимы очень широко.  </p><p>За годы работы сокращение количества ошибок при классификации изображений давалось колоссальным наращиванием вычислительной нагрузки. Например, в 2012 году модель <a href="https://en.wikipedia.org/wiki/AlexNet" rel="noopener noreferrer nofollow">AlexNet</a>, впервые показавшая мощь глубокого обучения на графическом процессоре (GPU), обучалась с применением двух графических процессоров на протяжении пяти-шести дней. К 2018 году другая модель, <a href="https://arxiv.org/pdf/1707.07012.pdf" rel="noopener noreferrer nofollow">NASNet</a>-A, наполовину сократила количество допускаемых ошибок по сравнению с AlexNet, но потребовавшийся для этого объем вычислительных ресурсов оказался более чем в 1000 раз значительнее.</p><p>Анализ этого феномена также позволил оценить, как эти результаты сочетаются с теоретическими ожиданиями. Теоретически вычисления должны масштабироваться как минимум в четвертой степени относительно фактического повышения производительности. На практике же для этого требовалось масштабирование как минимум в <em>девятой</em> степени.</p><p>Данная девятая степень означает, что для уменьшения показателя ошибок наполовину следует ожидать, что вам потребуется более чем в 500 раз нарастить вычислительные ресурсы. Это сокрушительно высокая цена. Правда, здесь есть и луч надежды: такая пропасть между теоретическими прогнозами и практическими результатами может означать, что до сих пор остаются неоткрытые варианты оптимизации алгоритмов, которые позволили бы радикально повысить эффективность глубокого обучения.</p><p>Как отмечалось выше, закон Мура и другие показатели совершенствования аппаратного обеспечения позволили невероятно повысить производительность чипов. Означает ли это, что постоянная эскалация требований к вычислительным мощностям непринципиальна? К сожалению, нет. Из 1000-кратной разницы в вычислениях между AlexNet и NASNet-A, лишь шестикратное увеличение было достигнуто на уровне улучшения оборудования. Вся остальная разница была достигнута наращиванием количества процессоров и все более длительным их использованием, а все это влечет дополнительные издержки.</p><p>Оценив кривую «издержки-производительность» для распознавания изображений, можно использовать эти данные для прогноза, сколько вычислительных ресурсов потребуется для достижения еще более впечатляющих показателей производительности в будущем. Так, для снижения доли ошибок до 5% понадобится 10 <sup>19</sup> миллиардов операций с плавающей точкой. </p><p><a href="https://arxiv.org/abs/1906.02243" rel="noopener noreferrer nofollow">Важная работа</a> ученых из Массачусетского университета в Амхерсте позволяет понять, каковы экономические издержки и углеродный след, сопряженные с такой вычислительной нагрузкой. Результаты удручают: обучение такой модели обойдется в 100 миллиардов долларов США и даст столько же парниковых выбросов, сколько дает город Нью-Йорк за месяц. А если оценить вычислительную нагрузку, необходимую для снижения доли ошибок до 1%, результаты будут гораздо печальнее. </p><p>Оправданны ли в данном случае экстраполяции на несколько порядков? И да, и нет. Конечно же, важно понимать, что прогнозы не совсем точны, но при таких плачевных результатах абсолютная точность и не нужна, чтобы передать общий месседж о неустойчивости подобных систем. Экстраполяция такого рода <em>была бы</em> неправомерной, если бы мы предположили, что исследователи так и будут следовать по нынешнему пути, вплоть до экстремального результата. Нет, мы этого не делаем. Исследователи, имея дело с зашкаливающими расходами, будут вынуждены либо найти более эффективные способы для решения задач глубокого обучения, либо забросить такие задачи – и прогресс остановится. </p><p>С другой стороны, экстраполяция таких результатов не только разумна, но и важна, поскольку помогает понять масштаб вызова, маячащего перед нами. Фронт этой проблемы уже хорошо просматривается. Когда компания <a href="https://www.deepmind.com/" rel="noopener noreferrer nofollow">DeepMind</a>, ныне входящая в состав Google, учила свою систему играть в го, стоимость этой работы <a href="https://www.yuzeh.com/data/agz-cost.html" rel="noopener noreferrer nofollow">оценивалась в 35 миллионов долларов</a>. Когда исследователи DeepMind спроектировали <a href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii" rel="noopener noreferrer nofollow">систему для игры в <em>StarCraft II</em></a>, они намеренно не пробовали нескольких вариантов проектирования одного важного компонента, поскольку такое обучение стоило бы слишком дорого.</p><p><a href="https://openai.com/" rel="noopener noreferrer nofollow">OpenAI</a>, важный аналитический центр, занимающийся изучением машинного обучения, недавно спроектировал и натренировал весьма прославленную <a href="https://spectrum.ieee.org/tech-talk/artificial-intelligence/machine-learning/open-ais-powerful-text-generating-tool-is-ready-for-business" rel="noopener noreferrer nofollow">языковую систему глубокого обучения GPT-3</a>, потратив на это более 4 миллионов долларов. Хотя они и допустили ошибку при реализации системы, эту ошибку не исправляли, пояснив это в приложении к своей научной статье так: "<a href="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Supplemental.pdf" rel="noopener noreferrer nofollow">в силу стоимости обучения, не представлялось возможным переобучить модель</a>."</p><p>Даже бизнес, не относящийся к высоким технологиям, в настоящее время начинает чураться глубокого обучения из-за его высоких вычислительных издержек. Недавно крупная европейская сеть супермаркетов отказалась от своей системы глубокого обучения, хотя та и заметно помогала прогнозировать, какие товары будут приобретаться. Руководство компании свернуло эту разработку, сочтя, что обучение и эксплуатация этой системы обойдется слишком дорого.</p><p><strong>Имея дело с растущими</strong> экономическими и экологическими издержками, сообщество исследователей глубокого обучения изыскивает способы повысить производительность систем, но так, чтобы потребности в вычислительных мощностях не становились запредельными. Если этого не сделать, процессы начнут стагнировать. Но рано отчаиваться: для решения этой проблемы делается многое.</p><p>Одна из стратегий – использовать процессоры, спроектированные специально с расчетом на эффективное глубокое обучение. Такой подход широко использовался на протяжении минувшего десятилетия, тогда на смену CPU пришли GPU, а в некоторых случаях – программируемые пользователем вентильные матрицы и специфичные для конкретного приложения интегральные схемы (в том числе, <a href="https://spectrum.ieee.org/tech-talk/computing/hardware/heres-how-googles-tpu-v4-ai-chip-stacked-up-in-training-tests" rel="noopener noreferrer nofollow">Tensor Processing Unit</a> от Google). В целом, при всех этих подходах жертвуется универсальностью вычислительной платформы ради повышения эффективности и специализации. Но такая специализация становится все менее выгодной. Поэтому в долгосрочной перспективе будет перспективнее брать на вооружение совершенно иные аппаратные фреймворки, возможно, основанные на аналоговых, нейроморфных, оптических или квантовых системах. Правда, пока такие совершенно иные аппаратные фреймворки еще не играют заметной роли. </p><p>Другой подход к снижению вычислительного бремени сосредоточен на создании таких нейронных сетей, которые при реализации получаются меньше, чем современные. Такая тактика снижает стоимость каждого прогона нейронной сети, но часто увеличивает стоимость обучения (что и описано выше в этой статье). Какие из этих расходов наиболее важны – в основном зависит от ситуации. Для часто используемой модели именно эксплуатационные расходы будут наибольшим компонентом всей суммы инвестиций. Для других моделей, например, таких, которые часто требуется учить заново, могут преобладать расходы на обучение. В любом случае, общая сумма расходов будет превышать расходы собственно на обучение. Поэтому, как было показано выше, если расходы на обучение слишком высоки, то и общие расходы могут стать неподъемными. </p><p>И в этом основная проблема, касающаяся всех тактик, направленных на достижение компактной реализации: они не позволяют в достаточной степени сократить расходы на обучение. Например, какая-нибудь модель позволяет обучить большую сеть, но сильно увеличивает сложность обучения. Другая сопряжена с обучением большой сети и последующим отсечением неважных связей. Третья находит как можно более эффективную архитектуру, оптимизируясь на основе множества моделей – такой подход иногда называют «поиском нейронных архитектур». Притом, что каждая из этих техник может весьма положительно отразиться на реализации, на обучении она отражается слабо и определенно не решает описанных выше проблем, возникающих при обработке данных. Во многих случаях расходы только повышаются.  </p><p>Сейчас начинает развиваться подход, который позволил бы снизить расходы на обучение, он называется «мета-обучение». Идея в том, что система учится на разнообразных данных, и затем этот результат может применяться во многих областях. Например, чтобы не создавать отдельных систем для распознавания на изображениях собак, кошек и автомобилей, можно обучить на всех трех этих множествах данных одну и ту же систему и многократно использовать ее.</p><p>К сожалению, недавняя работа <a href="https://cbmm.mit.edu/about/people/barbu" rel="noopener noreferrer nofollow">Андрея Барбу</a> из MIT показала, насколько сложным может быть метаобучение. Они с соавторами показали, что даже небольшие различия между исходными данными и материалом, на котором предполагается использовать сеть, могут сильно понизить ее производительность. Они продемонстрировали, что современные системы распознавания изображений очень чувствительны к таким факторам, как, например, под каким углом и в какой позе был сфотографирован объект. Поэтому даже простая задача распознавания одних и тех же субъектов в различных позах может почти наполовину сбить точность системы.</p><p><a href="http://people.eecs.berkeley.edu/~brecht/" rel="noopener noreferrer nofollow">Бенджамин Рехт</a> из Калифорнийского университета в Беркли и другие говорят об этом еще резче, показывая, что даже на совершенно новых множествах данных, в которых намеренно имитируются учебные данные, производительность сети падает более чем на 10%. Если даже небольшие изменения в данных столь подрывают производительность, то для полноценного метаобучения может понадобиться запредельно много информации. Поэтому большие перспективы метаобучения еще далеки от воплощения. </p><p>Другая стратегия, которая, возможно, позволит преодолеть вычислительные пределы глубокого обучения – перейти на другие, возможно, еще не открытые или недооцененные типы машинного обучения. Как было рассказано выше, системы машинного обучения, сконструированные с привлечением экспертного опыта, могут показывать завидную эффективность, но значительно уступают системам глубокого обучения, если эксперты не учтут при их формировании всех важных факторов. Разрабатываются <a href="https://arxiv.org/abs/2012.05876" rel="noopener noreferrer nofollow">нейро-символические</a> методы и другие приемы, которые позволили бы сочетать силу экспертных знаний и рассуждений с гибкостью, присущей многим нейронным сетям.</p><p>Как и в ситуации, с которой столкнулся Розенблатт на завре развития нейронных сетей, сегодня глубокое обучение подходит к границам возможностей имеющихся вычислительных инструментов. Столкнувшись с необходимостью непозволительного наращивания вычислений, необходимо либо откорректировать наши подходы к глубокому обучению, либо смириться с тем, что в будущем его прогресс значительно замедлится. Конечно, хотелось бы адаптироваться. Возможен умный прорыв, который дал бы нам либо более эффективные модели глубокого обучения, либо гораздо более мощное аппаратное обеспечение, которое позволило бы и дальше использовать эти крайне гибкие модели. В противном случае возможен откат, и нам вновь придется привлекать к работе экспертов, чтобы выявлять, каким именно данным необходимо обучать сеть.   </p></div></div> <!----> <!----></div> <div class="tm-article-presenter__meta"><div class="tm-separated-list tm-article-presenter__meta-list"><span class="tm-separated-list__title">Теги:</span> <ul class="tm-separated-list__list"><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D0%B3%D0%BB%D1%83%D0%B1%D0%BE%D0%BA%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%5D" class="tm-tags-list__link">глубокое обучение</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5%20%D1%81%D0%B5%D1%82%D0%B8%5D" class="tm-tags-list__link">нейронные сети</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D1%80%D0%B0%D1%81%D0%BF%D0%BE%D0%B7%D0%BD%D0%B0%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B7%D0%BE%D0%B2%5D" class="tm-tags-list__link">распознавание образов</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D1%8B%D0%B9%20%D0%BF%D0%B5%D1%80%D0%B5%D0%B2%D0%BE%D0%B4%5D" class="tm-tags-list__link">машинный перевод</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D0%B8%D1%81%D1%81%D0%BB%D0%B5%D0%B4%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5%5D" class="tm-tags-list__link">исследование</a></li></ul></div> <div class="tm-separated-list tm-article-presenter__meta-list"><span class="tm-separated-list__title">Хабы:</span> <ul class="tm-separated-list__list"><li class="tm-separated-list__item"><a href="/ru/hub/machine_learning/" class="tm-hubs-list__link">
    Машинное обучение
  </a></li><li class="tm-separated-list__item"><a href="/ru/hub/artificial_intelligence/" class="tm-hubs-list__link">
    Искусственный интеллект
  </a></li></ul></div></div></article></div> <!----></div> <div class="tm-article-sticky-panel"><div class="tm-data-icons tm-article-sticky-panel__icons"><div class="tm-article-rating tm-data-icons__item"><div class="tm-votes-meter tm-article-rating__votes-switcher"><svg height="16" width="16" class="tm-svg-img tm-votes-meter__icon tm-votes-meter__icon_medium"><title>Всего голосов 23: ↑21 и ↓2</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-rating"></use></svg> <span title="Всего голосов 23: ↑21 и ↓2" class="tm-votes-meter__value tm-votes-meter__value_positive tm-votes-meter__value_medium">+19</span></div> <DIV class="v-portal" style="display:none;"></DIV></div> <!----> <span title="Количество просмотров" class="tm-icon-counter tm-data-icons__item"><svg height="16" width="16" class="tm-svg-img tm-icon-counter__icon"><title>Просмотры</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-views"></use></svg> <span class="tm-icon-counter__value">9.6K</span></span> <button title="Добавить в закладки" type="button" class="bookmarks-button tm-data-icons__item"><span title="Добавить в закладки" class="tm-svg-icon__wrapper bookmarks-button__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Добавить в закладки</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-favorite"></use></svg></span> <span title="Количество пользователей, добавивших публикацию в закладки" class="bookmarks-button__counter">
    47
  </span></button> <!----> <div title="Поделиться" class="tm-sharing tm-data-icons__item"><button type="button" class="tm-sharing__button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="tm-sharing__icon"><path fill="currentColor" d="M10.33.275l9.047 7.572a.2.2 0 010 .306l-9.048 7.572a.2.2 0 01-.328-.153V11c-8 0-9.94 6-9.94 6S-1 5 10 5V.428a.2.2 0 01.328-.153z"></path></svg></button> <!----></div> <DIV class="v-portal" style="display:none;"></DIV></div> </div></div> <!----> <!----> <div class="tm-article-presenter__footer"><div class="tm-article-blocks"><!----> <section class="tm-block tm-block_spacing-bottom"><!----> <div class="tm-block__body tm-block__body_variant-balanced"><div class="tm-article-author"> <div class="tm-user-card tm-article-author__user-card tm-user-card_variant-article"><div class="tm-user-card__info-container"><div class="tm-user-card__header"><div class="tm-user-card__header-data"><a href="/ru/users/Sivchenko_translate/" class="tm-user-card__userpic tm-user-card__userpic_size-40"><div class="tm-entity-image"><svg class="tm-svg-img tm-image-placeholder tm-image-placeholder_green"><!----> <use xlink:href="/img/megazord-v24.ce74655c.svg#placeholder-user"></use></svg></div></a> <div class="tm-user-card__meta"><div title=" 18 голосов " class="tm-karma tm-user-card__karma"><div class="tm-karma__votes tm-karma__votes_positive">
    16
  </div> <div class="tm-karma__text">
    Карма
  </div></div> <div title="Рейтинг пользователя" class="tm-rating tm-user-card__rating"><div class="tm-rating__header"> <div class="tm-rating__counter">24</div></div> <div class="tm-rating__text">
    Рейтинг
  </div></div></div></div></div> <div class="tm-user-card__info tm-user-card__info_variant-article"><div class="tm-user-card__title tm-user-card__title_variant-article"><!----> <a href="/ru/users/Sivchenko_translate/" class="tm-user-card__nickname tm-user-card__nickname_variant-article">
          @Sivchenko_translate
        </a> <!----></div> <p class="tm-user-card__short-info tm-user-card__short-info_variant-article">Пользователь</p></div></div> <div class="tm-user-card__buttons tm-user-card__buttons_variant-article"><!----> <!----> <!----> <!----> <!----></div></div> <!----></div> <DIV class="v-portal" style="display:none;"></DIV></div> <!----></section> <div class="tm-article-blocks__comments"><div class="tm-article-page-comments"><div class="tm-article-comments-counter-link tm-article-comments-counter-button"><a href="/ru/post/582646/comments/" class="tm-article-comments-counter-link__link tm-article-comments-counter-link__link_button-style"><svg height="16" width="16" class="tm-svg-img tm-article-comments-counter-link__icon tm-article-comments-counter-link__icon_contrasted"><title>Комментарии</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-comments"></use></svg> <span class="tm-article-comments-counter-link__value tm-article-comments-counter-link__value_contrasted">
       Комментарии 27 
    </span></a> <!----></div></div></div> <div class="tm-ad-banner__container tm-page-article__banner"><!----> <div id="articleBottomBanner" class="tm-ad-banner"></div></div> <!----> <!----> <!----> <!----> </div></div></div></div></div> <div class="tm-page__sidebar"><div class="tm-layout-sidebar"><div class="tm-layout-sidebar__ads tm-layout-sidebar__ads_initial"><div class="tm-ad-banner__container tm-layout-sidebar__banner"><!----> <div id="sidebarBanner" class="tm-ad-banner"></div></div></div> <div class="tm-sexy-sidebar tm-sexy-sidebar_initial" style="margin-top:0px;"><!----> <!----></div></div></div></div></div></div></main> <!----></div> <div class="tm-footer-menu"><div class="tm-page-width"><div class="tm-footer-menu__container"><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Ваш аккаунт
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/kek/v1/auth/habrahabr/?back=/ru/post/582646/&amp;hl=ru" rel="nofollow" target="_self">
                Войти
              </a></li><li class="tm-footer-menu__list-item"><a href="/kek/v1/auth/habrahabr-register/?back=/ru/post/582646/&amp;hl=ru" rel="nofollow" target="_self">
                Регистрация
              </a></li></ul></div></div><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Разделы
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/ru/" class="footer-menu__item-link router-link-active">
                Публикации
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/news/" class="footer-menu__item-link">
                Новости
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/hubs/" class="footer-menu__item-link">
                Хабы
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/companies/" class="footer-menu__item-link">
                Компании
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/users/" class="footer-menu__item-link">
                Авторы
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/sandbox/" class="footer-menu__item-link">
                Песочница
              </a></li></ul></div></div><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Информация
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/ru/docs/help/" class="footer-menu__item-link">
                Устройство сайта
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/authors/codex/" class="footer-menu__item-link">
                Для авторов
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/companies/corpblogs/" class="footer-menu__item-link">
                Для компаний
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/docs/transparency/" class="footer-menu__item-link">
                Документы
              </a></li><li class="tm-footer-menu__list-item"><a href="https://account.habr.com/info/agreement" target="_blank">
                Соглашение
              </a></li><li class="tm-footer-menu__list-item"><a href="https://account.habr.com/info/confidential/" target="_blank">
                Конфиденциальность
              </a></li></ul></div></div><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Услуги
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="https://docs.google.com/presentation/d/e/2PACX-1vQLwRfQmXibiUlWaRg-BAc38s7oM3lJiaPju7qmdJsp8ysIvZ_G-Npem0njJLMozE2bPHMpDqiI5hhy/pub?start=false&amp;loop=false&amp;delayms=60000&amp;slide=id.g91a03369cd_4_297" target="_blank">
                Реклама
              </a></li><li class="tm-footer-menu__list-item"><a href="https://habrastorage.org/storage/stuff/habr/service_price.pdf" target="_blank">
                Тарифы
              </a></li><li class="tm-footer-menu__list-item"><a href="https://docs.google.com/presentation/d/e/2PACX-1vQJJds8-Di7BQSP_guHxICN7woVYoN5NP_22ra-BIo4bqnTT9FR6fB-Ku2P0AoRpX0Ds-LRkDeAoD8F/pub?start=false&amp;loop=false&amp;delayms=60000" target="_blank">
                Контент
              </a></li><li class="tm-footer-menu__list-item"><a href="https://tmtm.timepad.ru/" target="_blank">
                Семинары
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/megaprojects/" class="footer-menu__item-link">
                Мегапроекты
              </a></li></ul></div></div></div></div></div> <div class="tm-footer"><div class="tm-page-width"><div class="tm-footer__container"><!----> <div class="tm-footer__social"><a href="https://www.facebook.com/habrahabr.ru" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Facebook</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-facebook"></use></svg></a><a href="https://twitter.com/habr_com" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Twitter</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-twitter"></use></svg></a><a href="https://vk.com/habr" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>VK</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-vkontakte"></use></svg></a><a href="https://telegram.me/habr_com" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Telegram</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-telegram"></use></svg></a><a href="https://www.youtube.com/channel/UCd_sTwKqVrweTt4oAKY5y4w" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Youtube</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-youtube"></use></svg></a><a href="https://zen.yandex.ru/habr" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Яндекс Дзен</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-zen"></use></svg></a></div> <DIV class="v-portal" style="display:none;"></DIV> <button class="tm-footer__link"><!---->
        Настройка языка
      </button> <a href="/ru/about" class="tm-footer__link">
        О сайте
      </a> <a href="/ru/feedback/" class="tm-footer__link">
        Техническая поддержка
      </a> <!----> <a href="/berserk-mode-nope" class="tm-footer__link">
        Вернуться на старую версию
      </a> <div class="tm-footer-copyright"><span class="tm-copyright"><span class="tm-copyright__years">© 2006–2021 </span> <span class="tm-copyright__name">«<a href="https://company.habr.com/" rel="noopener" target="_blank" class="tm-copyright__link">Habr</a>»</span></span></div></div></div></div> <!----> <!----></div> <div class="vue-portal-target"></div></div>
<script>window.__INITIAL_STATE__={"adblock":{"hasAcceptableAdsFilter":false,"hasAdblock":false},"articlesList":{"articlesList":{"582646":{"id":"582646","timePublished":"2021-10-10T16:45:39+00:00","isCorporative":false,"lang":"ru","titleHtml":"Отдача от глубокого обучения снижается. Что с этим делать","leadData":{"textHtml":"\u003Cp\u003E\u003Cstrong\u003EВ настоящее время&nbsp;\u003C\u002Fstrong\u003Eглубокое обучение используется для перевода, прогнозирования&nbsp;\u003Ca href=\"https:\u002F\u002Fwww.nature.com\u002Farticles\u002Fd41586-020-03348-4\" rel=\"noopener noreferrer nofollow\"\u003Eукладки белков\u003C\u002Fa\u003E,\u003Ca href=\"https:\u002F\u002Fwww.nature.com\u002Farticles\u002Fd41586-020-03157-9\" rel=\"noopener noreferrer nofollow\"\u003E&nbsp;анализа рентгеновских и других медицинских снимков \u003C\u002Fa\u003E, а также для&nbsp;\u003Ca href=\"https:\u002F\u002Fdeepmind.com\u002Fresearch\u002Fcase-studies\u002Falphago-the-story-so-far\" rel=\"noopener noreferrer nofollow\"\u003Eигр, столь сложных как го \u003C\u002Fa\u003E&nbsp;- вот лишь некоторые варианты применения этой технологии, которая становится всепроникающей. Успех в этой и других отраслях привел технологию машинного обучения от безвестности в нулевые до доминирования сегодня. \u003C\u002Fp\u003E\u003Cp\u003EХотя, славные дни глубокого обучения начались сравнительно недавно, зародилась эта парадигма много лет назад. В 1958 году, когда компьютеры-мейнфреймы еще занимали целые залы и работали на электронно-лучевых трубках, \u003Ca href=\"https:\u002F\u002Fnews.cornell.edu\u002Fstories\u002F2019\u002F09\u002Fprofessors-perceptron-paved-way-ai-60-years-too-soon\" rel=\"noopener noreferrer nofollow\"\u003EФрэнк Розенблатт из Корнельского университета\u003C\u002Fa\u003E, исходя из знаний о том, как устроены связи между нейронами в мозге, спроектировал первую нейронную сеть, которую прозорливо описал как «устройство для распознавания образов». Но амбиции Розенблатта сильно опережали его время – и он об этом знал. Даже в своей инаугурационной статье он был вынужден признать, с каким волчьим аппетитом нейронная сеть жрет вычислительные ресурсы, сетуя на то, что «по мере того, как количество связей в сети растет… нагрузка на традиционный цифровой компьютер вскоре становится чрезмерной». &nbsp;\u003C\u002Fp\u003E","imageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fc6e\u002F34c\u002F13f\u002Fc6e34c13f2b2d9ca7c1989b27056cd75.jpg","buttonTextHtml":"Читать далее","image":{"url":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fc6e\u002F34c\u002F13f\u002Fc6e34c13f2b2d9ca7c1989b27056cd75.jpg","fit":"cover","positionY":0,"positionX":0}},"editorVersion":"2.0","postType":"article","postLabels":[{"type":"translation","data":{"originalAuthorName":"NEIL C. THOMPSON KRISTJAN GREENEWALD KEEHEON LEE GABRIEL F. MANSO","originalUrl":"https:\u002F\u002Fspectrum.ieee.org\u002Fdeep-learning-computational-cost"}}],"author":{"scoreStats":{"score":16,"votesCount":18},"rating":24,"relatedData":null,"contacts":[],"authorContacts":[],"paymentDetails":{"paymentYandexMoney":null,"paymentPayPalMe":null,"paymentWebmoney":null},"id":"2793727","alias":"Sivchenko_translate","fullname":null,"avatarUrl":null,"speciality":null},"statistics":{"commentsCount":27,"favoritesCount":47,"readingCount":9577,"score":19,"votesCount":23},"hubs":[{"relatedData":null,"id":"19439","alias":"machine_learning","type":"collective","title":"Машинное обучение","titleHtml":"Машинное обучение","isProfiled":true},{"relatedData":null,"id":"21922","alias":"artificial_intelligence","type":"collective","title":"Искусственный интеллект","titleHtml":"Искусственный интеллект","isProfiled":false}],"flows":[{"id":"1","alias":"develop","title":"Разработка"},{"id":"7","alias":"popsci","title":"Научпоп"}],"relatedData":null,"textHtml":"\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\"\u003E\u003Cp\u003E\u003Cstrong\u003EВ настоящее время \u003C\u002Fstrong\u003Eглубокое обучение используется для перевода, прогнозирования \u003Ca href=\"https:\u002F\u002Fwww.nature.com\u002Farticles\u002Fd41586-020-03348-4\" rel=\"noopener noreferrer nofollow\"\u003Eукладки белков\u003C\u002Fa\u003E,\u003Ca href=\"https:\u002F\u002Fwww.nature.com\u002Farticles\u002Fd41586-020-03157-9\" rel=\"noopener noreferrer nofollow\"\u003E анализа рентгеновских и других медицинских снимков \u003C\u002Fa\u003E, а также для \u003Ca href=\"https:\u002F\u002Fdeepmind.com\u002Fresearch\u002Fcase-studies\u002Falphago-the-story-so-far\" rel=\"noopener noreferrer nofollow\"\u003Eигр, столь сложных как го \u003C\u002Fa\u003E - вот лишь некоторые варианты применения этой технологии, которая становится всепроникающей. Успех в этой и других отраслях привел технологию машинного обучения от безвестности в нулевые до доминирования сегодня. \u003C\u002Fp\u003E\u003Cp\u003EХотя, славные дни глубокого обучения начались сравнительно недавно, зародилась эта парадигма много лет назад. В 1958 году, когда компьютеры-мейнфреймы еще занимали целые залы и работали на электронно-лучевых трубках, \u003Ca href=\"https:\u002F\u002Fnews.cornell.edu\u002Fstories\u002F2019\u002F09\u002Fprofessors-perceptron-paved-way-ai-60-years-too-soon\" rel=\"noopener noreferrer nofollow\"\u003EФрэнк Розенблатт из Корнельского университета\u003C\u002Fa\u003E, исходя из знаний о том, как устроены связи между нейронами в мозге, спроектировал первую нейронную сеть, которую прозорливо описал как «устройство для распознавания образов». Но амбиции Розенблатта сильно опережали его время – и он об этом знал. Даже в своей инаугурационной статье он был вынужден признать, с каким волчьим аппетитом нейронная сеть жрет вычислительные ресурсы, сетуя на то, что «по мере того, как количество связей в сети растет… нагрузка на традиционный цифровой компьютер вскоре становится чрезмерной».  \u003C\u002Fp\u003E\u003Cp\u003EК счастью для разработчиков таких искусственных нейронных сетей, вскоре влившихся в концепцию «глубокого обучения» и получивших дополнительные слои нейронов, десятилетия работы \u003Ca href=\"https:\u002F\u002Fspectrum.ieee.org\u002Fthe-multiple-lives-of-moores-law\" rel=\"noopener noreferrer nofollow\"\u003Eзакона Мура\u003C\u002Fa\u003E и проистекающее из него улучшение аппаратного обеспечения позволили примерно в \u003Ca href=\"https:\u002F\u002Fs3-us-west-2.amazonaws.com\u002Fieeeshutpages\u002Fxplore\u002Fxplore-ie-notice.html?\" rel=\"noopener noreferrer nofollow\"\u003E10 миллионов раз\u003C\u002Fa\u003E увеличить объем вычислений, выполняемых компьютером в секунду. Поэтому, когда исследователи вернулись к разработке глубокого обучения в конце нулевых, у них уже были орудия, соразмерные этому вызову.     \u003C\u002Fp\u003E\u003Cp\u003EНовые компьютеры с принципиально возросшей мощностью позволили создавать сети, в которых было неизмеримо больше связей и нейронов, чем в прототипах; соответственно, они гораздо лучше подходили для моделирования сложных феноменов. Исследователи пользовались возможностью делать перерывы между записями, применяя алгоритмы глубокого обучения к решению новых задач.\u003C\u002Fp\u003E\u003Cp\u003EПритом, что взлет глубокого обучения вышел аэрокосмическим, будущее его может оказаться тернистым. Как и Розенблатт полвека назад, нынешние исследователи глубокого обучения приближаются к пределу возможностей тех инструментов, которыми располагают. Чтобы понять, почему это переформатирует все машинное обучение, сначала давайте разберемся, почему глубокое обучение оказалось столь успешным, и чего стоит держать его на таком уровне.\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EГлубокое обучение\u003C\u002Fstrong\u003E – это современное воплощение в давней тенденции развития искусственного интеллекта, а именно движения от четко очерченных экспертных систем, опирающихся на конкретные знания, к гибким статистическим моделям. Первые системы в истории ИИ были основаны на правилах, для вывода результатов в них применялись логика и экспертные знания. Позже в такие системы была добавлена возможность обучения для настраивания некоторых параметров, но обычно число таких параметров было невелико.\u003C\u002Fp\u003E\u003Cp\u003EНынешние нейронные сети тоже изучают значения параметров, но эти параметры входят в состав столь гибких компьютерных моделей, что – если эти модели достаточно велики – становятся универсальными аппроксиматорами функций, то есть, подходят для описания данных любого типа. Именно по причине такой неограниченной гибкости глубокое обучение применимо к столь разным предметным областям.\u003C\u002Fp\u003E\u003Cp\u003EГибкость нейронных сетей обусловлена тем, что модель может принять множество видов ввода, а сеть – скомбинировать эту информацию мириадами способов. Таким образом, вывод нейронной сети получается применением не простых формул, а неизмеримо сложных. \u003C\u002Fp\u003E\u003Cp\u003EНапример, когда ультрасовременная система распознавания образов \u003Ca href=\"https:\u002F\u002Fpaperswithcode.com\u002Fpaper\u002Fself-training-with-noisy-student-improves\" rel=\"noopener noreferrer nofollow\"\u003ENoisy Student\u003C\u002Fa\u003E преобразует пиксельные значения изображения в вероятности, описывающие, что же может быть на нем, это делается при помощи сети, содержащей 480 миллионов параметров. Еще примечательнее сам процесс обучения, удостоверяющий верность такого множества параметров, поскольку в нем использовалось всего 1,2 миллиона размеченных изображений. Это может смутить тех читателей, которые помнят из школьного курса алгебры, что уравнений больше, чем неизвестных. Оказывается, ключ технологии – в том, что это правило нарушается.\u003C\u002Fp\u003E\u003Cp\u003EМодели глубокого обучения переполнены параметрами, то есть, параметров в них больше, чем точек данных, доступных для обучения. В классическом сценарии это должно приводить к переобучению, когда модель не только выучит общие тенденции, но и усвоит случайные закономерности, имеющиеся в учебных данных. При глубоком обучении такая ловушка обходится, поскольку параметры инициализируются случайным образом, а затем их множества уточняются итерация за итерацией для более полного соответствия данным. Такой метод называется «стохастический градиентный спуск». Удивительно, но (доказано) эта процедура гарантирует, что усвоенная модель хорошо обобщается.\u003C\u002Fp\u003E\u003Cp\u003EУспех гибких моделей глубокого обучения хорошо просматривается на примере машинного перевода. Десятилетиями применяются программы для перевода текстов с языка на язык. На первых этапах эта задача решалась при помощи правил, которые формулировали эксперты по грамматике. Но чем больше становился доступный объем текстовых данных на каждом конкретном языке, тем успешнее они поддавались обработке при помощи статистических подходов – имеющих столь мудреные наименования, как «максимальная энтропия», «скрытые Марковские модели» и «условные случайные поля». \u003C\u002Fp\u003E\u003Cp\u003EИсходно успешность этих подходов для обработки каждого языка зависела от объема данных, доступных на нем, и от грамматических свойств языка. Например, перевод на основе правил для таких языков, как урду, арабский и малайский поначалу превосходил по качеству статистические методы перевода. Сегодня глубокое обучение обставило все альтернативные подходы и показало превосходные результаты практически на любом материале, к которому применялось.\u003C\u002Fp\u003E\u003Cp\u003EИтак, повод для оптимизма в том, что глубокое обучение обеспечивает чрезвычайную гибкость в работе. Повод для пессимизма – в том, что эта гибкость требует огромных вычислительных ресурсов. Эта темная сторона реальности имеет два аспекта.\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" alt=\"Экстраполируя достижения последних лет, можно предположить, что к 2025 году уровень допущения ошибок в наилучших системах глубокого обучения, предназначенных для распознавания объектов из множества данных ImageNet должен сократиться всего до 5 процентов [вверху]. Но вычислительные ресурсы и энергия, которые потребуются для обучения такой системы будущего, также будут колоссальными, и приведут к такому же выбросу углекислого газа, какой дает за месяц весь город Нью-Йорк [внизу].ИСТОЧНИК: N.C. THOMPSON, K. GREENEWALD, K. LEE, G.F. MANSO\" title=\"Экстраполируя достижения последних лет, можно предположить, что к 2025 году уровень допущения ошибок в наилучших системах глубокого обучения, предназначенных для распознавания объектов из множества данных ImageNet должен сократиться всего до 5 процентов [вверху]. Но вычислительные ресурсы и энергия, которые потребуются для обучения такой системы будущего, также будут колоссальными, и приведут к такому же выбросу углекислого газа, какой дает за месяц весь город Нью-Йорк [внизу].ИСТОЧНИК: N.C. THOMPSON, K. GREENEWALD, K. LEE, G.F. MANSO\" height=\"684\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F041\u002F60b\u002Fcc8\u002F04160bcc879bab8464ecd384700e7ded.png\" data-width=\"614\"\u002F\u003E\u003Cfigcaption\u003EЭкстраполируя достижения последних лет, можно предположить, что к 2025 году уровень допущения ошибок в наилучших системах глубокого обучения, предназначенных для распознавания объектов из множества данных ImageNet должен сократиться всего до 5 процентов [вверху]. Но вычислительные ресурсы и энергия, которые потребуются для обучения такой системы будущего, также будут колоссальными, и приведут к такому же выбросу углекислого газа, какой дает за месяц весь город Нью-Йорк [внизу].ИСТОЧНИК: N.C. THOMPSON, K. GREENEWALD, K. LEE, G.F. MANSO\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EПервая часть актуальна для всех статистических моделей: чтобы улучшить производительность в \u003Cem\u003Ek \u003C\u002Fem\u003Eраз, для обучения модели должно применяться как минимум в \u003Cem\u003Ek\u003C\u002Fem\u003E\u003Csup\u003E2\u003C\u002Fsup\u003E раз больше точек данных. Вторая часть вычислительных издержек связана непосредственно с чрезмерной параметризацией. Если учесть ее в полной мере, то для вышеуказанного улучшения требуемый рост издержек составит \u003Cem\u003Eкак минимум k\u003C\u002Fem\u003E\u003Csup\u003E4\u003C\u002Fsup\u003E. Эта четверка в степени дается очень дорого. Например, для десятикратного улучшения результата потребуется как минимум 10 000-кратное увеличение объема вычислений. \u003C\u002Fp\u003E\u003Cp\u003EЧтобы еще выразительнее представить этот компромисс между гибкостью модели и объемом вычислений, рассмотрим такой сценарий: мы пытаемся спрогнозировать, покажет ли рентгенограмма пациента признаки рака. Далее предположим, что верный ответ может быть найден, если проанализировать 100 деталей рентгенограммы (такие детали зачастую называются «переменными» или «признаками»). Сложность в том, что заранее мы не знаем, какие переменные будут важны, и пул переменных, которые потребуется рассмотреть, может быть очень велик. \u003C\u002Fp\u003E\u003Cp\u003EЕсли бы эта проблема решалась с позиции применения экспертных систем, то нам бы потребовались специалисты, разбирающиеся в радиологии и онкологии, чтобы они указали те переменные, которые считают важными, после чего системе будет разрешено проверять лишь эти переменные. Гибкий подход к обучению — это протестировать максимально возможное количество переменных и позволить системе самой вывести, какие из этих переменных важны. В таком случае системе потребуется гораздо больше данных для анализа, и этот процесс повлечет значительно более высокие вычислительные издержки.  \u003C\u002Fp\u003E\u003Cp\u003EМодели, для которых эксперты установили релевантные переменные, могут быстро изучить, какие значения лучше всего работают с этими переменными, и для этого потребуется сравнительно небольшой объем вычислений. Именно поэтому как раз такие модели были наиболее популярны поначалу. Но обучение застопоривается, если эксперт не смог правильно задать все переменные, которые должны быть включены в модель. Напротив, гибкие модели, в частности, модели глубокого обучения, менее эффективны и требуют неизмеримо больше вычислений, чтобы потягаться в производительности с экспертными моделями. Но при наличии достаточного объема вычислений (и множества данных) гибкие модели могут обставить те, для которых эксперты попытались указать все релевантные переменные. \u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EОпределенно, можно повысить \u003C\u002Fstrong\u003Eпроизводительность глубокого обучения, если задействовать больше вычислительных ресурсов для построения более крупных моделей и обучать их на более обширных данных. Но какова будет цена такого вычислительного бремени? Могут ли эти расходы вырасти настолько, что станут тормозить прогресс? \u003C\u002Fp\u003E\u003Cp\u003EЧтобы конкретно ответить на эти вопросы, \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F2007.05558\" rel=\"noopener noreferrer nofollow\"\u003Eнедавно были собраны данные\u003C\u002Fa\u003E из более чем 1000 исследовательских статей по глубокому обучению, охватывающих такие предметные области, как классификация изображений, обнаружение объектов, ответы на вопросы, распознавание именованных сущностей и машинный перевод. Здесь мы подробно обсудим только классфикацию изображений, но описанные ниже закономерности применимы очень широко.  \u003C\u002Fp\u003E\u003Cp\u003EЗа годы работы сокращение количества ошибок при классификации изображений давалось колоссальным наращиванием вычислительной нагрузки. Например, в 2012 году модель \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FAlexNet\" rel=\"noopener noreferrer nofollow\"\u003EAlexNet\u003C\u002Fa\u003E, впервые показавшая мощь глубокого обучения на графическом процессоре (GPU), обучалась с применением двух графических процессоров на протяжении пяти-шести дней. К 2018 году другая модель, \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1707.07012.pdf\" rel=\"noopener noreferrer nofollow\"\u003ENASNet\u003C\u002Fa\u003E-A, наполовину сократила количество допускаемых ошибок по сравнению с AlexNet, но потребовавшийся для этого объем вычислительных ресурсов оказался более чем в 1000 раз значительнее.\u003C\u002Fp\u003E\u003Cp\u003EАнализ этого феномена также позволил оценить, как эти результаты сочетаются с теоретическими ожиданиями. Теоретически вычисления должны масштабироваться как минимум в четвертой степени относительно фактического повышения производительности. На практике же для этого требовалось масштабирование как минимум в \u003Cem\u003Eдевятой\u003C\u002Fem\u003E степени.\u003C\u002Fp\u003E\u003Cp\u003EДанная девятая степень означает, что для уменьшения показателя ошибок наполовину следует ожидать, что вам потребуется более чем в 500 раз нарастить вычислительные ресурсы. Это сокрушительно высокая цена. Правда, здесь есть и луч надежды: такая пропасть между теоретическими прогнозами и практическими результатами может означать, что до сих пор остаются неоткрытые варианты оптимизации алгоритмов, которые позволили бы радикально повысить эффективность глубокого обучения.\u003C\u002Fp\u003E\u003Cp\u003EКак отмечалось выше, закон Мура и другие показатели совершенствования аппаратного обеспечения позволили невероятно повысить производительность чипов. Означает ли это, что постоянная эскалация требований к вычислительным мощностям непринципиальна? К сожалению, нет. Из 1000-кратной разницы в вычислениях между AlexNet и NASNet-A, лишь шестикратное увеличение было достигнуто на уровне улучшения оборудования. Вся остальная разница была достигнута наращиванием количества процессоров и все более длительным их использованием, а все это влечет дополнительные издержки.\u003C\u002Fp\u003E\u003Cp\u003EОценив кривую «издержки-производительность» для распознавания изображений, можно использовать эти данные для прогноза, сколько вычислительных ресурсов потребуется для достижения еще более впечатляющих показателей производительности в будущем. Так, для снижения доли ошибок до 5% понадобится 10 \u003Csup\u003E19\u003C\u002Fsup\u003E миллиардов операций с плавающей точкой. \u003C\u002Fp\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1906.02243\" rel=\"noopener noreferrer nofollow\"\u003EВажная работа\u003C\u002Fa\u003E ученых из Массачусетского университета в Амхерсте позволяет понять, каковы экономические издержки и углеродный след, сопряженные с такой вычислительной нагрузкой. Результаты удручают: обучение такой модели обойдется в 100 миллиардов долларов США и даст столько же парниковых выбросов, сколько дает город Нью-Йорк за месяц. А если оценить вычислительную нагрузку, необходимую для снижения доли ошибок до 1%, результаты будут гораздо печальнее. \u003C\u002Fp\u003E\u003Cp\u003EОправданны ли в данном случае экстраполяции на несколько порядков? И да, и нет. Конечно же, важно понимать, что прогнозы не совсем точны, но при таких плачевных результатах абсолютная точность и не нужна, чтобы передать общий месседж о неустойчивости подобных систем. Экстраполяция такого рода \u003Cem\u003Eбыла бы\u003C\u002Fem\u003E неправомерной, если бы мы предположили, что исследователи так и будут следовать по нынешнему пути, вплоть до экстремального результата. Нет, мы этого не делаем. Исследователи, имея дело с зашкаливающими расходами, будут вынуждены либо найти более эффективные способы для решения задач глубокого обучения, либо забросить такие задачи – и прогресс остановится. \u003C\u002Fp\u003E\u003Cp\u003EС другой стороны, экстраполяция таких результатов не только разумна, но и важна, поскольку помогает понять масштаб вызова, маячащего перед нами. Фронт этой проблемы уже хорошо просматривается. Когда компания \u003Ca href=\"https:\u002F\u002Fwww.deepmind.com\u002F\" rel=\"noopener noreferrer nofollow\"\u003EDeepMind\u003C\u002Fa\u003E, ныне входящая в состав Google, учила свою систему играть в го, стоимость этой работы \u003Ca href=\"https:\u002F\u002Fwww.yuzeh.com\u002Fdata\u002Fagz-cost.html\" rel=\"noopener noreferrer nofollow\"\u003Eоценивалась в 35 миллионов долларов\u003C\u002Fa\u003E. Когда исследователи DeepMind спроектировали \u003Ca href=\"https:\u002F\u002Fdeepmind.com\u002Fblog\u002Farticle\u002Falphastar-mastering-real-time-strategy-game-starcraft-ii\" rel=\"noopener noreferrer nofollow\"\u003Eсистему для игры в \u003Cem\u003EStarCraft II\u003C\u002Fem\u003E\u003C\u002Fa\u003E, они намеренно не пробовали нескольких вариантов проектирования одного важного компонента, поскольку такое обучение стоило бы слишком дорого.\u003C\u002Fp\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fopenai.com\u002F\" rel=\"noopener noreferrer nofollow\"\u003EOpenAI\u003C\u002Fa\u003E, важный аналитический центр, занимающийся изучением машинного обучения, недавно спроектировал и натренировал весьма прославленную \u003Ca href=\"https:\u002F\u002Fspectrum.ieee.org\u002Ftech-talk\u002Fartificial-intelligence\u002Fmachine-learning\u002Fopen-ais-powerful-text-generating-tool-is-ready-for-business\" rel=\"noopener noreferrer nofollow\"\u003Eязыковую систему глубокого обучения GPT-3\u003C\u002Fa\u003E, потратив на это более 4 миллионов долларов. Хотя они и допустили ошибку при реализации системы, эту ошибку не исправляли, пояснив это в приложении к своей научной статье так: \"\u003Ca href=\"https:\u002F\u002Fproceedings.neurips.cc\u002Fpaper\u002F2020\u002Ffile\u002F1457c0d6bfcb4967418bfb8ac142f64a-Supplemental.pdf\" rel=\"noopener noreferrer nofollow\"\u003Eв силу стоимости обучения, не представлялось возможным переобучить модель\u003C\u002Fa\u003E.\"\u003C\u002Fp\u003E\u003Cp\u003EДаже бизнес, не относящийся к высоким технологиям, в настоящее время начинает чураться глубокого обучения из-за его высоких вычислительных издержек. Недавно крупная европейская сеть супермаркетов отказалась от своей системы глубокого обучения, хотя та и заметно помогала прогнозировать, какие товары будут приобретаться. Руководство компании свернуло эту разработку, сочтя, что обучение и эксплуатация этой системы обойдется слишком дорого.\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EИмея дело с растущими\u003C\u002Fstrong\u003E экономическими и экологическими издержками, сообщество исследователей глубокого обучения изыскивает способы повысить производительность систем, но так, чтобы потребности в вычислительных мощностях не становились запредельными. Если этого не сделать, процессы начнут стагнировать. Но рано отчаиваться: для решения этой проблемы делается многое.\u003C\u002Fp\u003E\u003Cp\u003EОдна из стратегий – использовать процессоры, спроектированные специально с расчетом на эффективное глубокое обучение. Такой подход широко использовался на протяжении минувшего десятилетия, тогда на смену CPU пришли GPU, а в некоторых случаях – программируемые пользователем вентильные матрицы и специфичные для конкретного приложения интегральные схемы (в том числе, \u003Ca href=\"https:\u002F\u002Fspectrum.ieee.org\u002Ftech-talk\u002Fcomputing\u002Fhardware\u002Fheres-how-googles-tpu-v4-ai-chip-stacked-up-in-training-tests\" rel=\"noopener noreferrer nofollow\"\u003ETensor Processing Unit\u003C\u002Fa\u003E от Google). В целом, при всех этих подходах жертвуется универсальностью вычислительной платформы ради повышения эффективности и специализации. Но такая специализация становится все менее выгодной. Поэтому в долгосрочной перспективе будет перспективнее брать на вооружение совершенно иные аппаратные фреймворки, возможно, основанные на аналоговых, нейроморфных, оптических или квантовых системах. Правда, пока такие совершенно иные аппаратные фреймворки еще не играют заметной роли. \u003C\u002Fp\u003E\u003Cp\u003EДругой подход к снижению вычислительного бремени сосредоточен на создании таких нейронных сетей, которые при реализации получаются меньше, чем современные. Такая тактика снижает стоимость каждого прогона нейронной сети, но часто увеличивает стоимость обучения (что и описано выше в этой статье). Какие из этих расходов наиболее важны – в основном зависит от ситуации. Для часто используемой модели именно эксплуатационные расходы будут наибольшим компонентом всей суммы инвестиций. Для других моделей, например, таких, которые часто требуется учить заново, могут преобладать расходы на обучение. В любом случае, общая сумма расходов будет превышать расходы собственно на обучение. Поэтому, как было показано выше, если расходы на обучение слишком высоки, то и общие расходы могут стать неподъемными. \u003C\u002Fp\u003E\u003Cp\u003EИ в этом основная проблема, касающаяся всех тактик, направленных на достижение компактной реализации: они не позволяют в достаточной степени сократить расходы на обучение. Например, какая-нибудь модель позволяет обучить большую сеть, но сильно увеличивает сложность обучения. Другая сопряжена с обучением большой сети и последующим отсечением неважных связей. Третья находит как можно более эффективную архитектуру, оптимизируясь на основе множества моделей – такой подход иногда называют «поиском нейронных архитектур». Притом, что каждая из этих техник может весьма положительно отразиться на реализации, на обучении она отражается слабо и определенно не решает описанных выше проблем, возникающих при обработке данных. Во многих случаях расходы только повышаются.  \u003C\u002Fp\u003E\u003Cp\u003EСейчас начинает развиваться подход, который позволил бы снизить расходы на обучение, он называется «мета-обучение». Идея в том, что система учится на разнообразных данных, и затем этот результат может применяться во многих областях. Например, чтобы не создавать отдельных систем для распознавания на изображениях собак, кошек и автомобилей, можно обучить на всех трех этих множествах данных одну и ту же систему и многократно использовать ее.\u003C\u002Fp\u003E\u003Cp\u003EК сожалению, недавняя работа \u003Ca href=\"https:\u002F\u002Fcbmm.mit.edu\u002Fabout\u002Fpeople\u002Fbarbu\" rel=\"noopener noreferrer nofollow\"\u003EАндрея Барбу\u003C\u002Fa\u003E из MIT показала, насколько сложным может быть метаобучение. Они с соавторами показали, что даже небольшие различия между исходными данными и материалом, на котором предполагается использовать сеть, могут сильно понизить ее производительность. Они продемонстрировали, что современные системы распознавания изображений очень чувствительны к таким факторам, как, например, под каким углом и в какой позе был сфотографирован объект. Поэтому даже простая задача распознавания одних и тех же субъектов в различных позах может почти наполовину сбить точность системы.\u003C\u002Fp\u003E\u003Cp\u003E\u003Ca href=\"http:\u002F\u002Fpeople.eecs.berkeley.edu\u002F~brecht\u002F\" rel=\"noopener noreferrer nofollow\"\u003EБенджамин Рехт\u003C\u002Fa\u003E из Калифорнийского университета в Беркли и другие говорят об этом еще резче, показывая, что даже на совершенно новых множествах данных, в которых намеренно имитируются учебные данные, производительность сети падает более чем на 10%. Если даже небольшие изменения в данных столь подрывают производительность, то для полноценного метаобучения может понадобиться запредельно много информации. Поэтому большие перспективы метаобучения еще далеки от воплощения. \u003C\u002Fp\u003E\u003Cp\u003EДругая стратегия, которая, возможно, позволит преодолеть вычислительные пределы глубокого обучения – перейти на другие, возможно, еще не открытые или недооцененные типы машинного обучения. Как было рассказано выше, системы машинного обучения, сконструированные с привлечением экспертного опыта, могут показывать завидную эффективность, но значительно уступают системам глубокого обучения, если эксперты не учтут при их формировании всех важных факторов. Разрабатываются \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F2012.05876\" rel=\"noopener noreferrer nofollow\"\u003Eнейро-символические\u003C\u002Fa\u003E методы и другие приемы, которые позволили бы сочетать силу экспертных знаний и рассуждений с гибкостью, присущей многим нейронным сетям.\u003C\u002Fp\u003E\u003Cp\u003EКак и в ситуации, с которой столкнулся Розенблатт на завре развития нейронных сетей, сегодня глубокое обучение подходит к границам возможностей имеющихся вычислительных инструментов. Столкнувшись с необходимостью непозволительного наращивания вычислений, необходимо либо откорректировать наши подходы к глубокому обучению, либо смириться с тем, что в будущем его прогресс значительно замедлится. Конечно, хотелось бы адаптироваться. Возможен умный прорыв, который дал бы нам либо более эффективные модели глубокого обучения, либо гораздо более мощное аппаратное обеспечение, которое позволило бы и дальше использовать эти крайне гибкие модели. В противном случае возможен откат, и нам вновь придется привлекать к работе экспертов, чтобы выявлять, каким именно данным необходимо обучать сеть.   \u003C\u002Fp\u003E\u003C\u002Fdiv\u003E","tags":[{"titleHtml":"глубокое обучение"},{"titleHtml":"нейронные сети"},{"titleHtml":"распознавание образов"},{"titleHtml":"машинный перевод"},{"titleHtml":"исследование"}],"metadata":{"stylesUrls":[],"scriptUrls":[],"shareImageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fc6e\u002F34c\u002F13f\u002Fc6e34c13f2b2d9ca7c1989b27056cd75.jpg","shareImageWidth":1200,"shareImageHeight":630,"vkShareImageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fc6e\u002F34c\u002F13f\u002Fc6e34c13f2b2d9ca7c1989b27056cd75.jpg","schemaJsonLd":"{\"@context\":\"http:\\\u002F\\\u002Fschema.org\",\"@type\":\"Article\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https:\\\u002F\\\u002Fhabr.com\\\u002Fru\\\u002Fpost\\\u002F582646\\\u002F\"},\"headline\":\"Отдача от глубокого обучения снижается. Что с этим делать\",\"datePublished\":\"2021-10-10T19:45:39+03:00\",\"dateModified\":\"2021-10-10T22:57:08+03:00\",\"author\":{\"@type\":\"Person\",\"name\":\"Sivchenko_translate\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Habr\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fa_\\\u002Flk\\\u002F9m\\\u002Fa_lk9mjkccjox-zccjrpfolmkmq.png\"}},\"description\":\"В настоящее время&nbsp;глубокое обучение используется для перевода, прогнозирования&nbsp;укладки белков,&nbsp;анализа рентгеновских и других медицинских снимков , а также для&nbsp;и...\",\"url\":\"https:\\\u002F\\\u002Fhabr.com\\\u002Fru\\\u002Fpost\\\u002F582646\\\u002F#post-content-body\",\"about\":[\"h_machine_learning\",\"h_artificial_intelligence\",\"f_develop\",\"f_popsci\"],\"image\":[\"https:\\\u002F\\\u002Fhabr.com\\\u002Fshare\\\u002Fpublication\\\u002F582646\\\u002F2958b1d2633ee075daf1a61fddeb04f5\\\u002F\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F041\\\u002F60b\\\u002Fcc8\\\u002F04160bcc879bab8464ecd384700e7ded.png\"]}","metaDescription":"В настоящее время&nbsp;глубокое обучение используется для перевода, прогнозирования&nbsp;укладки белков,&nbsp;анализа рентгеновских и других медицинских снимков , а также для&nbsp;игр, столь сложных...","mainImageUrl":null,"amp":true},"polls":[],"commentsEnabled":true,"rulesRemindEnabled":false,"votesEnabled":true,"status":"published","plannedPublishTime":null,"checked":null,"isEditorial":true}},"articlesIds":{},"isLoading":false,"pagesCount":{},"route":{},"reasonsList":null,"view":"cards","lastVisitedRoute":{},"ssrCommentsArticleIds":[""],"karma":{}},"authorContribution":{"authors":{}},"betaTest":{"currentAnnouncement":null,"announcements":{},"announcementCards":null,"announcementComments":{},"announcementCommentThreads":{},"announcementCommentingStatuses":{},"archivedList":[]},"authorStatistics":{"articleRefs":{},"articleIds":{},"pagesCount":{},"route":{},"viewsCount":[],"maxStatsCount":{}},"career":{"seoLandings":[],"hubs":"machine_learning,artificial_intelligence"},"comments":{"articleComments":{},"searchCommentsResults":null,"previewComment":null,"pagesCount":null,"commentAccess":{},"scrollParents":{},"pageArticleComments":{"lastViewedComment":0,"postId":null,"lastCommentTimestamp":"","moderated":[],"moderatedIds":[],"commentRoute":""}},"companies":{"companyRefs":{},"companyIds":{},"companyTopIds":{},"pagesCount":{},"companyProfiles":{},"companiesCategories":[],"companiesCategoriesTotalCount":0,"companiesWidgets":{},"companiesWorkers":{},"companiesFans":{},"route":{},"isLoading":false,"companyWorkersLoading":false,"companyFansLoading":false,"vacancies":{}},"companiesContribution":{"hubs":{},"flows":{},"companyRefs":{}},"companyHubsContribution":{"contributionRefs":{"hubRefs":{},"hubIds":{}}},"conversation":{"messages":[],"respondent":null,"isLoadMore":false},"conversations":{"conversations":[],"unreadCount":0,"pagesCount":0,"isLoadMore":false},"desktopState":{"desktopFl":null,"desktopHl":null,"isChecked":false,"isLoginDemanded":false},"dfp":{"slotsDict":{}},"docs":{"menu":{},"articles":{},"mainMenu":[],"loading":{"main":false,"dropdown":false,"article":false}},"feature":{"isProbablyVisible":"true"},"flows":{"flows":[{"alias":"develop","id":1,"route":{"name":"FLOW_PAGE","params":{"flowName":"develop"}}},{"alias":"admin","id":6,"route":{"name":"FLOW_PAGE","params":{"flowName":"admin"}}},{"alias":"design","id":2,"route":{"name":"FLOW_PAGE","params":{"flowName":"design"}}},{"alias":"management","id":3,"route":{"name":"FLOW_PAGE","params":{"flowName":"management"}}},{"alias":"marketing","id":4,"route":{"name":"FLOW_PAGE","params":{"flowName":"marketing"}}},{"alias":"popsci","id":7,"route":{"name":"FLOW_PAGE","params":{"flowName":"popsci"}}}]},"global":{"isPwa":false,"device":"desktop","isHabrCom":true},"hubs":{"hubRefs":{},"hubIds":{},"pagesCount":{},"isLoading":false,"route":{}},"hubsBlock":{"hubRefs":{},"hubIds":{}},"i18n":{"fl":"ru","hl":"ru"},"info":{"infoPage":{},"isLoading":true},"location":{"urlStruct":{"protocol":null,"slashes":null,"auth":null,"host":null,"port":null,"hostname":null,"hash":null,"search":null,"query":{},"pathname":null,"path":null,"href":""},"searchQuery":null},"me":{"user":null,"ppgDemanded":false,"karmaResetInfo":{"canReincarnate":null,"wasReincarnated":null,"currentScore":null},"notes":null},"mostReadingList":{"mostReadingListIds":[],"mostReadingListRefs":null,"promoPost":null},"pinnedPost":{"pinnedPost":null},"ppa":{"articles":{},"card":null,"transactions":null,"totalTransactions":null,"isAccessible":null},"projectsBlocks":{"activeBlocks":{}},"pullRefresh":{"shouldRefresh":false},"sandbox":{"articleIds":[],"articleRefs":{},"pagesCount":null,"route":{},"lastVisitedRoute":{},"isLoading":false},"settingsOther":{"inputs":{"uiLang":{"errors":[],"ref":null,"value":""},"articlesLangEnglish":{"errors":[],"ref":null,"value":false},"articlesLangRussian":{"errors":[],"ref":null,"value":false},"agreement":{"errors":[],"ref":null,"value":false},"email":{"errors":[],"ref":null,"value":true},"digest":{"errors":[],"ref":null,"value":true}}},"similarList":{"similarListIds":[],"similarListRefs":null},"ssr":{"error":null,"isDataLoaded":false,"isDataLoading":false,"isHydrationFailed":false,"isServer":false},"userHubsContribution":{"contributionRefs":{"hubRefs":{},"hubIds":{}}},"userInvites":{"availableInvites":0,"usedInvitesIds":[],"usedInvitesRefs":{},"usedInvitesPagesCount":0,"unusedInvitesIds":[],"unusedInvitesRefs":{},"unusedInvitesPagesCount":0},"users":{"authorRefs":{},"authorIds":{},"pagesCount":{},"authorProfiles":{},"userHubs":{},"userInvitations":{},"authorFollowers":{},"authorFollowed":{},"karmaStats":[],"statistics":null,"isLoading":false,"authorFollowersLoading":false,"authorFollowedLoading":false,"userHubsLoading":false,"userInvitationsLoading":false,"route":{}},"viewport":{"prevScrollY":{},"scrollY":0,"width":0},"tracker":{"items":{},"pagesCache":{},"markedViewedSilently":{},"markedRead":{},"unreadCounters":{"applications":null,"system":null,"mentions":null,"subscribers":null,"posts_and_comments":null},"unviewedCounters":{"applications":null,"system":null,"mentions":null,"subscribers":null,"posts_and_comments":null}}};(function(){var s;(s=document.currentScript||document.scripts[document.scripts.length-1]).parentNode.removeChild(s);}());</script>
<script src="https://assets.habr.com/habr-web/js/chunk-vendors.c2c3fc9a.js" defer></script><script src="https://assets.habr.com/habr-web/js/app.c0af73e7.js" defer></script>



    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    </script>
  
  <script type="text/javascript" >
    (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
    m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
    (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

    ym(24049213, "init", {
      defer:true,
      trackLinks:true,
      accurateTrackBounce:true,
      webvisor:false,
    });
  </script>
  <noscript>
    <div>
      <img src="https://mc.yandex.ru/watch/24049213" style="position:absolute; left:-9999px;" alt="" />
    </div>
  </noscript>
  
    <script type="text/javascript">
      window.addEventListener('load', function () {
        setTimeout(() => {
          const img = new Image();
          img.src = 'https://vk.com/rtrg?p=VK-RTRG-421343-57vKE';
        }, 0);
      });
    </script>
  
</body>
</html>
