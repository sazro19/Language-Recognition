<!DOCTYPE html>
<html lang="ru" data-vue-meta="%7B%22lang%22:%7B%22ssr%22:%22ru%22%7D%7D">
<head >
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0,viewport-fit=cover">
  <title>Многозадачная модель T5 для русского языка / Хабр</title>
  <style>
    /* cyrillic-ext */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveSxf6TF0.woff2) format('woff2');
      unicode-range: U+0460-052F, U+1C80-1C88, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;
    }

    /* cyrillic */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveQhf6TF0.woff2) format('woff2');
      unicode-range: U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;
    }

    /* latin-ext */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveSBf6TF0.woff2) format('woff2');
      unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
    }

    /* latin */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveRhf6.woff2) format('woff2');
      unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
    }
  </style>
  <link rel="preload" href="https://assets.habr.com/habr-web/css/chunk-vendors.e7843cc0.css" as="style"><link rel="preload" href="https://assets.habr.com/habr-web/js/chunk-vendors.c2c3fc9a.js" as="script"><link rel="preload" href="https://assets.habr.com/habr-web/css/app.02ee25a4.css" as="style"><link rel="preload" href="https://assets.habr.com/habr-web/js/app.c0af73e7.js" as="script">
  <link rel="stylesheet" href="https://assets.habr.com/habr-web/css/chunk-vendors.e7843cc0.css"><link rel="stylesheet" href="https://assets.habr.com/habr-web/css/app.02ee25a4.css">
  <script>window.i18nFetch = new Promise((res, rej) => {
          const xhr = new XMLHttpRequest();
          xhr.open('GET', '/js/i18n/ru-compiled.85eb77f0b17c8235e7b64b9f81ea5ec2.json');
          xhr.responseType = 'json';
          xhr.onload = function(e) {
            if (this.status === 200) {
              res({ru: xhr.response});
            } else {
              rej(e);
            }
          };
          xhr.send();
        });</script>
  
  <script data-vue-meta="ssr" src="/js/ads.js" onload="window['zhY4i4nJ9K'] = true" data-vmid="checkad"></script><script data-vue-meta="ssr" type="application/ld+json" data-vmid="ldjson-schema">{"@context":"http:\/\/schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/habr.com\/ru\/post\/581932\/"},"headline":"Многозадачная модель T5 для русского языка","datePublished":"2021-10-06T16:28:47+03:00","dateModified":"2021-10-06T17:23:26+03:00","author":{"@type":"Person","name":"Давид Дале"},"publisher":{"@type":"Organization","name":"Habr","logo":{"@type":"ImageObject","url":"https:\/\/habrastorage.org\/webt\/a_\/lk\/9m\/a_lk9mjkccjox-zccjrpfolmkmq.png"}},"description":"Модель T5 &ndash; это нейросеть, которая уже обучена хорошо понимать и генерировать текст, и которую можно дообучить на собственную задачу, будь то перевод, суммаризац...","url":"https:\/\/habr.com\/ru\/post\/581932\/#post-content-body","about":["h_python","h_sw","h_programming","h_machine_learning","h_natural_language_processing","f_develop"],"image":["https:\/\/habr.com\/share\/publication\/581932\/59d0d23c6b4cc81ed0b2955f5e9842c6\/","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/63b\/74b\/7aa\/63b74b7aac04b5b1f923aa9b1f0cc58e.png"]}</script>
  <script src="//www.googletagservices.com/tag/js/gpt.js" async></script>
  <style>.grecaptcha-badge{visibility: hidden;}</style>
  <meta name="habr-version" content="2.49.0">
  
  <meta data-vue-meta="ssr" property="fb:app_id" content="444736788986613"><meta data-vue-meta="ssr" property="fb:pages" content="472597926099084"><meta data-vue-meta="ssr" name="twitter:card" content="summary_large_image"><meta data-vue-meta="ssr" name="twitter:site" content="@habr_eng"><meta data-vue-meta="ssr" property="og:title" content="Многозадачная модель T5 для русского языка" data-vmid="og:title"><meta data-vue-meta="ssr" name="twitter:title" content="Многозадачная модель T5 для русского языка" data-vmid="twitter:title"><meta data-vue-meta="ssr" name="aiturec:title" content="Многозадачная модель T5 для русского языка" data-vmid="aiturec:title"><meta data-vue-meta="ssr" name="description" content="Модель T5 – это нейросеть, которая уже обучена хорошо понимать и генерировать текст, и которую можно дообучить на собственную задачу, будь то перевод, суммаризация текстов, или генерация ответа..." data-vmid="description"><meta data-vue-meta="ssr" itemprop="description" content="Модель T5 – это нейросеть, которая уже обучена хорошо понимать и генерировать текст, и которую можно дообучить на собственную задачу, будь то перевод, суммаризация текстов, или генерация ответа..." data-vmid="description:itemprop"><meta data-vue-meta="ssr" property="og:description" content="Модель T5 – это нейросеть, которая уже обучена хорошо понимать и генерировать текст, и которую можно дообучить на собственную задачу, будь то перевод, суммаризация текстов, или генерация ответа..." data-vmid="og:description"><meta data-vue-meta="ssr" name="twitter:description" content="Модель T5 – это нейросеть, которая уже обучена хорошо понимать и генерировать текст, и которую можно дообучить на собственную задачу, будь то перевод, суммаризация текстов, или генерация ответа..." data-vmid="twitter:description"><meta data-vue-meta="ssr" property="aiturec:description" content="Модель T5 – это нейросеть, которая уже обучена хорошо понимать и генерировать текст, и которую можно дообучить на собственную задачу, будь то перевод, суммаризация текстов, или генерация ответа..." data-vmid="aiturec:description"><meta data-vue-meta="ssr" itemprop="image" content="https://habrastorage.org/getpro/habr/upload_files/63b/74b/7aa/63b74b7aac04b5b1f923aa9b1f0cc58e.png" data-vmid="image:itemprop"><meta data-vue-meta="ssr" property="og:image" content="https://habrastorage.org/getpro/habr/upload_files/63b/74b/7aa/63b74b7aac04b5b1f923aa9b1f0cc58e.png" data-vmid="og:image"><meta data-vue-meta="ssr" property="aiturec:image" content="https://habrastorage.org/getpro/habr/upload_files/63b/74b/7aa/63b74b7aac04b5b1f923aa9b1f0cc58e.png" data-vmid="aiturec:image"><meta data-vue-meta="ssr" name="twitter:image" content="https://habrastorage.org/getpro/habr/upload_files/63b/74b/7aa/63b74b7aac04b5b1f923aa9b1f0cc58e.png" data-vmid="twitter:image"><meta data-vue-meta="ssr" property="vk:image" content="https://habrastorage.org/getpro/habr/upload_files/63b/74b/7aa/63b74b7aac04b5b1f923aa9b1f0cc58e.png" data-vmid="vk:image"><meta data-vue-meta="ssr" property="aiturec:item_id" content="581932" data-vmid="aiturec:item_id"><meta data-vue-meta="ssr" property="aiturec:datetime" content="2021-10-06T13:28:47.000Z" data-vmid="aiturec:datetime"><meta data-vue-meta="ssr" property="og:type" content="article" data-vmid="og:type"><meta data-vue-meta="ssr" property="og:locale" content="ru_RU" data-vmid="og:locale"><meta data-vue-meta="ssr" property="og:image:width" content="1200" data-vmid="og:image:width"><meta data-vue-meta="ssr" property="og:image:height" content="630" data-vmid="og:image:height">
  <link data-vue-meta="ssr" href="https://habr.com/ru/rss/post/581932/?fl=ru" type="application/rss+xml" title="" rel="alternate" name="rss"><link data-vue-meta="ssr" href="https://habr.com/ru/post/581932/" rel="canonical" data-vmid="canonical"><link data-vue-meta="ssr" data-vmid="hreflang"><link data-vue-meta="ssr" image_src="image" href="https://habrastorage.org/getpro/habr/upload_files/63b/74b/7aa/63b74b7aac04b5b1f923aa9b1f0cc58e.png" data-vmid="image:href"><link data-vue-meta="ssr" rel="amphtml" href="https://habr.com/ru/amp/post/581932/">
  <meta name="apple-mobile-web-app-status-bar-style" content="#303b44">
  <meta name="msapplication-TileColor" content="#629FBC">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="mobile-web-app-capable" content="yes">
  <link
    rel="shortcut icon"
    type="image/png"
    sizes="16x16"
    href="https://assets.habr.com/habr-web/img/favicons/favicon-16.png"
  >
  <link
    rel="shortcut icon"
    type="image/png"
    sizes="32x32"
    href="https://assets.habr.com/habr-web/img/favicons/favicon-32.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="76x76"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-76.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="120x120"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-120.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="152x152"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-152.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="180x180"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-180.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="256x256"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-256.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1136x640.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2436x1125.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1792x828.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_828x1792.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1334x750.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1242x2668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2208x1242.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1125x2436.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1242x2208.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2732x2048.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2688x1242.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2224x1668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_750x1334.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2048x2732.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2388x1668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1668x2224.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_640x1136.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1668x2388.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2048x1536.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1536x2048.png"
  >
  <link
    rel="mask-icon"
    color="#77a2b6"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-120.svg"
  >
  <link
    crossorigin="use-credentials"
    href="/manifest.webmanifest"
    rel="manifest"
  >
</head>
<body>


<div id="app" data-server-rendered="true" data-async-called="true"><div class="tm-layout__wrapper"><!----> <div></div> <!----> <header class="tm-header"><div class="tm-page-width"><div class="tm-header__container"><!----> <span class="tm-header__logo-wrap"><a href="/ru/" class="tm-header__logo tm-header__logo_ru"><svg height="16" width="16" class="tm-svg-img tm-header__icon"><title>Хабр</title> <use xlink:href="/img/habr-logo-ru.svg#logo"></use></svg></a> <span class="tm-header__beta-sign" style="display:none;">β</span></span> <div class="tm-dropdown tm-header__projects"><div class="tm-dropdown__head"><button class="tm-header__dropdown-toggle"><svg height="16" width="16" class="tm-svg-img tm-header__icon tm-header__icon_dropdown"><title>Открыть список</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#arrow-down"></use></svg></button></div> <!----></div> <a href="/ru/sandbox/start/" class="tm-header__become-author-btn">
            Как стать автором
          </a> <div class="tm-feature tm-header__feature tm-feature_variant-inline"><!----></div> <!----> <!----></div></div></header> <div class="tm-layout"><div class="tm-page-progress-bar"></div> <div data-menu-sticky="true" class="tm-base-layout__header tm-base-layout__header_is-sticky"><div class="tm-page-width"><div class="tm-base-layout__header-wrapper"><div class="tm-main-menu"><div class="tm-main-menu__section"><nav class="tm-main-menu__section-content"><!----> <a href="/ru/all/" class="tm-main-menu__item">
        Все потоки
      </a> <a href="/ru/flows/develop/" class="tm-main-menu__item">
          Разработка
        </a><a href="/ru/flows/admin/" class="tm-main-menu__item">
          Администрирование
        </a><a href="/ru/flows/design/" class="tm-main-menu__item">
          Дизайн
        </a><a href="/ru/flows/management/" class="tm-main-menu__item">
          Менеджмент
        </a><a href="/ru/flows/marketing/" class="tm-main-menu__item">
          Маркетинг
        </a><a href="/ru/flows/popsci/" class="tm-main-menu__item">
          Научпоп
        </a></nav></div></div> <div class="tm-header-user-menu tm-base-layout__user-menu"><a href="/ru/search/" class="tm-header-user-menu__item tm-header-user-menu__search"><svg height="24" width="24" class="tm-svg-img tm-header-user-menu__icon tm-header-user-menu__icon_search tm-header-user-menu__icon_dark"><title>Поиск</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#search"></use></svg></a> <!----> <!----> <!----> <div class="tm-header-user-menu__item tm-header-user-menu__user_desktop"><div class="tm-dropdown"><div class="tm-dropdown__head"><svg height="24" width="24" data-test-id="menu-toggle-guest" class="tm-svg-img tm-header-user-menu__icon"><title>Профиль</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#header-user"></use></svg> <!----></div> <!----></div> <!----></div> <!----></div></div></div></div> <!----> <div class="tm-page-width"></div> <main class="tm-layout__container"><div hl="ru" data-async-called="true" class="tm-page"><div class="tm-page-width"><!----> <div class="tm-page__wrapper"><div class="tm-page__main tm-page__main_has-sidebar"><div class="pull-down"><div class="pull-down__header" style="height:0px;"><div class="pull-down__content" style="bottom:10px;"><svg height="24" width="24" class="tm-svg-img pull-down__arrow"><title>Обновить</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#pull-arrow"></use></svg></div></div> <div class="tm-article-presenter"> <div class="tm-article-presenter__body"><div class="tm-misprint-area"><div class="tm-misprint-area__wrapper"><article class="tm-article-presenter__content tm-article-presenter__content_narrow"><div class="tm-article-presenter__header"> <div class="tm-article-snippet tm-article-presenter__snippet"><div class="tm-article-snippet__meta-container"><div class="tm-article-snippet__meta"><span class="tm-user-info tm-article-snippet__author"><a href="/ru/users/cointegrated/" title="cointegrated" class="tm-user-info__userpic"><div class="tm-entity-image"><img alt="" height="24" loading="lazy" src="//habrastorage.org/r/w32/getpro/habr/avatars/0c9/340/334/0c934033455e38414b615e309d337a3d.jpg" width="24" class="tm-entity-image__pic"></div></a> <span class="tm-user-info__user"><a href="/ru/users/cointegrated/" class="tm-user-info__username">
      cointegrated
    </a> </span></span> <span class="tm-article-snippet__datetime-published"><time datetime="2021-10-06T13:28:47.000Z" title="2021-10-06, 16:28">6  октября   в 16:28</time></span></div> <!----></div> <h1 lang="ru" class="tm-article-snippet__title tm-article-snippet__title_h1"><span>Многозадачная модель T5 для русского языка</span></h1> <div class="tm-article-snippet__hubs"><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/python/" class="tm-article-snippet__hubs-item-link"><span>Python</span> <span title="Профильный хаб" class="tm-article-snippet__profiled-hub">*</span></a></span><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/sw/" class="tm-article-snippet__hubs-item-link"><span>Семантика</span> <span title="Профильный хаб" class="tm-article-snippet__profiled-hub">*</span></a></span><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/programming/" class="tm-article-snippet__hubs-item-link"><span>Программирование</span> <span title="Профильный хаб" class="tm-article-snippet__profiled-hub">*</span></a></span><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/machine_learning/" class="tm-article-snippet__hubs-item-link"><span>Машинное обучение</span> <span title="Профильный хаб" class="tm-article-snippet__profiled-hub">*</span></a></span><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/natural_language_processing/" class="tm-article-snippet__hubs-item-link"><span>Natural Language Processing</span> <span title="Профильный хаб" class="tm-article-snippet__profiled-hub">*</span></a></span></div> <!----> <!----> <!----></div></div> <!----> <div data-gallery-root="" lang="ru" class="tm-article-body"><div id="post-content-body" class="article-formatted-body article-formatted-body_version-2"><div xmlns="http://www.w3.org/1999/xhtml"><p>Модель T5 – это нейросеть, которая уже обучена хорошо понимать и генерировать текст, и которую можно дообучить на собственную задачу, будь то перевод, суммаризация текстов, или генерация ответа чат-бота. </p><p>В этом посте я рассказываю про <a href="https://huggingface.co/cointegrated/rut5-base-multitask" rel="noopener noreferrer nofollow">первую многозадачную модель T5 для русского языка</a> и показываю, как её можно обучить на новой задаче.</p><figure class="full-width "><img src="/img/image-loader.svg" alt="Русскоязычная модель T5 худо-бедно решает десяток разных задач" title="Русскоязычная модель T5 худо-бедно решает десяток разных задач" height="720" data-src="https://habrastorage.org/getpro/habr/upload_files/63b/74b/7aa/63b74b7aac04b5b1f923aa9b1f0cc58e.png" data-width="1280"/><figcaption>Русскоязычная модель T5 худо-бедно решает десяток разных задач</figcaption></figure><h3>Зачем нужна русскоязычная T5</h3><p>T5 – нейросетевая модель для понимания и генерации текста. Изобрели её <a href="https://arxiv.org/abs/1910.10683" rel="noopener noreferrer nofollow">в работе от Google</a> два года назад, и расшифровывается это название как <em>text-to-text transfer transformer</em>. <a href="https://arxiv.org/abs/1706.03762" rel="noopener noreferrer nofollow">Трансформер </a>– это архитектура нейросетей, позволяющая извлекать из текста довольно объёмную информацию. Благодаря этой архитектуре модели типа BERT круто понимают тексты, а модели типа GPT весьма правдоподобно их генерируют. <em>Text-to-text</em> означает, что модель T5 принимает на вход тексты и "читает" их энкодером (как BERT), а потом "пишет" декодером новые тексты и отдаёт на выход. Слово <em>transfer</em> говорит о цели этой модели: она предобучалась восстанавливать пропущенные фрагменты текста, но при желании её можно дообучить на новые, более полезные задачи: перевод, перефразирование, суммаризация текстов, генерация диалоговых ответов, и т.п.</p><p>Гугл выпустил две версии T5: <a href="https://huggingface.co/t5-base" rel="noopener noreferrer nofollow">первая </a>понимает только английский язык, зато дообучалась на 24 разных задачах, а <a href="https://huggingface.co/google/mt5-base" rel="noopener noreferrer nofollow">вторая </a>понимает 101 язык (включая русский), но умеет только заполнять пропуски в тексте. Поэтому я решил сначала ужать мультиязычную модель T5 (mT5) до двух языков: русского и английского, выкинув ненужные токены из её словаря и соответствующие строки из матриц входных и выходных эмбеддингов. Процесс подробно описан <a href="https://towardsdatascience.com/how-to-adapt-a-multilingual-t5-model-for-a-single-language-b9f94f3d9c90" rel="noopener noreferrer nofollow">в этом посте</a>, а в результате модель "похудела" с 2.2 до 0.9 ГБ, а значит, стала более удобной для применения. Эту уменьшенная модель я выложил под именем <a href="https://huggingface.co/cointegrated/rut5-base" rel="noopener noreferrer nofollow">cointegrated/rut5-base</a>. А дальше я пошёл по пути Google и дообучил свою русскую T5 решать одновременно несколько разных русских и англоязычных задач.</p><h2>Многозадачная модель</h2><p>Для каждой задачи, как и в гугловской статье, я использовал свой префикс, который надо писать, отделив символом <code>|</code>, перед входным текстом. Сами задачи такие:</p><p><strong>Перевод</strong> (префикс <code>translate ru-en</code> или <code>translate en-ru</code>). Обучал на датасете <a href="https://huggingface.co/datasets/opus_wikipedia" rel="noopener noreferrer nofollow">opus_wikipedia</a>.</p><pre><code class="python">print(generate(
    'translate ru-en | Каждый охотник желает знать, где сидит фазан.'))
# Each hunter wants to know, where he is.

print(generate(
    'translate en-ru | Every hunter wants to know where the duck is.'))
# Все охотники хотят знать, где находится птица.</code></pre><p><strong>Перефразирование</strong> (<code>paraphrase</code>). Обучал на датасете <a href="https://huggingface.co/datasets/tapaco" rel="noopener noreferrer nofollow">tapaco</a>. </p><pre><code class="python">print(generate(
    'paraphrase | Каждый охотник желает знать, где сидит фазан.', 
    encoder_no_repeat_ngram_size=1, repetition_penalty=0.5, 
    no_repeat_ngram_size=1
))
# В любом случае каждый рыбак мечтает познакомиться со своей фермой</code></pre><p><strong>Заполнение пропусков в тексте</strong> (<code>fill</code>). Пропуски можно обозначать как <code>___</code> или  <code>_3_</code>, где <code>3</code>  – примерное количество слов, которые надо вставить. Обучал на корпусе ru_web-public_2019_1M из <a href="https://wortschatz.uni-leipzig.de/en/download/Russian" rel="noopener noreferrer nofollow">Лейпцигской коллекции</a>.</p><pre><code class="python">print(generate('fill | Каждый охотник _3_, где сидит фазан.'))
# смотрит на озеро</code></pre><p><strong>Восстановление текста</strong> из зашумлённого мешка слов (<code>assemble</code>). Обучал также на Лейпцигском корпусе.</p><pre><code class="python">print(generate('assemble | охотник каждый знать фазан сидит'))
# Каждый охотник знает, что фазан сидит.</code></pre><p><strong>Упрощение</strong> текстов (<code>simplify</code>). Обучал на данных дорожки <a href="https://github.com/dialogue-evaluation/RuSimpleSentEval" rel="noopener noreferrer nofollow">RuSimpleSentEval</a>.</p><pre><code class="python">print(generate(
    'simplify | Местным продуктом-специалитетом с защищённым ' \
    'географическим наименованием по происхождению считается ' \
    'люнебургский степной барашек.', max_length=32))
# Местным продуктом-специалитетом считается люнебургский степной барашек.</code></pre><p><strong>Диалоговый ответ</strong> (<code>reply</code> для ответов в стиле художественной литературы, обученных на <a href="https://github.com/Koziev/NLP_Datasets/blob/master/Conversations/Data/ru.conversations.txt" rel="noopener noreferrer nofollow">корпусе Козиева</a>, и  <code>answer</code> для ответов, обученных на otvet.mail.ru). </p><pre><code class="python">print(generate('reply | Помогите мне закадрить девушку'))
# Что я хочу?

print(generate('answer | Помогите мне закадрить девушку'))
# я хочу познакомиться с девушкой!!!!!!!!</code></pre><p><strong>Ответ на вопросы по тексту</strong> (<code>comprehend</code>). Обучал на датасете SberQUAD.</p><pre><code class="python">print(generate(
    'comprehend | На фоне земельного конфликта между владельцами овец и ' \
    'ранчеро разворачивается история любви овцевода Моргана Лейна, ' \
    'прибывшего в США из Австралии, и Марии Синглетон, владелицы ' \
    'богатого скотоводческого ранчо. Вопрос: откуда приехал Морган?'))
# из Австралии</code></pre><p><strong>Задавание вопросов по тексту</strong> (<code>ask</code>), обучал также на SberQUAD.</p><pre><code class="python">print(generate(
    'ask | На фоне земельного конфликта между владельцами овец и ' \
    'ранчеро разворачивается история любви овцевода Моргана Лейна, ' \
    'прибывшего в США из Австралии, и Марии Синглетон, владелицы ' \
    'богатого скотоводческого ранчо.', max_length=32))
# Что разворачивается на фоне земельного конфликта 
# между владельцами овец и ранчеро?ро?</code></pre><p><strong>Генерация заголовка</strong> к новостной статье (<code>headline</code>). Обучал на <a href="https://huggingface.co/IlyaGusev/rubert_telegram_headlines" rel="noopener noreferrer nofollow">датасете Ильи Гусева</a> по мотивам соревнования Телеграма. </p><pre><code class="python">print(generate(
    'headline | На фоне земельного конфликта между владельцами овец ' \
    'и ранчеро разворачивается история любви овцевода Моргана Лейна, ' \
    'прибывшего в США из Австралии, и Марии Синглетон, владелицы ' \
    'богатого скотоводческого ранчо.', max_length=32))
# На фоне земельного конфликта разворачивается история любви 
# овцевода Моргана Лейна и Марии Синглетон</code></pre><p>Как же работает эта магическая функция <code>generate</code>? Стандартный <code>python</code> и код из <a href="https://huggingface.co/transformers/" rel="noopener noreferrer nofollow">transformers</a>, и дальше вы можете запускать любой из примеров выше. Попробовать это вы можете в <a href="https://colab.research.google.com/drive/1u1gNRuUQGUI15cSk9Kfm2yMlUDAcj4P9?usp=sharing" rel="noopener noreferrer nofollow">демо-блокноте</a>.</p><pre><code class="python"># !pip install transformers sentencepiece --quiet
import torch
from transformers import T5ForConditionalGeneration, T5Tokenizer
model_name = "cointegrated/rut5-base-multitask"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

def generate(text, **kwargs):
    inputs = tokenizer(text, return_tensors='pt')
    with torch.no_grad():
        hypotheses = model.generate(**inputs, num_beams=5, **kwargs)
    return tokenizer.decode(hypotheses[0], skip_special_tokens=True)</code></pre><p>Весь код, которым я дообучал модель, есть <a href="https://gist.github.com/avidale/4de1454bf41822dc862fddbd779d4cc6" rel="noopener noreferrer nofollow">в этом блокноте</a>; он не очень чистый, ибо я только экспериментировал, и содержит несколько задач, не вошедших в опубликованную версию модели. А сама модель выложена в каталог Huggingface: <a href="https://huggingface.co/cointegrated/rut5-base-multitask" rel="noopener noreferrer nofollow">cointegrated/rut5-base-multitask</a>.</p><h2>Как обучать T5 на собственных данных</h2><p>Текущая версия модели задумывалась как демонстрация возможностей предобученной seq2seq модели и как болванка для последующего дообучения. Поэтому я особо не возился ни с подбором гиперпараметров, ни с качеством датасетов. Следовательно, если более внимательно поработать с датасетом и гиперпараметрами и дообучить модель на какую-то одну задачу, работать она будет ещё лучше. Например, так я уже дообучал её <a href="https://habr.com/ru/post/564916/" rel="noopener noreferrer nofollow">на задаче перефразирования</a>. А здесь я покажу, как обучить модель отвечать на вопросы из кроссвордов, используя фреймворки <a href="https://huggingface.co/transformers/" rel="noopener noreferrer nofollow">transformers </a>и <a href="https://pytorch.org" rel="noopener noreferrer nofollow">pytorch</a> (при необходимости, модель можно потом сконвертировать в tensorflow или другой формат). Полный код примера есть всё <a href="https://colab.research.google.com/drive/1u1gNRuUQGUI15cSk9Kfm2yMlUDAcj4P9?usp=sharing" rel="noopener noreferrer nofollow">в том же демо блокноте</a>. </p><p>Инициализировать модель можно многозадачной версией. Тут, как и везде в библиотеке <code>transformers</code>, <code>model</code> – это сама нейросеть, а <code>tokenizer</code> – это часть модели, ответственная за сопоставление текстов со словарём: разбиение текстов на числовые токены, и сбор текстов из токенов обратно. <code>optimizer</code> – это объект, отвечающий за градиентный спуск; он нужен только на время обучения.</p><pre><code class="python">import torch 
from transformers import T5ForConditionalGeneration, T5Tokenizer
raw_model = 'cointegrated/rut5-base-multitask' 
model = T5ForConditionalGeneration.from_pretrained(raw_model).cuda();
tokenizer = T5Tokenizer.from_pretrained(raw_model)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)</code></pre><p>Ниже  – полный код обучения модели.  Предполагается, что переменная <code>pairs</code>  – это список из пар, состоящих из вопроса и ответа. На GPU одна эпоха обучения на 75 тысячах примеров занимает около 15 минут.</p><pre><code class="python">from tqdm.auto import trange
import random
import numpy as np

batch_size = 16  # сколько примеров показываем модели за один шаг
report_steps = 200  # раз в сколько шагов печатаем результат
epochs = 3  # сколько раз мы покажем данные модели

model.train()
losses = []
for epoch in range(epochs):
    print('EPOCH', epoch)
    random.shuffle(pairs)
    for i in trange(0, int(len(pairs) / batch_size)):
        batch = pairs[i * batch_size: (i + 1) * batch_size]
        # кодируем вопрос и ответ 
        x = tokenizer([p[0] for p in batch], return_tensors='pt', padding=True).to(model.device)
        y = tokenizer([p[1] for p in batch], return_tensors='pt', padding=True).to(model.device)
        # -100 - специальное значение, позволяющее не учитывать токены
        y.input_ids[y.input_ids == 0] = -100
        # вычисляем функцию потерь
        loss = model(
            input_ids=x.input_ids,
            attention_mask=x.attention_mask,
            labels=y.input_ids,
            decoder_attention_mask=y.attention_mask,
            return_dict=True
        ).loss
        # делаем шаг градиентного спуска
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        # печатаем скользящее среднее значение функции потерь
        losses.append(loss.item())
        if i % report_steps == 0:
            print('step', i, 'loss', np.mean(losses[-report_steps:]))</code></pre><p>Три эпохи я выбрал от балды. Размер батча я выбрал опытным путём: максимальный, при котором хватает памяти на GPU. <code>learning_rate</code>, равный <code>1e-5</code>, я выставил, исходя из опыта: обычно при нём модель обучается не очень быстро, но качественно.</p><p>Код для генерации ответа обученной моделью весьма прост:</p><pre><code class="python">model.eval()

def answer(x, **kwargs):
    inputs = tokenizer(x, return_tensors='pt').to(model.device)
    with torch.no_grad():
        hypotheses = model.generate(**inputs, **kwargs)
    return tokenizer.decode(hypotheses[0], skip_special_tokens=True)</code></pre><p>Посмотрим, насколько хорошо модель выучила свою тренировочную выборку. </p><pre><code>Какое животное раньше называли камелопардом?
answer: Жираф
model:  акула
---
Грамматическая категория глагола, выражающая отношение действия к действительности (в лингвистике)
answer: наклонение
model:  действие
---
О чём поется в песне Greenday – «Wake Me Up When September Ends» (Разбуди меня, когда сентябрь кончится)?
answer: О смерти его отца
model:  о лете
---
Соседка Земли по Солнечной системе
answer: Венера
model:  Африка
---
Отношение размеров на чертеже, карте и т. п. к действительным размерам на местности, предмете
answer: масштаб
model:  пропорциональность</code></pre><p>Что ж, модель пытается, но часто "мажет". Возможно, стоит поучить её в течение ещё нескольких эпох. А теперь посмотрим, насколько хорошо модель справляется с ответами на вопросы, которые она не видела.  Кажется, довольно пристойно.</p><pre><code>Минерал, сульфид марганца
answer: алабандин
model:  сульфид
---
Где находится родина табака?
answer: Южная Америка
model:  Бразилия
---
Старинный русский головной убор с приподнятым вверх спереди и сзади околышем
answer: кораблик
model:  шнур
---
Почетное звание у тюрков и монголов, дававшееся за воинские подвиги
answer: батыр
model:  аким
---
Счетный прибор
answer: арифмометр
model:  таблица</code></pre><p>Обученную модель можно сохранить на диск, а потом, при желании, даже выложить на хаб Huggingface. Я выложил её под названием <a href="https://huggingface.co/cointegrated/rut5-base-quiz" rel="noopener noreferrer nofollow">cointegrated/rut5-base-quiz</a>. </p><pre><code class="python">new_model_name = 'rut5-base-quiz'  # название папки для сохранения
model.save_pretrained(new_model_name)
tokenizer.save_pretrained(new_model_name)</code></pre><h2>Вместо заключения</h2><p>Предобученные seq2seq модели – это здорово. Сейчас ими можно решать много разных задач NLP, а Google считает, что вообще чуть ли не все. Моя <a href="https://huggingface.co/cointegrated/rut5-base-multitask" rel="noopener noreferrer nofollow">моделька</a> показывает, что отчасти это уже верно и для русского языка.</p></div></div> <!----> <!----></div> <div class="tm-article-presenter__meta"><div class="tm-separated-list tm-article-presenter__meta-list"><span class="tm-separated-list__title">Теги:</span> <ul class="tm-separated-list__list"><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5BT5%5D" class="tm-tags-list__link">T5</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D1%82%D1%80%D0%B0%D0%BD%D1%81%D1%84%D0%BE%D1%80%D0%BC%D0%B5%D1%80%5D" class="tm-tags-list__link">трансформер</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D1%81%D0%B5%D1%82%D1%8C%5D" class="tm-tags-list__link">нейросеть</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bnatural%20language%20processing%5D" class="tm-tags-list__link">natural language processing</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bnlp%5D" class="tm-tags-list__link">nlp</a></li></ul></div> <div class="tm-separated-list tm-article-presenter__meta-list"><span class="tm-separated-list__title">Хабы:</span> <ul class="tm-separated-list__list"><li class="tm-separated-list__item"><a href="/ru/hub/python/" class="tm-hubs-list__link">
    Python
  </a></li><li class="tm-separated-list__item"><a href="/ru/hub/sw/" class="tm-hubs-list__link">
    Семантика
  </a></li><li class="tm-separated-list__item"><a href="/ru/hub/programming/" class="tm-hubs-list__link">
    Программирование
  </a></li><li class="tm-separated-list__item"><a href="/ru/hub/machine_learning/" class="tm-hubs-list__link">
    Машинное обучение
  </a></li><li class="tm-separated-list__item"><a href="/ru/hub/natural_language_processing/" class="tm-hubs-list__link">
    Natural Language Processing
  </a></li></ul></div></div></article></div> <!----></div> <div class="tm-article-sticky-panel"><div class="tm-data-icons tm-article-sticky-panel__icons"><div class="tm-article-rating tm-data-icons__item"><div class="tm-votes-meter tm-article-rating__votes-switcher"><svg height="16" width="16" class="tm-svg-img tm-votes-meter__icon tm-votes-meter__icon_medium"><title>Всего голосов 16: ↑16 и ↓0</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-rating"></use></svg> <span title="Всего голосов 16: ↑16 и ↓0" class="tm-votes-meter__value tm-votes-meter__value_positive tm-votes-meter__value_medium">+16</span></div> <DIV class="v-portal" style="display:none;"></DIV></div> <!----> <span title="Количество просмотров" class="tm-icon-counter tm-data-icons__item"><svg height="16" width="16" class="tm-svg-img tm-icon-counter__icon"><title>Просмотры</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-views"></use></svg> <span class="tm-icon-counter__value">2K</span></span> <button title="Добавить в закладки" type="button" class="bookmarks-button tm-data-icons__item"><span title="Добавить в закладки" class="tm-svg-icon__wrapper bookmarks-button__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Добавить в закладки</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-favorite"></use></svg></span> <span title="Количество пользователей, добавивших публикацию в закладки" class="bookmarks-button__counter">
    40
  </span></button> <!----> <div title="Поделиться" class="tm-sharing tm-data-icons__item"><button type="button" class="tm-sharing__button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="tm-sharing__icon"><path fill="currentColor" d="M10.33.275l9.047 7.572a.2.2 0 010 .306l-9.048 7.572a.2.2 0 01-.328-.153V11c-8 0-9.94 6-9.94 6S-1 5 10 5V.428a.2.2 0 01.328-.153z"></path></svg></button> <!----></div> <DIV class="v-portal" style="display:none;"></DIV></div> </div></div> <!----> <!----> <div class="tm-article-presenter__footer"><div class="tm-article-blocks"><!----> <section class="tm-block tm-block_spacing-bottom"><!----> <div class="tm-block__body tm-block__body_variant-balanced"><div class="tm-article-author"> <div class="tm-user-card tm-article-author__user-card tm-user-card_variant-article"><div class="tm-user-card__info-container"><div class="tm-user-card__header"><div class="tm-user-card__header-data"><a href="/ru/users/cointegrated/" class="tm-user-card__userpic tm-user-card__userpic_size-40"><div class="tm-entity-image"><img alt="" src="//habrastorage.org/getpro/habr/avatars/0c9/340/334/0c934033455e38414b615e309d337a3d.jpg" class="tm-entity-image__pic"></div></a> <div class="tm-user-card__meta"><div title=" 123 голоса " class="tm-karma tm-user-card__karma"><div class="tm-karma__votes tm-karma__votes_positive">
    103
  </div> <div class="tm-karma__text">
    Карма
  </div></div> <div title="Рейтинг пользователя" class="tm-rating tm-user-card__rating"><div class="tm-rating__header"> <div class="tm-rating__counter">40</div></div> <div class="tm-rating__text">
    Рейтинг
  </div></div></div></div></div> <div class="tm-user-card__info tm-user-card__info_variant-article"><div class="tm-user-card__title tm-user-card__title_variant-article"><span class="tm-user-card__name tm-user-card__name_variant-article">Давид Дале</span> <a href="/ru/users/cointegrated/" class="tm-user-card__nickname tm-user-card__nickname_variant-article">
          @cointegrated
        </a> <!----></div> <p class="tm-user-card__short-info tm-user-card__short-info_variant-article">Разработчик / Аналитик / Data Scientist / NLPшник</p></div></div> <div class="tm-user-card__buttons tm-user-card__buttons_variant-article"><!----> <!----> <button type="submit" class="tm-user-card__button btn btn_transparent btn_small">
      Задонатить
    </button> <!----> <!----></div></div> <div class="tm-article-author__user-contacts"><a href="https://daviddale.ru" rel="noopener" target="_blank" class="tm-article-author__contact">
      Сайт
    </a><a href="https://facebook.com/dale.david.fluteman" rel="noopener" target="_blank" class="tm-article-author__contact">
      Facebook
    </a><a href="https://vk.com/dale.david" rel="noopener" target="_blank" class="tm-article-author__contact">
      ВКонтакте
    </a><a href="https://github.com/avidale/" rel="noopener" target="_blank" class="tm-article-author__contact">
      Github
    </a><a href="https://medium.com/@cointegrated" rel="noopener" target="_blank" class="tm-article-author__contact">
      Medium
    </a></div></div> <DIV class="v-portal" style="display:none;"></DIV></div> <!----></section> <div class="tm-article-blocks__comments"><div class="tm-article-page-comments"><div class="tm-article-comments-counter-link tm-article-comments-counter-button"><a href="/ru/post/581932/comments/" class="tm-article-comments-counter-link__link tm-article-comments-counter-link__link_button-style"><svg height="16" width="16" class="tm-svg-img tm-article-comments-counter-link__icon tm-article-comments-counter-link__icon_contrasted"><title>Комментарии</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-comments"></use></svg> <span class="tm-article-comments-counter-link__value tm-article-comments-counter-link__value_contrasted">
       Комментарии 4 
    </span></a> <!----></div></div></div> <div class="tm-ad-banner__container tm-page-article__banner"><!----> <div id="articleBottomBanner" class="tm-ad-banner"></div></div> <!----> <!----> <!----> <!----> </div></div></div></div></div> <div class="tm-page__sidebar"><div class="tm-layout-sidebar"><div class="tm-layout-sidebar__ads tm-layout-sidebar__ads_initial"><div class="tm-ad-banner__container tm-layout-sidebar__banner"><!----> <div id="sidebarBanner" class="tm-ad-banner"></div></div></div> <div class="tm-sexy-sidebar tm-sexy-sidebar_initial" style="margin-top:0px;"><!----> <!----></div></div></div></div></div></div></main> <!----></div> <div class="tm-footer-menu"><div class="tm-page-width"><div class="tm-footer-menu__container"><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Ваш аккаунт
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/kek/v1/auth/habrahabr/?back=/ru/post/581932/&amp;hl=ru" rel="nofollow" target="_self">
                Войти
              </a></li><li class="tm-footer-menu__list-item"><a href="/kek/v1/auth/habrahabr-register/?back=/ru/post/581932/&amp;hl=ru" rel="nofollow" target="_self">
                Регистрация
              </a></li></ul></div></div><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Разделы
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/ru/" class="footer-menu__item-link router-link-active">
                Публикации
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/news/" class="footer-menu__item-link">
                Новости
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/hubs/" class="footer-menu__item-link">
                Хабы
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/companies/" class="footer-menu__item-link">
                Компании
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/users/" class="footer-menu__item-link">
                Авторы
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/sandbox/" class="footer-menu__item-link">
                Песочница
              </a></li></ul></div></div><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Информация
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/ru/docs/help/" class="footer-menu__item-link">
                Устройство сайта
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/authors/codex/" class="footer-menu__item-link">
                Для авторов
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/companies/corpblogs/" class="footer-menu__item-link">
                Для компаний
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/docs/transparency/" class="footer-menu__item-link">
                Документы
              </a></li><li class="tm-footer-menu__list-item"><a href="https://account.habr.com/info/agreement" target="_blank">
                Соглашение
              </a></li><li class="tm-footer-menu__list-item"><a href="https://account.habr.com/info/confidential/" target="_blank">
                Конфиденциальность
              </a></li></ul></div></div><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Услуги
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="https://docs.google.com/presentation/d/e/2PACX-1vQLwRfQmXibiUlWaRg-BAc38s7oM3lJiaPju7qmdJsp8ysIvZ_G-Npem0njJLMozE2bPHMpDqiI5hhy/pub?start=false&amp;loop=false&amp;delayms=60000&amp;slide=id.g91a03369cd_4_297" target="_blank">
                Реклама
              </a></li><li class="tm-footer-menu__list-item"><a href="https://habrastorage.org/storage/stuff/habr/service_price.pdf" target="_blank">
                Тарифы
              </a></li><li class="tm-footer-menu__list-item"><a href="https://docs.google.com/presentation/d/e/2PACX-1vQJJds8-Di7BQSP_guHxICN7woVYoN5NP_22ra-BIo4bqnTT9FR6fB-Ku2P0AoRpX0Ds-LRkDeAoD8F/pub?start=false&amp;loop=false&amp;delayms=60000" target="_blank">
                Контент
              </a></li><li class="tm-footer-menu__list-item"><a href="https://tmtm.timepad.ru/" target="_blank">
                Семинары
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/megaprojects/" class="footer-menu__item-link">
                Мегапроекты
              </a></li></ul></div></div></div></div></div> <div class="tm-footer"><div class="tm-page-width"><div class="tm-footer__container"><!----> <div class="tm-footer__social"><a href="https://www.facebook.com/habrahabr.ru" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Facebook</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-facebook"></use></svg></a><a href="https://twitter.com/habr_com" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Twitter</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-twitter"></use></svg></a><a href="https://vk.com/habr" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>VK</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-vkontakte"></use></svg></a><a href="https://telegram.me/habr_com" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Telegram</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-telegram"></use></svg></a><a href="https://www.youtube.com/channel/UCd_sTwKqVrweTt4oAKY5y4w" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Youtube</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-youtube"></use></svg></a><a href="https://zen.yandex.ru/habr" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Яндекс Дзен</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-zen"></use></svg></a></div> <DIV class="v-portal" style="display:none;"></DIV> <button class="tm-footer__link"><!---->
        Настройка языка
      </button> <a href="/ru/about" class="tm-footer__link">
        О сайте
      </a> <a href="/ru/feedback/" class="tm-footer__link">
        Техническая поддержка
      </a> <!----> <a href="/berserk-mode-nope" class="tm-footer__link">
        Вернуться на старую версию
      </a> <div class="tm-footer-copyright"><span class="tm-copyright"><span class="tm-copyright__years">© 2006–2021 </span> <span class="tm-copyright__name">«<a href="https://company.habr.com/" rel="noopener" target="_blank" class="tm-copyright__link">Habr</a>»</span></span></div></div></div></div> <!----> <!----></div> <div class="vue-portal-target"></div></div>
<script>window.__INITIAL_STATE__={"adblock":{"hasAcceptableAdsFilter":false,"hasAdblock":false},"articlesList":{"articlesList":{"581932":{"id":"581932","timePublished":"2021-10-06T13:28:47+00:00","isCorporative":false,"lang":"ru","titleHtml":"Многозадачная модель T5 для русского языка","leadData":{"textHtml":"\u003Cp\u003EМодель T5 – это нейросеть, которая уже обучена хорошо понимать и генерировать текст, и которую можно дообучить на собственную задачу, будь то перевод, суммаризация текстов, или генерация ответа чат-бота. \u003C\u002Fp\u003E\u003Cp\u003EВ этом посте я рассказываю про \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fcointegrated\u002Frut5-base-multitask\" rel=\"noopener noreferrer nofollow\"\u003Eпервую многозадачную модель T5 для русского языка\u003C\u002Fa\u003E и показываю, как её можно обучить на новой задаче.\u003C\u002Fp\u003E","imageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F63b\u002F74b\u002F7aa\u002F63b74b7aac04b5b1f923aa9b1f0cc58e.png","buttonTextHtml":"Читать далее","image":{"url":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F63b\u002F74b\u002F7aa\u002F63b74b7aac04b5b1f923aa9b1f0cc58e.png","fit":"cover","positionY":0,"positionX":0}},"editorVersion":"2.0","postType":"article","postLabels":[],"author":{"scoreStats":{"score":103,"votesCount":123},"rating":40,"relatedData":null,"contacts":[{"title":"Сайт","url":"https:\u002F\u002Fdaviddale.ru","value":"https:\u002F\u002Fdaviddale.ru"},{"title":"Facebook","url":"https:\u002F\u002Ffacebook.com\u002Fdale.david.fluteman","value":"dale.david.fluteman"},{"title":"ВКонтакте","url":"https:\u002F\u002Fvk.com\u002Fdale.david","value":"dale.david"},{"title":"Github","url":"https:\u002F\u002Fgithub.com\u002Favidale\u002F","value":"avidale"},{"title":"Medium","url":"https:\u002F\u002Fmedium.com\u002F@cointegrated","value":"cointegrated"}],"authorContacts":[{"title":"Сайт","url":"https:\u002F\u002Fdaviddale.ru","value":"https:\u002F\u002Fdaviddale.ru"},{"title":"Facebook","url":"https:\u002F\u002Ffacebook.com\u002Fdale.david.fluteman","value":"dale.david.fluteman"},{"title":"ВКонтакте","url":"https:\u002F\u002Fvk.com\u002Fdale.david","value":"dale.david"},{"title":"Github","url":"https:\u002F\u002Fgithub.com\u002Favidale\u002F","value":"avidale"},{"title":"Medium","url":"https:\u002F\u002Fmedium.com\u002F@cointegrated","value":"cointegrated"}],"paymentDetails":{"paymentYandexMoney":"410011860612306","paymentPayPalMe":null,"paymentWebmoney":"212945711688"},"id":"1383398","alias":"cointegrated","fullname":"Давид Дале","avatarUrl":"\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Favatars\u002F0c9\u002F340\u002F334\u002F0c934033455e38414b615e309d337a3d.jpg","speciality":"Разработчик \u002F Аналитик \u002F Data Scientist \u002F NLPшник"},"statistics":{"commentsCount":4,"favoritesCount":40,"readingCount":1975,"score":16,"votesCount":16},"hubs":[{"relatedData":null,"id":"340","alias":"python","type":"collective","title":"Python","titleHtml":"Python","isProfiled":true},{"relatedData":null,"id":"345","alias":"sw","type":"collective","title":"Семантика","titleHtml":"Семантика","isProfiled":true},{"relatedData":null,"id":"359","alias":"programming","type":"collective","title":"Программирование","titleHtml":"Программирование","isProfiled":true},{"relatedData":null,"id":"19439","alias":"machine_learning","type":"collective","title":"Машинное обучение","titleHtml":"Машинное обучение","isProfiled":true},{"relatedData":null,"id":"22125","alias":"natural_language_processing","type":"collective","title":"Natural Language Processing","titleHtml":"Natural Language Processing","isProfiled":true}],"flows":[{"id":"1","alias":"develop","title":"Разработка"}],"relatedData":null,"textHtml":"\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\"\u003E\u003Cp\u003EМодель T5 – это нейросеть, которая уже обучена хорошо понимать и генерировать текст, и которую можно дообучить на собственную задачу, будь то перевод, суммаризация текстов, или генерация ответа чат-бота. \u003C\u002Fp\u003E\u003Cp\u003EВ этом посте я рассказываю про \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fcointegrated\u002Frut5-base-multitask\" rel=\"noopener noreferrer nofollow\"\u003Eпервую многозадачную модель T5 для русского языка\u003C\u002Fa\u003E и показываю, как её можно обучить на новой задаче.\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" alt=\"Русскоязычная модель T5 худо-бедно решает десяток разных задач\" title=\"Русскоязычная модель T5 худо-бедно решает десяток разных задач\" height=\"720\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F63b\u002F74b\u002F7aa\u002F63b74b7aac04b5b1f923aa9b1f0cc58e.png\" data-width=\"1280\"\u002F\u003E\u003Cfigcaption\u003EРусскоязычная модель T5 худо-бедно решает десяток разных задач\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003EЗачем нужна русскоязычная T5\u003C\u002Fh3\u003E\u003Cp\u003ET5 – нейросетевая модель для понимания и генерации текста. Изобрели её \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1910.10683\" rel=\"noopener noreferrer nofollow\"\u003Eв работе от Google\u003C\u002Fa\u003E два года назад, и расшифровывается это название как \u003Cem\u003Etext-to-text transfer transformer\u003C\u002Fem\u003E. \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1706.03762\" rel=\"noopener noreferrer nofollow\"\u003EТрансформер \u003C\u002Fa\u003E– это архитектура нейросетей, позволяющая извлекать из текста довольно объёмную информацию. Благодаря этой архитектуре модели типа BERT круто понимают тексты, а модели типа GPT весьма правдоподобно их генерируют. \u003Cem\u003EText-to-text\u003C\u002Fem\u003E означает, что модель T5 принимает на вход тексты и \"читает\" их энкодером (как BERT), а потом \"пишет\" декодером новые тексты и отдаёт на выход. Слово \u003Cem\u003Etransfer\u003C\u002Fem\u003E говорит о цели этой модели: она предобучалась восстанавливать пропущенные фрагменты текста, но при желании её можно дообучить на новые, более полезные задачи: перевод, перефразирование, суммаризация текстов, генерация диалоговых ответов, и т.п.\u003C\u002Fp\u003E\u003Cp\u003EГугл выпустил две версии T5: \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Ft5-base\" rel=\"noopener noreferrer nofollow\"\u003Eпервая \u003C\u002Fa\u003Eпонимает только английский язык, зато дообучалась на 24 разных задачах, а \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fgoogle\u002Fmt5-base\" rel=\"noopener noreferrer nofollow\"\u003Eвторая \u003C\u002Fa\u003Eпонимает 101 язык (включая русский), но умеет только заполнять пропуски в тексте. Поэтому я решил сначала ужать мультиязычную модель T5 (mT5) до двух языков: русского и английского, выкинув ненужные токены из её словаря и соответствующие строки из матриц входных и выходных эмбеддингов. Процесс подробно описан \u003Ca href=\"https:\u002F\u002Ftowardsdatascience.com\u002Fhow-to-adapt-a-multilingual-t5-model-for-a-single-language-b9f94f3d9c90\" rel=\"noopener noreferrer nofollow\"\u003Eв этом посте\u003C\u002Fa\u003E, а в результате модель \"похудела\" с 2.2 до 0.9 ГБ, а значит, стала более удобной для применения. Эту уменьшенная модель я выложил под именем \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fcointegrated\u002Frut5-base\" rel=\"noopener noreferrer nofollow\"\u003Ecointegrated\u002Frut5-base\u003C\u002Fa\u003E. А дальше я пошёл по пути Google и дообучил свою русскую T5 решать одновременно несколько разных русских и англоязычных задач.\u003C\u002Fp\u003E\u003Ch2\u003EМногозадачная модель\u003C\u002Fh2\u003E\u003Cp\u003EДля каждой задачи, как и в гугловской статье, я использовал свой префикс, который надо писать, отделив символом \u003Ccode\u003E|\u003C\u002Fcode\u003E, перед входным текстом. Сами задачи такие:\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EПеревод\u003C\u002Fstrong\u003E (префикс \u003Ccode\u003Etranslate ru-en\u003C\u002Fcode\u003E или \u003Ccode\u003Etranslate en-ru\u003C\u002Fcode\u003E). Обучал на датасете \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdatasets\u002Fopus_wikipedia\" rel=\"noopener noreferrer nofollow\"\u003Eopus_wikipedia\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Eprint(generate(\n    'translate ru-en | Каждый охотник желает знать, где сидит фазан.'))\n# Each hunter wants to know, where he is.\n\nprint(generate(\n    'translate en-ru | Every hunter wants to know where the duck is.'))\n# Все охотники хотят знать, где находится птица.\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003E\u003Cstrong\u003EПерефразирование\u003C\u002Fstrong\u003E (\u003Ccode\u003Eparaphrase\u003C\u002Fcode\u003E). Обучал на датасете \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdatasets\u002Ftapaco\" rel=\"noopener noreferrer nofollow\"\u003Etapaco\u003C\u002Fa\u003E. \u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Eprint(generate(\n    'paraphrase | Каждый охотник желает знать, где сидит фазан.', \n    encoder_no_repeat_ngram_size=1, repetition_penalty=0.5, \n    no_repeat_ngram_size=1\n))\n# В любом случае каждый рыбак мечтает познакомиться со своей фермой\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003E\u003Cstrong\u003EЗаполнение пропусков в тексте\u003C\u002Fstrong\u003E (\u003Ccode\u003Efill\u003C\u002Fcode\u003E). Пропуски можно обозначать как \u003Ccode\u003E___\u003C\u002Fcode\u003E или  \u003Ccode\u003E_3_\u003C\u002Fcode\u003E, где \u003Ccode\u003E3\u003C\u002Fcode\u003E  – примерное количество слов, которые надо вставить. Обучал на корпусе ru_web-public_2019_1M из \u003Ca href=\"https:\u002F\u002Fwortschatz.uni-leipzig.de\u002Fen\u002Fdownload\u002FRussian\" rel=\"noopener noreferrer nofollow\"\u003EЛейпцигской коллекции\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Eprint(generate('fill | Каждый охотник _3_, где сидит фазан.'))\n# смотрит на озеро\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003E\u003Cstrong\u003EВосстановление текста\u003C\u002Fstrong\u003E из зашумлённого мешка слов (\u003Ccode\u003Eassemble\u003C\u002Fcode\u003E). Обучал также на Лейпцигском корпусе.\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Eprint(generate('assemble | охотник каждый знать фазан сидит'))\n# Каждый охотник знает, что фазан сидит.\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003E\u003Cstrong\u003EУпрощение\u003C\u002Fstrong\u003E текстов (\u003Ccode\u003Esimplify\u003C\u002Fcode\u003E). Обучал на данных дорожки \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fdialogue-evaluation\u002FRuSimpleSentEval\" rel=\"noopener noreferrer nofollow\"\u003ERuSimpleSentEval\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Eprint(generate(\n    'simplify | Местным продуктом-специалитетом с защищённым ' \\\n    'географическим наименованием по происхождению считается ' \\\n    'люнебургский степной барашек.', max_length=32))\n# Местным продуктом-специалитетом считается люнебургский степной барашек.\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003E\u003Cstrong\u003EДиалоговый ответ\u003C\u002Fstrong\u003E (\u003Ccode\u003Ereply\u003C\u002Fcode\u003E для ответов в стиле художественной литературы, обученных на \u003Ca href=\"https:\u002F\u002Fgithub.com\u002FKoziev\u002FNLP_Datasets\u002Fblob\u002Fmaster\u002FConversations\u002FData\u002Fru.conversations.txt\" rel=\"noopener noreferrer nofollow\"\u003Eкорпусе Козиева\u003C\u002Fa\u003E, и  \u003Ccode\u003Eanswer\u003C\u002Fcode\u003E для ответов, обученных на otvet.mail.ru). \u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Eprint(generate('reply | Помогите мне закадрить девушку'))\n# Что я хочу?\n\nprint(generate('answer | Помогите мне закадрить девушку'))\n# я хочу познакомиться с девушкой!!!!!!!!\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003E\u003Cstrong\u003EОтвет на вопросы по тексту\u003C\u002Fstrong\u003E (\u003Ccode\u003Ecomprehend\u003C\u002Fcode\u003E). Обучал на датасете SberQUAD.\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Eprint(generate(\n    'comprehend | На фоне земельного конфликта между владельцами овец и ' \\\n    'ранчеро разворачивается история любви овцевода Моргана Лейна, ' \\\n    'прибывшего в США из Австралии, и Марии Синглетон, владелицы ' \\\n    'богатого скотоводческого ранчо. Вопрос: откуда приехал Морган?'))\n# из Австралии\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003E\u003Cstrong\u003EЗадавание вопросов по тексту\u003C\u002Fstrong\u003E (\u003Ccode\u003Eask\u003C\u002Fcode\u003E), обучал также на SberQUAD.\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Eprint(generate(\n    'ask | На фоне земельного конфликта между владельцами овец и ' \\\n    'ранчеро разворачивается история любви овцевода Моргана Лейна, ' \\\n    'прибывшего в США из Австралии, и Марии Синглетон, владелицы ' \\\n    'богатого скотоводческого ранчо.', max_length=32))\n# Что разворачивается на фоне земельного конфликта \n# между владельцами овец и ранчеро?ро?\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003E\u003Cstrong\u003EГенерация заголовка\u003C\u002Fstrong\u003E к новостной статье (\u003Ccode\u003Eheadline\u003C\u002Fcode\u003E). Обучал на \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002FIlyaGusev\u002Frubert_telegram_headlines\" rel=\"noopener noreferrer nofollow\"\u003Eдатасете Ильи Гусева\u003C\u002Fa\u003E по мотивам соревнования Телеграма. \u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Eprint(generate(\n    'headline | На фоне земельного конфликта между владельцами овец ' \\\n    'и ранчеро разворачивается история любви овцевода Моргана Лейна, ' \\\n    'прибывшего в США из Австралии, и Марии Синглетон, владелицы ' \\\n    'богатого скотоводческого ранчо.', max_length=32))\n# На фоне земельного конфликта разворачивается история любви \n# овцевода Моргана Лейна и Марии Синглетон\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EКак же работает эта магическая функция \u003Ccode\u003Egenerate\u003C\u002Fcode\u003E? Стандартный \u003Ccode\u003Epython\u003C\u002Fcode\u003E и код из \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Ftransformers\u002F\" rel=\"noopener noreferrer nofollow\"\u003Etransformers\u003C\u002Fa\u003E, и дальше вы можете запускать любой из примеров выше. Попробовать это вы можете в \u003Ca href=\"https:\u002F\u002Fcolab.research.google.com\u002Fdrive\u002F1u1gNRuUQGUI15cSk9Kfm2yMlUDAcj4P9?usp=sharing\" rel=\"noopener noreferrer nofollow\"\u003Eдемо-блокноте\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003E# !pip install transformers sentencepiece --quiet\nimport torch\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\nmodel_name = \"cointegrated\u002Frut5-base-multitask\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\ndef generate(text, **kwargs):\n    inputs = tokenizer(text, return_tensors='pt')\n    with torch.no_grad():\n        hypotheses = model.generate(**inputs, num_beams=5, **kwargs)\n    return tokenizer.decode(hypotheses[0], skip_special_tokens=True)\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EВесь код, которым я дообучал модель, есть \u003Ca href=\"https:\u002F\u002Fgist.github.com\u002Favidale\u002F4de1454bf41822dc862fddbd779d4cc6\" rel=\"noopener noreferrer nofollow\"\u003Eв этом блокноте\u003C\u002Fa\u003E; он не очень чистый, ибо я только экспериментировал, и содержит несколько задач, не вошедших в опубликованную версию модели. А сама модель выложена в каталог Huggingface: \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fcointegrated\u002Frut5-base-multitask\" rel=\"noopener noreferrer nofollow\"\u003Ecointegrated\u002Frut5-base-multitask\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Ch2\u003EКак обучать T5 на собственных данных\u003C\u002Fh2\u003E\u003Cp\u003EТекущая версия модели задумывалась как демонстрация возможностей предобученной seq2seq модели и как болванка для последующего дообучения. Поэтому я особо не возился ни с подбором гиперпараметров, ни с качеством датасетов. Следовательно, если более внимательно поработать с датасетом и гиперпараметрами и дообучить модель на какую-то одну задачу, работать она будет ещё лучше. Например, так я уже дообучал её \u003Ca href=\"https:\u002F\u002Fhabr.com\u002Fru\u002Fpost\u002F564916\u002F\" rel=\"noopener noreferrer nofollow\"\u003Eна задаче перефразирования\u003C\u002Fa\u003E. А здесь я покажу, как обучить модель отвечать на вопросы из кроссвордов, используя фреймворки \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Ftransformers\u002F\" rel=\"noopener noreferrer nofollow\"\u003Etransformers \u003C\u002Fa\u003Eи \u003Ca href=\"https:\u002F\u002Fpytorch.org\" rel=\"noopener noreferrer nofollow\"\u003Epytorch\u003C\u002Fa\u003E (при необходимости, модель можно потом сконвертировать в tensorflow или другой формат). Полный код примера есть всё \u003Ca href=\"https:\u002F\u002Fcolab.research.google.com\u002Fdrive\u002F1u1gNRuUQGUI15cSk9Kfm2yMlUDAcj4P9?usp=sharing\" rel=\"noopener noreferrer nofollow\"\u003Eв том же демо блокноте\u003C\u002Fa\u003E. \u003C\u002Fp\u003E\u003Cp\u003EИнициализировать модель можно многозадачной версией. Тут, как и везде в библиотеке \u003Ccode\u003Etransformers\u003C\u002Fcode\u003E, \u003Ccode\u003Emodel\u003C\u002Fcode\u003E – это сама нейросеть, а \u003Ccode\u003Etokenizer\u003C\u002Fcode\u003E – это часть модели, ответственная за сопоставление текстов со словарём: разбиение текстов на числовые токены, и сбор текстов из токенов обратно. \u003Ccode\u003Eoptimizer\u003C\u002Fcode\u003E – это объект, отвечающий за градиентный спуск; он нужен только на время обучения.\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Eimport torch \nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\nraw_model = 'cointegrated\u002Frut5-base-multitask' \nmodel = T5ForConditionalGeneration.from_pretrained(raw_model).cuda();\ntokenizer = T5Tokenizer.from_pretrained(raw_model)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EНиже  – полный код обучения модели.  Предполагается, что переменная \u003Ccode\u003Epairs\u003C\u002Fcode\u003E  – это список из пар, состоящих из вопроса и ответа. На GPU одна эпоха обучения на 75 тысячах примеров занимает около 15 минут.\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Efrom tqdm.auto import trange\nimport random\nimport numpy as np\n\nbatch_size = 16  # сколько примеров показываем модели за один шаг\nreport_steps = 200  # раз в сколько шагов печатаем результат\nepochs = 3  # сколько раз мы покажем данные модели\n\nmodel.train()\nlosses = []\nfor epoch in range(epochs):\n    print('EPOCH', epoch)\n    random.shuffle(pairs)\n    for i in trange(0, int(len(pairs) \u002F batch_size)):\n        batch = pairs[i * batch_size: (i + 1) * batch_size]\n        # кодируем вопрос и ответ \n        x = tokenizer([p[0] for p in batch], return_tensors='pt', padding=True).to(model.device)\n        y = tokenizer([p[1] for p in batch], return_tensors='pt', padding=True).to(model.device)\n        # -100 - специальное значение, позволяющее не учитывать токены\n        y.input_ids[y.input_ids == 0] = -100\n        # вычисляем функцию потерь\n        loss = model(\n            input_ids=x.input_ids,\n            attention_mask=x.attention_mask,\n            labels=y.input_ids,\n            decoder_attention_mask=y.attention_mask,\n            return_dict=True\n        ).loss\n        # делаем шаг градиентного спуска\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        # печатаем скользящее среднее значение функции потерь\n        losses.append(loss.item())\n        if i % report_steps == 0:\n            print('step', i, 'loss', np.mean(losses[-report_steps:]))\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EТри эпохи я выбрал от балды. Размер батча я выбрал опытным путём: максимальный, при котором хватает памяти на GPU. \u003Ccode\u003Elearning_rate\u003C\u002Fcode\u003E, равный \u003Ccode\u003E1e-5\u003C\u002Fcode\u003E, я выставил, исходя из опыта: обычно при нём модель обучается не очень быстро, но качественно.\u003C\u002Fp\u003E\u003Cp\u003EКод для генерации ответа обученной моделью весьма прост:\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Emodel.eval()\n\ndef answer(x, **kwargs):\n    inputs = tokenizer(x, return_tensors='pt').to(model.device)\n    with torch.no_grad():\n        hypotheses = model.generate(**inputs, **kwargs)\n    return tokenizer.decode(hypotheses[0], skip_special_tokens=True)\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EПосмотрим, насколько хорошо модель выучила свою тренировочную выборку. \u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode\u003EКакое животное раньше называли камелопардом?\nanswer: Жираф\nmodel:  акула\n---\nГрамматическая категория глагола, выражающая отношение действия к действительности (в лингвистике)\nanswer: наклонение\nmodel:  действие\n---\nО чём поется в песне Greenday – «Wake Me Up When September Ends» (Разбуди меня, когда сентябрь кончится)?\nanswer: О смерти его отца\nmodel:  о лете\n---\nСоседка Земли по Солнечной системе\nanswer: Венера\nmodel:  Африка\n---\nОтношение размеров на чертеже, карте и т. п. к действительным размерам на местности, предмете\nanswer: масштаб\nmodel:  пропорциональность\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EЧто ж, модель пытается, но часто \"мажет\". Возможно, стоит поучить её в течение ещё нескольких эпох. А теперь посмотрим, насколько хорошо модель справляется с ответами на вопросы, которые она не видела.  Кажется, довольно пристойно.\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode\u003EМинерал, сульфид марганца\nanswer: алабандин\nmodel:  сульфид\n---\nГде находится родина табака?\nanswer: Южная Америка\nmodel:  Бразилия\n---\nСтаринный русский головной убор с приподнятым вверх спереди и сзади околышем\nanswer: кораблик\nmodel:  шнур\n---\nПочетное звание у тюрков и монголов, дававшееся за воинские подвиги\nanswer: батыр\nmodel:  аким\n---\nСчетный прибор\nanswer: арифмометр\nmodel:  таблица\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EОбученную модель можно сохранить на диск, а потом, при желании, даже выложить на хаб Huggingface. Я выложил её под названием \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fcointegrated\u002Frut5-base-quiz\" rel=\"noopener noreferrer nofollow\"\u003Ecointegrated\u002Frut5-base-quiz\u003C\u002Fa\u003E. \u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Enew_model_name = 'rut5-base-quiz'  # название папки для сохранения\nmodel.save_pretrained(new_model_name)\ntokenizer.save_pretrained(new_model_name)\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Ch2\u003EВместо заключения\u003C\u002Fh2\u003E\u003Cp\u003EПредобученные seq2seq модели – это здорово. Сейчас ими можно решать много разных задач NLP, а Google считает, что вообще чуть ли не все. Моя \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fcointegrated\u002Frut5-base-multitask\" rel=\"noopener noreferrer nofollow\"\u003Eмоделька\u003C\u002Fa\u003E показывает, что отчасти это уже верно и для русского языка.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E","tags":[{"titleHtml":"T5"},{"titleHtml":"трансформер"},{"titleHtml":"нейросеть"},{"titleHtml":"natural language processing"},{"titleHtml":"nlp"}],"metadata":{"stylesUrls":[],"scriptUrls":[],"shareImageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F63b\u002F74b\u002F7aa\u002F63b74b7aac04b5b1f923aa9b1f0cc58e.png","shareImageWidth":1200,"shareImageHeight":630,"vkShareImageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F63b\u002F74b\u002F7aa\u002F63b74b7aac04b5b1f923aa9b1f0cc58e.png","schemaJsonLd":"{\"@context\":\"http:\\\u002F\\\u002Fschema.org\",\"@type\":\"Article\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https:\\\u002F\\\u002Fhabr.com\\\u002Fru\\\u002Fpost\\\u002F581932\\\u002F\"},\"headline\":\"Многозадачная модель T5 для русского языка\",\"datePublished\":\"2021-10-06T16:28:47+03:00\",\"dateModified\":\"2021-10-06T17:23:26+03:00\",\"author\":{\"@type\":\"Person\",\"name\":\"Давид Дале\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Habr\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fa_\\\u002Flk\\\u002F9m\\\u002Fa_lk9mjkccjox-zccjrpfolmkmq.png\"}},\"description\":\"Модель T5 &ndash; это нейросеть, которая уже обучена хорошо понимать и генерировать текст, и которую можно дообучить на собственную задачу, будь то перевод, суммаризац...\",\"url\":\"https:\\\u002F\\\u002Fhabr.com\\\u002Fru\\\u002Fpost\\\u002F581932\\\u002F#post-content-body\",\"about\":[\"h_python\",\"h_sw\",\"h_programming\",\"h_machine_learning\",\"h_natural_language_processing\",\"f_develop\"],\"image\":[\"https:\\\u002F\\\u002Fhabr.com\\\u002Fshare\\\u002Fpublication\\\u002F581932\\\u002F59d0d23c6b4cc81ed0b2955f5e9842c6\\\u002F\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F63b\\\u002F74b\\\u002F7aa\\\u002F63b74b7aac04b5b1f923aa9b1f0cc58e.png\"]}","metaDescription":"Модель T5 – это нейросеть, которая уже обучена хорошо понимать и генерировать текст, и которую можно дообучить на собственную задачу, будь то перевод, суммаризация текстов, или генерация ответа...","mainImageUrl":null,"amp":true},"polls":[],"commentsEnabled":true,"rulesRemindEnabled":false,"votesEnabled":true,"status":"published","plannedPublishTime":null,"checked":null,"isEditorial":false}},"articlesIds":{},"isLoading":false,"pagesCount":{},"route":{},"reasonsList":null,"view":"cards","lastVisitedRoute":{},"ssrCommentsArticleIds":[""],"karma":{}},"authorContribution":{"authors":{}},"betaTest":{"currentAnnouncement":null,"announcements":{},"announcementCards":null,"announcementComments":{},"announcementCommentThreads":{},"announcementCommentingStatuses":{},"archivedList":[]},"authorStatistics":{"articleRefs":{},"articleIds":{},"pagesCount":{},"route":{},"viewsCount":[],"maxStatsCount":{}},"career":{"seoLandings":[],"hubs":"python,sw,programming,machine_learning,natural_language_processing"},"comments":{"articleComments":{},"searchCommentsResults":null,"previewComment":null,"pagesCount":null,"commentAccess":{},"scrollParents":{},"pageArticleComments":{"lastViewedComment":0,"postId":null,"lastCommentTimestamp":"","moderated":[],"moderatedIds":[],"commentRoute":""}},"companies":{"companyRefs":{},"companyIds":{},"companyTopIds":{},"pagesCount":{},"companyProfiles":{},"companiesCategories":[],"companiesCategoriesTotalCount":0,"companiesWidgets":{},"companiesWorkers":{},"companiesFans":{},"route":{},"isLoading":false,"companyWorkersLoading":false,"companyFansLoading":false,"vacancies":{}},"companiesContribution":{"hubs":{},"flows":{},"companyRefs":{}},"companyHubsContribution":{"contributionRefs":{"hubRefs":{},"hubIds":{}}},"conversation":{"messages":[],"respondent":null,"isLoadMore":false},"conversations":{"conversations":[],"unreadCount":0,"pagesCount":0,"isLoadMore":false},"desktopState":{"desktopFl":null,"desktopHl":null,"isChecked":false,"isLoginDemanded":false},"dfp":{"slotsDict":{}},"docs":{"menu":{},"articles":{},"mainMenu":[],"loading":{"main":false,"dropdown":false,"article":false}},"feature":{"isProbablyVisible":"true"},"flows":{"flows":[{"alias":"develop","id":1,"route":{"name":"FLOW_PAGE","params":{"flowName":"develop"}}},{"alias":"admin","id":6,"route":{"name":"FLOW_PAGE","params":{"flowName":"admin"}}},{"alias":"design","id":2,"route":{"name":"FLOW_PAGE","params":{"flowName":"design"}}},{"alias":"management","id":3,"route":{"name":"FLOW_PAGE","params":{"flowName":"management"}}},{"alias":"marketing","id":4,"route":{"name":"FLOW_PAGE","params":{"flowName":"marketing"}}},{"alias":"popsci","id":7,"route":{"name":"FLOW_PAGE","params":{"flowName":"popsci"}}}]},"global":{"isPwa":false,"device":"desktop","isHabrCom":true},"hubs":{"hubRefs":{},"hubIds":{},"pagesCount":{},"isLoading":false,"route":{}},"hubsBlock":{"hubRefs":{},"hubIds":{}},"i18n":{"fl":"ru","hl":"ru"},"info":{"infoPage":{},"isLoading":true},"location":{"urlStruct":{"protocol":null,"slashes":null,"auth":null,"host":null,"port":null,"hostname":null,"hash":null,"search":null,"query":{},"pathname":null,"path":null,"href":""},"searchQuery":null},"me":{"user":null,"ppgDemanded":false,"karmaResetInfo":{"canReincarnate":null,"wasReincarnated":null,"currentScore":null},"notes":null},"mostReadingList":{"mostReadingListIds":[],"mostReadingListRefs":null,"promoPost":null},"pinnedPost":{"pinnedPost":null},"ppa":{"articles":{},"card":null,"transactions":null,"totalTransactions":null,"isAccessible":null},"projectsBlocks":{"activeBlocks":{}},"pullRefresh":{"shouldRefresh":false},"sandbox":{"articleIds":[],"articleRefs":{},"pagesCount":null,"route":{},"lastVisitedRoute":{},"isLoading":false},"settingsOther":{"inputs":{"uiLang":{"errors":[],"ref":null,"value":""},"articlesLangEnglish":{"errors":[],"ref":null,"value":false},"articlesLangRussian":{"errors":[],"ref":null,"value":false},"agreement":{"errors":[],"ref":null,"value":false},"email":{"errors":[],"ref":null,"value":true},"digest":{"errors":[],"ref":null,"value":true}}},"similarList":{"similarListIds":[],"similarListRefs":null},"ssr":{"error":null,"isDataLoaded":false,"isDataLoading":false,"isHydrationFailed":false,"isServer":false},"userHubsContribution":{"contributionRefs":{"hubRefs":{},"hubIds":{}}},"userInvites":{"availableInvites":0,"usedInvitesIds":[],"usedInvitesRefs":{},"usedInvitesPagesCount":0,"unusedInvitesIds":[],"unusedInvitesRefs":{},"unusedInvitesPagesCount":0},"users":{"authorRefs":{},"authorIds":{},"pagesCount":{},"authorProfiles":{},"userHubs":{},"userInvitations":{},"authorFollowers":{},"authorFollowed":{},"karmaStats":[],"statistics":null,"isLoading":false,"authorFollowersLoading":false,"authorFollowedLoading":false,"userHubsLoading":false,"userInvitationsLoading":false,"route":{}},"viewport":{"prevScrollY":{},"scrollY":0,"width":0},"tracker":{"items":{},"pagesCache":{},"markedViewedSilently":{},"markedRead":{},"unreadCounters":{"applications":null,"system":null,"mentions":null,"subscribers":null,"posts_and_comments":null},"unviewedCounters":{"applications":null,"system":null,"mentions":null,"subscribers":null,"posts_and_comments":null}}};(function(){var s;(s=document.currentScript||document.scripts[document.scripts.length-1]).parentNode.removeChild(s);}());</script>
<script src="https://assets.habr.com/habr-web/js/chunk-vendors.c2c3fc9a.js" defer></script><script src="https://assets.habr.com/habr-web/js/app.c0af73e7.js" defer></script>



    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    </script>
  
  <script type="text/javascript" >
    (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
    m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
    (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

    ym(24049213, "init", {
      defer:true,
      trackLinks:true,
      accurateTrackBounce:true,
      webvisor:false,
    });
  </script>
  <noscript>
    <div>
      <img src="https://mc.yandex.ru/watch/24049213" style="position:absolute; left:-9999px;" alt="" />
    </div>
  </noscript>
  
    <script type="text/javascript">
      window.addEventListener('load', function () {
        setTimeout(() => {
          const img = new Image();
          img.src = 'https://vk.com/rtrg?p=VK-RTRG-421343-57vKE';
        }, 0);
      });
    </script>
  
</body>
</html>
