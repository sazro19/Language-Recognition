<!DOCTYPE html>
<html lang="ru" data-vue-meta="%7B%22lang%22:%7B%22ssr%22:%22ru%22%7D%7D">
<head >
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0,viewport-fit=cover">
  <title>Изменить сохранения Spark! Часть первая: разделяй и… сортируй / Хабр</title>
  <style>
    /* cyrillic-ext */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveSxf6TF0.woff2) format('woff2');
      unicode-range: U+0460-052F, U+1C80-1C88, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;
    }

    /* cyrillic */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveQhf6TF0.woff2) format('woff2');
      unicode-range: U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;
    }

    /* latin-ext */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveSBf6TF0.woff2) format('woff2');
      unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
    }

    /* latin */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveRhf6.woff2) format('woff2');
      unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
    }
  </style>
  <link rel="preload" href="https://assets.habr.com/habr-web/css/chunk-vendors.e7843cc0.css" as="style"><link rel="preload" href="https://assets.habr.com/habr-web/js/chunk-vendors.c2c3fc9a.js" as="script"><link rel="preload" href="https://assets.habr.com/habr-web/css/app.02ee25a4.css" as="style"><link rel="preload" href="https://assets.habr.com/habr-web/js/app.c0af73e7.js" as="script">
  <link rel="stylesheet" href="https://assets.habr.com/habr-web/css/chunk-vendors.e7843cc0.css"><link rel="stylesheet" href="https://assets.habr.com/habr-web/css/app.02ee25a4.css">
  <script>window.i18nFetch = new Promise((res, rej) => {
          const xhr = new XMLHttpRequest();
          xhr.open('GET', '/js/i18n/ru-compiled.85eb77f0b17c8235e7b64b9f81ea5ec2.json');
          xhr.responseType = 'json';
          xhr.onload = function(e) {
            if (this.status === 200) {
              res({ru: xhr.response});
            } else {
              rej(e);
            }
          };
          xhr.send();
        });</script>
  
  <script data-vue-meta="ssr" src="/js/ads.js" onload="window['zhY4i4nJ9K'] = true" data-vmid="checkad"></script><script data-vue-meta="ssr" type="application/ld+json" data-vmid="ldjson-schema">{"@context":"http:\/\/schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/habr.com\/ru\/company\/sberbank\/blog\/582090\/"},"headline":"Изменить сохранения Spark! Часть первая: разделяй и… сортируй","datePublished":"2021-10-07T10:45:24+03:00","dateModified":"2021-10-15T11:36:41+03:00","author":{"@type":"Person","name":"Sber"},"publisher":{"@type":"Organization","name":"Habr","logo":{"@type":"ImageObject","url":"https:\/\/habrastorage.org\/webt\/a_\/lk\/9m\/a_lk9mjkccjox-zccjrpfolmkmq.png"}},"description":"Автор: Иван Калининский, участник профессионального сообщества Сбера&nbsp;SberProfi&nbsp;DWH\/BigData.Профессиональное сообщество&nbsp;SberProfi&nbsp;DWH\/BigData&nbsp;отвечает за развитие...","url":"https:\/\/habr.com\/ru\/company\/sberbank\/blog\/582090\/#post-content-body","about":["c_sberbank","h_db_admins","h_bigdata","f_develop","f_admin"],"image":["https:\/\/habr.com\/share\/publication\/582090\/5ea735b56f9a531032f1bdf9e372feaf\/","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/872\/21f\/373\/87221f373acb676fa493e066662a3872.jpg","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/4ca\/b99\/7e5\/4cab997e5c17f6756bc26314251202e7.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/96f\/a84\/a38\/96fa84a385360aa6c226d57dce164d99.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/909\/5b1\/bcc\/9095b1bcc15e9091691ccd20069175c3.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/673\/63e\/a2c\/67363ea2c8432f75acdc8d9ab1244485.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/e4f\/0b0\/4a0\/e4f0b04a01b8ed7f3dc7827b54ef6d18.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/7d7\/e19\/98a\/7d7e1998a564103de10de0cbfa5ebe35.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/30f\/5ce\/de4\/30f5cede47c43a318408e4e5e7d320b9.png"]}</script>
  <script src="//www.googletagservices.com/tag/js/gpt.js" async></script>
  <style>.grecaptcha-badge{visibility: hidden;}</style>
  <meta name="habr-version" content="2.49.0">
  
  <meta data-vue-meta="ssr" property="fb:app_id" content="444736788986613"><meta data-vue-meta="ssr" property="fb:pages" content="472597926099084"><meta data-vue-meta="ssr" name="twitter:card" content="summary_large_image"><meta data-vue-meta="ssr" name="twitter:site" content="@habr_eng"><meta data-vue-meta="ssr" property="og:title" content="Изменить сохранения Spark! Часть первая: разделяй и… сортируй" data-vmid="og:title"><meta data-vue-meta="ssr" name="twitter:title" content="Изменить сохранения Spark! Часть первая: разделяй и… сортируй" data-vmid="twitter:title"><meta data-vue-meta="ssr" name="aiturec:title" content="Изменить сохранения Spark! Часть первая: разделяй и… сортируй" data-vmid="aiturec:title"><meta data-vue-meta="ssr" name="description" content="Автор: Иван Калининский, участник профессионального сообщества Сбера&amp;nbsp;SberProfi&amp;nbsp;DWH/BigData.Профессиональное сообщество&amp;nbsp;SberProfi&amp;nbsp;DWH/BigData&amp;nbsp;отвечает за развитие компетенций в..." data-vmid="description"><meta data-vue-meta="ssr" itemprop="description" content="Автор: Иван Калининский, участник профессионального сообщества Сбера&amp;nbsp;SberProfi&amp;nbsp;DWH/BigData.Профессиональное сообщество&amp;nbsp;SberProfi&amp;nbsp;DWH/BigData&amp;nbsp;отвечает за развитие компетенций в..." data-vmid="description:itemprop"><meta data-vue-meta="ssr" property="og:description" content="Автор: Иван Калининский, участник профессионального сообщества Сбера&amp;nbsp;SberProfi&amp;nbsp;DWH/BigData.Профессиональное сообщество&amp;nbsp;SberProfi&amp;nbsp;DWH/BigData&amp;nbsp;отвечает за развитие компетенций в..." data-vmid="og:description"><meta data-vue-meta="ssr" name="twitter:description" content="Автор: Иван Калининский, участник профессионального сообщества Сбера&amp;nbsp;SberProfi&amp;nbsp;DWH/BigData.Профессиональное сообщество&amp;nbsp;SberProfi&amp;nbsp;DWH/BigData&amp;nbsp;отвечает за развитие компетенций в..." data-vmid="twitter:description"><meta data-vue-meta="ssr" property="aiturec:description" content="Автор: Иван Калининский, участник профессионального сообщества Сбера&amp;nbsp;SberProfi&amp;nbsp;DWH/BigData.Профессиональное сообщество&amp;nbsp;SberProfi&amp;nbsp;DWH/BigData&amp;nbsp;отвечает за развитие компетенций в..." data-vmid="aiturec:description"><meta data-vue-meta="ssr" itemprop="image" content="https://habrastorage.org/getpro/habr/upload_files/872/21f/373/87221f373acb676fa493e066662a3872.jpg" data-vmid="image:itemprop"><meta data-vue-meta="ssr" property="og:image" content="https://habrastorage.org/getpro/habr/upload_files/872/21f/373/87221f373acb676fa493e066662a3872.jpg" data-vmid="og:image"><meta data-vue-meta="ssr" property="aiturec:image" content="https://habrastorage.org/getpro/habr/upload_files/872/21f/373/87221f373acb676fa493e066662a3872.jpg" data-vmid="aiturec:image"><meta data-vue-meta="ssr" name="twitter:image" content="https://habrastorage.org/getpro/habr/upload_files/872/21f/373/87221f373acb676fa493e066662a3872.jpg" data-vmid="twitter:image"><meta data-vue-meta="ssr" property="vk:image" content="https://habrastorage.org/getpro/habr/upload_files/872/21f/373/87221f373acb676fa493e066662a3872.jpg" data-vmid="vk:image"><meta data-vue-meta="ssr" property="aiturec:item_id" content="582090" data-vmid="aiturec:item_id"><meta data-vue-meta="ssr" property="aiturec:datetime" content="2021-10-07T07:45:24.000Z" data-vmid="aiturec:datetime"><meta data-vue-meta="ssr" property="og:type" content="article" data-vmid="og:type"><meta data-vue-meta="ssr" property="og:locale" content="ru_RU" data-vmid="og:locale"><meta data-vue-meta="ssr" property="og:image:width" content="1200" data-vmid="og:image:width"><meta data-vue-meta="ssr" property="og:image:height" content="630" data-vmid="og:image:height">
  <link data-vue-meta="ssr" href="https://habr.com/ru/rss/post/582090/?fl=ru" type="application/rss+xml" title="" rel="alternate" name="rss"><link data-vue-meta="ssr" href="https://habr.com/ru/company/sberbank/blog/582090/" rel="canonical" data-vmid="canonical"><link data-vue-meta="ssr" data-vmid="hreflang"><link data-vue-meta="ssr" image_src="image" href="https://habrastorage.org/getpro/habr/upload_files/872/21f/373/87221f373acb676fa493e066662a3872.jpg" data-vmid="image:href"><link data-vue-meta="ssr" rel="amphtml" href="https://habr.com/ru/amp/post/582090/">
  <meta name="apple-mobile-web-app-status-bar-style" content="#303b44">
  <meta name="msapplication-TileColor" content="#629FBC">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="mobile-web-app-capable" content="yes">
  <link
    rel="shortcut icon"
    type="image/png"
    sizes="16x16"
    href="https://assets.habr.com/habr-web/img/favicons/favicon-16.png"
  >
  <link
    rel="shortcut icon"
    type="image/png"
    sizes="32x32"
    href="https://assets.habr.com/habr-web/img/favicons/favicon-32.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="76x76"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-76.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="120x120"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-120.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="152x152"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-152.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="180x180"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-180.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="256x256"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-256.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1136x640.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2436x1125.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1792x828.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_828x1792.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1334x750.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1242x2668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2208x1242.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1125x2436.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1242x2208.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2732x2048.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2688x1242.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2224x1668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_750x1334.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2048x2732.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2388x1668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1668x2224.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_640x1136.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1668x2388.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2048x1536.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1536x2048.png"
  >
  <link
    rel="mask-icon"
    color="#77a2b6"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-120.svg"
  >
  <link
    crossorigin="use-credentials"
    href="/manifest.webmanifest"
    rel="manifest"
  >
</head>
<body>


<div id="app" data-server-rendered="true" data-async-called="true"><div class="tm-layout__wrapper"><!----> <div></div> <!----> <header class="tm-header"><div class="tm-page-width"><div class="tm-header__container"><!----> <span class="tm-header__logo-wrap"><a href="/ru/" class="tm-header__logo tm-header__logo_ru"><svg height="16" width="16" class="tm-svg-img tm-header__icon"><title>Хабр</title> <use xlink:href="/img/habr-logo-ru.svg#logo"></use></svg></a> <span class="tm-header__beta-sign" style="display:none;">β</span></span> <div class="tm-dropdown tm-header__projects"><div class="tm-dropdown__head"><button class="tm-header__dropdown-toggle"><svg height="16" width="16" class="tm-svg-img tm-header__icon tm-header__icon_dropdown"><title>Открыть список</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#arrow-down"></use></svg></button></div> <!----></div> <a href="/ru/sandbox/start/" class="tm-header__become-author-btn">
            Как стать автором
          </a> <div class="tm-feature tm-header__feature tm-feature_variant-inline"><!----></div> <!----> <!----></div></div></header> <div class="tm-layout"><div class="tm-page-progress-bar"></div> <div data-menu-sticky="true" class="tm-base-layout__header tm-base-layout__header_is-sticky"><div class="tm-page-width"><div class="tm-base-layout__header-wrapper"><div class="tm-main-menu"><div class="tm-main-menu__section"><nav class="tm-main-menu__section-content"><!----> <a href="/ru/all/" class="tm-main-menu__item">
        Все потоки
      </a> <a href="/ru/flows/develop/" class="tm-main-menu__item">
          Разработка
        </a><a href="/ru/flows/admin/" class="tm-main-menu__item">
          Администрирование
        </a><a href="/ru/flows/design/" class="tm-main-menu__item">
          Дизайн
        </a><a href="/ru/flows/management/" class="tm-main-menu__item">
          Менеджмент
        </a><a href="/ru/flows/marketing/" class="tm-main-menu__item">
          Маркетинг
        </a><a href="/ru/flows/popsci/" class="tm-main-menu__item">
          Научпоп
        </a></nav></div></div> <div class="tm-header-user-menu tm-base-layout__user-menu"><a href="/ru/search/" class="tm-header-user-menu__item tm-header-user-menu__search"><svg height="24" width="24" class="tm-svg-img tm-header-user-menu__icon tm-header-user-menu__icon_search tm-header-user-menu__icon_dark"><title>Поиск</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#search"></use></svg></a> <!----> <!----> <!----> <div class="tm-header-user-menu__item tm-header-user-menu__user_desktop"><div class="tm-dropdown"><div class="tm-dropdown__head"><svg height="24" width="24" data-test-id="menu-toggle-guest" class="tm-svg-img tm-header-user-menu__icon"><title>Профиль</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#header-user"></use></svg> <!----></div> <!----></div> <!----></div> <!----></div></div></div></div> <!----> <div class="tm-page-width"></div> <main class="tm-layout__container"><div hl="ru" companyName="sberbank" data-async-called="true" class="tm-page"><div class="tm-page-width"><div class="tm-page__header"><div class="tm-company-card__branding tm-company-article__branding tm-company-card__branding_loading"><div class="tm-company-card__branding-placeholder"><!----></div> <img src="//habrastorage.org/getpro/habr/branding/6ac/725/4eb/6ac7254eb269ffa84589b75da04eb5ae.png" width="100%" class="tm-company-card__branding-image"></div></div> <div class="tm-page__wrapper"><div class="tm-page__main tm-page__main_has-sidebar"><div class="pull-down"><div class="pull-down__header" style="height:0px;"><div class="pull-down__content" style="bottom:10px;"><svg height="24" width="24" class="tm-svg-img pull-down__arrow"><title>Обновить</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#pull-arrow"></use></svg></div></div> <div class="tm-article-presenter"><div class="tm-company-card tm-company-article__company-card"><div class="tm-company-card__info"><div class="tm-company-card__header"><a href="/ru/company/sberbank/profile/" class="tm-company-card__avatar"><div class="tm-entity-image"><img alt="" height="48" src="//habrastorage.org/getpro/habr/company/9db/3c1/ec0/9db3c1ec02265b8bcbfdfb0d23d8b9f2.jpg" width="48" class="tm-entity-image__pic"></div></a> <a href="https://career.habr.com/companies/sberbank-russia" rel="noopener" target="_blank" class="tm-grade tm-company-card__rating"><div class="tm-rating"><div class="tm-rating__header"><svg height="24" width="24" class="tm-svg-img tm-svg-grade__icon"><title>Оценка компании на Хабр Карьере</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#grade"></use></svg> <div class="tm-rating__counter tm-rating__counter_variant-grade">4.07</div></div> <div class="tm-rating__text tm-rating__text_variant-grade">
    Оценка
  </div></div></a> <div class="tm-rating tm-company-card__rating"><div class="tm-rating__header"> <div class="tm-rating__counter">229.68</div></div> <div class="tm-rating__text">
    Рейтинг
  </div></div></div> <div class="tm-company-card__info"><a href="/ru/company/sberbank/profile/" class="tm-company-card__name">
        Сбер
      </a> <div class="tm-company-card__description">Больше чем банк</div></div></div> <div class="tm-company-card__buttons"><!----> <!----></div></div> <div class="tm-article-presenter__body"><div class="tm-misprint-area"><div class="tm-misprint-area__wrapper"><article class="tm-article-presenter__content tm-article-presenter__content_narrow"><div class="tm-article-presenter__header"> <div class="tm-article-snippet tm-article-presenter__snippet"><div class="tm-article-snippet__meta-container"><div class="tm-article-snippet__meta"><span class="tm-user-info tm-article-snippet__author"><a href="/ru/users/Sber/" title="Sber" class="tm-user-info__userpic"><div class="tm-entity-image"><img alt="" height="24" loading="lazy" src="//habrastorage.org/r/w32/getpro/habr/avatars/b1f/595/703/b1f595703fffc92491048ef9b6882dff.png" width="24" class="tm-entity-image__pic"></div></a> <span class="tm-user-info__user"><a href="/ru/users/Sber/" class="tm-user-info__username">
      Sber
    </a> </span></span> <span class="tm-article-snippet__datetime-published"><time datetime="2021-10-07T07:45:24.000Z" title="2021-10-07, 10:45">7  октября   в 10:45</time></span></div> <!----></div> <h1 lang="ru" class="tm-article-snippet__title tm-article-snippet__title_h1"><span>Изменить сохранения Spark! Часть первая: разделяй и… сортируй</span></h1> <div class="tm-article-snippet__hubs"><span class="tm-article-snippet__hubs-item"><a href="/ru/company/sberbank/blog/" class="tm-article-snippet__hubs-item-link router-link-active"><span>Блог компании Сбер</span> <!----></a></span><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/db_admins/" class="tm-article-snippet__hubs-item-link"><span>Администрирование баз данных</span> <span title="Профильный хаб" class="tm-article-snippet__profiled-hub">*</span></a></span><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/bigdata/" class="tm-article-snippet__hubs-item-link"><span>Big Data</span> <span title="Профильный хаб" class="tm-article-snippet__profiled-hub">*</span></a></span></div> <!----> <!----> <!----></div></div> <!----> <div data-gallery-root="" lang="ru" class="tm-article-body"><div id="post-content-body" class="article-formatted-body article-formatted-body_version-2"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>Автор: Иван Калининский, участник профессионального сообщества Сбера SberProfi DWH/BigData.</em></p><p><em>Профессиональное сообщество SberProfi DWH/BigData отвечает за развитие компетенций в таких направлениях, как экосистема Hadoop, Teradata, Oracle DB, GreenPlum, а также BI инструментах Qlik, SAP BO, Tableau и др.</em></p><figure class="full-width "><img src="https://habrastorage.org/r/w780q1/getpro/habr/upload_files/872/21f/373/87221f373acb676fa493e066662a3872.jpg" width="1200" height="630" data-src="https://habrastorage.org/getpro/habr/upload_files/872/21f/373/87221f373acb676fa493e066662a3872.jpg" data-blurred="true"/><figcaption></figcaption></figure><p>В этой статье вы узнаете о том, какими способами мы пытались обновлять таблицы в Hadoop, содержащие сотни терабайт данных.</p><p><strong>И если в начале нашего пути процесс обновления длился несколько часов (до десяти-двенадцати часов), то теперь ему требуется всего тридцать-сорок минут, а использование вычислительных ресурсов уменьшено вдвое!</strong></p><p>При этом была создана библиотека расширения Spark, которая предоставляет DataSource для преобразования данных в файлах в формат этого DataSource, изменения данных командой MERGE через DataFrame API или SQL, а в будущем ещё и UPDATE, DELETE и некоторые операции DDL.</p><p>Файлы при этом можно будет читать любым привычным способом, ведь они не модифицированы, а метаданные не обязательны для их чтения.</p><p>Вы увидите код этой библиотеки на языке Scala, который сможете использовать, а может быть даже доработать и поделиться своими успехами.</p><p>Я постараюсь пояснить, почему был сделан тот или иной выбор, но могу умолчать о чём-то, что кажется очевидным, или, наоборот, о чём я не имею представления. Вы сможете задать вопросы, а я постараюсь ответить на них.</p><p>Это первая статья из нескольких, и в ней будет рассказано только о немногих реализованных классах (они нужны для распределения данных определённым способом), поэтому наберитесь терпения, я расскажу всё по частям. Впрочем, пора перейти к повествованию.</p><figure class=""><img src="/img/image-loader.svg" height="300" data-src="https://habrastorage.org/getpro/habr/upload_files/4ca/b99/7e5/4cab997e5c17f6756bc26314251202e7.png" data-width="351"/><figcaption></figcaption></figure><p>Для начала договоримся о терминах и определениях. Если это вам пока неинтересно или уже необязательно, переходите к следующему пункту, а если встретите незнакомый или сомнительный термин, вернитесь сюда. Возможно, здесь вы найдёте разъяснение.</p><h3>Термины и определения</h3><p><strong>Таблица</strong>: В нашем случае это копия таблицы из какой-либо RDBMS, сохраненная в одной директории HDFS("<a href="http://hdfs://nameservice/path/to/table"><u>hdfs://nameservice/path/to/table</u></a>") в файловом формате. Используемый нами в большинстве случаев файловый формат – Parquet, допустимы и другие форматы, например ORC, Avro, и даже CSV. Определение таблицы сохраняется в Hive с типом EXTERNAL. Мы не используем MANAGED таблицы;</p><p><strong>Секция</strong>: Часть таблицы, выделенная по некоторому правилу. Данные хранятся в отдельных папках в директории таблицы: "/path/to/table/part=value1", "/path/to/table/part=value2".  Правило секционирования может включать несколько полей, тогда секции в HDFS будут выглядеть так: "/path/to/table/part1=value_p1/part2=value_p2". Файлы с данными при этом находятся только в листовых папках. Информация о каждой секции должна присутствовать в HiveMetastore;</p><p><strong>RDD</strong>: Resilient Distributed Dataset, устойчивый распределённый набор данных, используется в Apache Spark для обработки данных. Он называется распределённым потому, что на кластере состоит из множества частей, которые мы будем называть партициями RDD;</p><p><strong>Партиция RDD: </strong>Часть RDD, я называю её партицией, а не секцией, чтобы отделить от секции Hive. В отношении данных - это итератор, который может пройти по всем записям, которые в нём содержатся. Как правило, будет сделан один и только один проход, после этого итератор будет считаться истощённым (exausted);</p><p><strong>Датафрейм</strong>: Набор данных, можно сказать, что это RDD и план трансформаций RDD, обеспечиваемый оптимизатором Catalyst;</p><p><strong>Партишенер</strong>: Класс, наследник org.apache.spark.Partitioner. Нужен для описания правила, по которому из RDD с определённым набором партиций получается RDD с другим набором партиций. Каждая запись исходного RDD должна быть по описанному правилу перемещена в определенную партицию другого RDD;</p><p><strong>Ключи: </strong>Первичные ключи таблицы, как правило, это одно поле (суррогатный первичный ключ), но полей может быть несколько. Как правило, подразумеваются конкретные значения, ведь сами поля в любом случае есть в каждой записи.</p><p><strong>Кардинальность: </strong>Количество уникальных значений определенного поля. Также может употребляться в значении общего количества записей или не пустых значений, но здесь мы будем говорить только об уникальных значениях.</p><p><strong>Селективность: </strong>Избирательность, мера, показывающая, насколько хорошо будет работать фильтр на основе предиката некоторого выражения. Тесно связана с кардинальностью, ведь если включить существующее значение поля с высокой кардинальностью в фильтр, то будет получено мало записей, возможно, всего одна. Значит, селективность этого фильтра высокая. В то же время, поле с низкой кардинальностью, например поле с типом BOOLEAN, у которого только два возможных значения (null не учитываем), будет иметь низкую селективность, и, если входит в фильтр, в общем случае, выберет половину набора данных.</p><p><strong>CDC</strong>:  Change data capture, программное решение для получения изменений данных из RDBMS;</p><p><strong>Shuffle</strong>: Перемешивание, перетасовывание, - операция, которая сопоставляет каждой записи из каждой партиции исходного RDD партицию конечного RDD;</p><p><strong>Бакет</strong>:  Bucket, "Ведро", некая объединенная группа значений. В нашем случае представляет собой записи, находящиеся в одном файле или в одной партиции RDD.</p><p><strong>Кейс класс</strong>: Вид класса в Scala, класс-образец.</p><p><strong>Трейт:</strong> Вид класса в Scala, описывающий поведение, которое может быть «подмешано» в другой класс. Похож на JavaInterface.</p><h3>Суть проблемы</h3><p>Apache Spark очень много всего умеет. Работает с разными файловыми форматами, с базами данных как RDBMS так и NoSQL, с потоковыми источниками данных. Но, к сожалению, иногда имеющиеся средства не подходят для решения казалось бы простой задачи. Например, есть таблица, хранящаяся в файловом формате в HDFS, а её определение сохранено в Hive. Конечно же, это копия какой-то таблицы из базы данных, а значит, она будет периодически обновляться: часть записей могут быть удалены, часть обновлены и ещё некоторое количество будет добавлено. Однако в файле формата Parquet нельзя ни обновить, ни удалить, ни даже вставить запись. Можно только создать новый файл, а старый удалить. Между тем, в таблице таких файлов может быть сотни тысяч.</p><p><strong>Если это небольшая таблица, несложно переписать её полностью, а вот со структурой, размер которой несколько терабайт или десятков терабайт так работать нельзя.</strong></p><p>Большие таблицы, с которыми мы работаем, разделены на секции, чтобы пользователи могли обращаться к отдельным частям.  Достаточно указать spark.sql.sources.partitionOverwriteMode=dynamic или dataframe.write.option("partitionOverwriteMode", "dynamic").save(path), и, начиная с версии 2.3, Spark перепишет только те секции, которые будут обновляться. Однако в то время мы были привязаны к версии 2.2, поэтому пришлось аккуратно сохранять данные в отдельную директорию, и только после успешной записи удалять папки с устаревшими секциями и переименовывать обновлённые и совершенно новые секции, чтобы они оказались в нужной директории.</p><p>Кроме того, даже если бы мы работали с более новой версией Spark, понадобилось бы сделать эффективный join основного набора данных и пришедшей дельты. Spark умеет делать partition pruning только при явно указанных фильтрах при чтении данных (до версии 2.3.5 точно так). В случае join, на том этапе, когда сравниваются значения полей, Spark уже не помнит, что это "особенные поля", названия директорий. В одной партиции могут находиться несколько значений полей секционирования, поэтому даже если в условии join указать эти поля, то это ни к чему хорошему не приведёт.</p><p>Проще говоря, Spark при самом наивном подходе прочитает всю таблицу, сделает полный shuffle, затем то же самое сделает с дельтой, объединит эти два набора данных и запишет всю таблицу заново.</p><p><strong>Это будет невообразимо долгий процесс, который может сломаться в середине или ближе к завершению, а значит, так делать нельзя. Взгляните на схему ниже, чтобы понять, что кое-где есть проблемы:</strong></p><figure class="full-width "><img src="/img/image-loader.svg" alt="Рисунок 1. Обновление таблицы без потери метаинформации" title="Рисунок 1. Обновление таблицы без потери метаинформации" height="841" data-src="https://habrastorage.org/getpro/habr/upload_files/96f/a84/a38/96fa84a385360aa6c226d57dce164d99.png" data-width="1783"/><figcaption>Рисунок 1. Обновление таблицы без потери метаинформации</figcaption></figure><p>Представим себе, что мы имеем таблицу с секциями A...Z, причём пришедшая дельта затрагивает только секции A и Z. Схема выше описывает, как пойдёт процесс, если выполнить такой код (обновление таблицы наивным способом):</p><pre><code>dfTable.as("t")

  .where('part="A" || 'part="Z")

  .join(dfDelta.as("d"), "t.part = d.part and t.id = d.id", "full_outer")

  .select(columns: _*) //предположим, что проекция подготовлена заранее и полностью описывает те поля, которые нам нужно получить

  .write

  .mode("overwrite")

  .option("partitionOverwriteMode", "dynamic")

  .partitionBy("part")

  .format("parquet")

  .option("compression", "gzip")

  .option("path", "/path/to/table/")

  .saveAsTable("hive_db.hive_table")</code></pre><p> Рассмотрим рисунок 1 подробнее (далее мы говорим исключительно о метаинформации; о потере данных речь не идёт, ведь если данные теряются, то никакого смысла в оптимизации нет и нужно полностью пересмотреть подход):</p><ul><li><p>красные пересекающиеся стрелки обозначают энтропийный процесс, при котором теряется первоначально содержащаяся в таблице метаинформация;</p></li><li><p>параллельные синие стрелки обозначают процесс с сохранением метаинформации;</p></li><li><p>желтыми стрелками обозначен противоречивый процесс.  </p></li></ul><p>На этапе подготовки RDD основную роль играет размер исходных файлов. Значения секций Hive в полученной структуре будут перемешаны, так же, как и отдельные сегменты файлов.</p><p>Чтение данных полностью повторяет структуру уже перемешанного предыдущего состояния, информация не теряется.</p><p>Spark понимает, что нужно соединить данные, и, если их размер достаточно большой, то будет выбран SortMergeJoin. Чтобы выполнить этот вид соединения, Spark должен сделать так, чтобы одинаковые ключи из основного набора данных и из дельты расположились совместно (в партициях RDD с одинаковыми индексами) и были отсортированы в пределах партиций. Записи можно только перераспределить и отсортировать заново, затратив на это время и ресурсы. Производится Shuffle: записи каждого набора перемешиваются по значению хешей. Перемешивание устраняет любую сортировку, которая сохранилась при чтении, теперь порядок записей никак не зависит от первоначального.</p><p>SortMergeJoin делает своё дело, полученный набор данных сохраняет тот же порядок записей, что был после Shuffle и локальной сортировки.</p><p>Последний процесс записывает данные обратно в таблицу, удаляя предыдущие версии секций A и Z навсегда! Это противоречивый процесс, поэтому он изображён в виде двойных жёлтых стрелок. Дело в том, что непосредственно перед сохранением записи будут отсортированы по полю part локально (в пределах партиций RDD). Возможно, что сортировка будет получена из предыдущего состояния (SortMergeJoin), ведь часть условия соединения "t.part = d.part" заставляет сортировать каждую партицию по этому полю, но если в SparkUI в DAG последним виден синий прямоугольник с надписью Sort, записи всё же будут пересортированы.</p><p>Такое упорядочивание записей необходимо для последовательного помещения записей в конечный файл, чтобы не переключаться между файлами из разных секций для каждой строки. Поскольку значения секций перемешаны в пределах партиций RDD, то конечные файлы будут меньше, чем ожидает разработчик.</p><p><strong>Как правило, из одной партиции RDD будет получено столько файлов, сколько есть изменяющихся секций. Если их только две, это может не быть проблемой, а если их тысяча или пять тысяч?</strong></p><p><strong>Процесс сохранения возвращает первоначальное состояние, но технические характеристики этого состояния могут быть намного хуже. Теперь очевидно, что нужна оптимизация.</strong></p><p>И тут нужно вспомнить старую мудрость: <em>«Нельзя заставить машину работать быстрее. Можно только сделать так, чтобы машина выполняла меньше работы»</em>.</p><p>Можно возразить, что затем и нужно горизонтальное масштабирование. Ведь очень просто добавить контейнеры в параметры запуска приложения Spark, подключить к кластеру новые ноды, поставить рядом несколько кластеров. Но это тушение пожара деньгами! Рано или поздно объем данных увеличится, и приложениям, запрашивающим огромные вычислительные ресурсы, будет тесно на любом кластере. Видя проблему, нужно попытаться решить её профессионально, не терять нужную информацию, а наоборот, сохранять и использовать её.</p><h3>Наш путь решения выявленной проблемы</h3><p>Так как полное обновление таблицы оказалось неприемлемым, мы разработали приложение, которое обновляет только имеющиеся секции. Способ работы с конкретными изменяющимися секциями получил развитие, и через некоторое время мы достигли такого результата (пошагово):</p><ol><li><p>Изменения, полученные извне, сохраняются с точно таким же секционированием, как и сама таблица. Записи уже предобработаны CDC, и поставляются в таком виде: записи на удаление и на вставку в одном экземпляре, записи на обновление в двух вариантах: предыдущая версия и новая версия. <br/><br/>Таким образом, мы работаем с набором данных, в котором есть записи для удаления и записи для добавления. Можно сказать, что каждая секция дельты состоит из двух датафреймов, назовём их dataframeInsert и dataframeDelete. При этом решается проблема миграции записей из одной секции в другую - нет нужды проверять, обновлены ли поля, по которым делается секционирование, или нет, и как-то обрабатывать эти изменения;</p></li><li><p>Каждая изменяемая секция основной таблицы (назовём её dataframeMain) проверяется: в ней должны быть ключи на удаление и не должно быть ключей на вставку;</p></li><li><p>Каждая секция основной таблицы соединяется с секцией данных, которые должны быть удалены, методом антисоединения: dataframeMain.join(dataframeDelete, primaryKeys, "left_anti");</p></li><li><p>К каждой секции основной таблицы добавляется секция данных, которые должны быть вставлены, методомdataframeMain.union(dataframeInsert);</p></li><li><p>Каждая секция основной таблицы проверяется: в ней должны быть ключи вставленных записей и не должно быть ключей на удаление;</p></li><li><p>Рассчитывается контрольная сумма данных для всех изменённых секций. Это просто функция functions.hashпримененная к каждому полю и просуммированная по всей секции;</p></li><li><p>Рассчитанные контрольные суммы сравниваются с контрольными суммами части полей исходной таблицы. У исходной таблицы нет предрасчитанных контрольных сумм, поэтому их приходится вычислять каждый раз.</p></li></ol><p>К сожалению, и этот подход оказался наивным, что видно из следующей таблицы, которая содержит практические показатели работы приложения:</p><p>Таблица 1</p><div><div class="table"><table><tbody><tr><td><p><strong>Размер таблицы, Тб</strong></p></td><td><p><strong>Количество секций</strong></p></td><td><p><strong>Обновляется секций</strong></p></td><td><p><strong>Время работы шага 2</strong></p></td><td><p><strong>Время работы шагов 3 и 4</strong></p></td><td><p><strong>Время работы шага 5</strong></p></td><td><p><strong>Суммарное время работы</strong></p></td></tr><tr><td><p>30</p></td><td><p>2000</p></td><td><p>500-600</p></td><td><p>2 часа</p></td><td><p>8-10 часов</p></td><td><p>2 часа</p></td><td><p>12-14 часов</p></td></tr><tr><td><p>7</p></td><td><p>2500</p></td><td><p>1000-1800</p></td><td><p>3 часа и более</p></td><td><p>4 часа</p></td><td><p>3 часа</p></td><td><p>10 часов</p></td></tr></tbody></table></div></div><p> </p><p>Требовалось найти лучший способ применения дельты к основному набору данных, и он появился.  Основная идея – сделать так, чтобы при обращении к таблице, особенно, если нужно переписать часть файлов, было можно работать только с нужными файлами, которых, как правило, меньшинство, и получить результат значительно быстрее.</p><p>Это достигается тремя уровнями разделения файлов:</p><ul><li><p>Опциональные:</p></li></ul><p>1.     Секционирование Hive: создание директорий, в названиях которых содержатся значения некоторого поля, а в них, в свою очередь, содержатся файлы с данными.</p><p>2.     Неявное секционирование: при этом партиции Hive не создаются, но в каждый файл должны попадать записи с единственным значением поля неявного секционирования. Значения неявных секций будут сохраняться в именах файлов, но получать их, читая все файлы, непрактично, поэтому информация будет сохраняться в отдельной структуре, называемой «карта данных».<br/>При этом важно избегать коллизий, поэтому стандартный способ HashPartitioning, используемый в Spark для shuffleможет оказаться неприемлемым.</p><ul><li><p>Обязательный:</p></li></ul><p>1.     Разделение по непересекающимся интервалам значений единственного поля. Это поле в дальнейшем будет называться "<em>поле упорядочивания</em>".</p><p>Рассмотрим это в схематичном виде:</p><figure class=""><img src="/img/image-loader.svg" alt="Рисунок 2. Таблица с явными и неявными секциями" title="Рисунок 2. Таблица с явными и неявными секциями" height="432" data-src="https://habrastorage.org/getpro/habr/upload_files/909/5b1/bcc/9095b1bcc15e9091691ccd20069175c3.png" data-width="325"/><figcaption>Рисунок 2. Таблица с явными и неявными секциями</figcaption></figure><p>На <em>рисунке 2</em> показан самый полный вариант использования DataSource. Обеспечивает секции, которые видны обычным, не знающим о внутреннем устройстве этого источника данных, инструментам: select * from hive_db.hive_table; sparkSession.table("hive_db.hive_table"); sparkSession.read.format("any_format").load("<a href="http://hdfs://nameservice/path/to/table"><u>hdfs://nameservice/path/to/table</u></a>").</p><p>Пользователи, пользующиеся такими инструментами, будут видеть эту таблицу как совершенно обычную таблицу с секциями, возможно, многоуровневыми (по нескольким полям, не обязательно по одному).</p><p>В то же время в каждой папке есть секции, которые внешним средствам не видны. Да, они содержатся в имени файла, но только для предупреждения, что файл не так прост, как может показаться. Чтение метаинформации в любом случае производится из карты данных.</p><figure class=""><img src="/img/image-loader.svg" alt="Рисунок 3. Таблица с только явными секциями Hive" title="Рисунок 3. Таблица с только явными секциями Hive" height="432" data-src="https://habrastorage.org/getpro/habr/upload_files/673/63e/a2c/67363ea2c8432f75acdc8d9ab1244485.png" data-width="325"/><figcaption>Рисунок 3. Таблица с только явными секциями Hive</figcaption></figure><p>Если посмотреть <em>на рисунок 3</em>, то видно, что для пользователей с обычными, не знающими этого DataSource, средствами это точно такая же таблица, как обычная таблица с секционированием Hive или таблица на рисунке 1. По сравнению с первым вариантом разницы совсем немного, отсутствие неявных секций не блокирует работу источника данных.</p><figure class=""><img src="/img/image-loader.svg" alt="Рисунок 4. Таблица только с неявными секциями" title="Рисунок 4. Таблица только с неявными секциями" height="440" data-src="https://habrastorage.org/getpro/habr/upload_files/e4f/0b0/4a0/e4f0b04a01b8ed7f3dc7827b54ef6d18.png" data-width="314"/><figcaption>Рисунок 4. Таблица только с неявными секциями</figcaption></figure><p>На <em>рисунке 4</em> показан вариант таблицы, в которой нет явных секций (да, так тоже можно), поэтому пользователи ванильных инструментов будут видеть её как обычную таблицу, не разделённую на части. При этом процесс обновления получает преимущества от выделения файлов по значениям полей неявного секционирования.</p><p>Если пользователи не требуют секционирование, а данные, содержащиеся в таблице, дают возможность фрагментирования, то этим вариантом стоит воспользоваться.</p><figure class=""><img src="/img/image-loader.svg" alt="Рисунок 5. Таблица без явных и неявных секций" title="Рисунок 5. Таблица без явных и неявных секций" height="422" data-src="https://habrastorage.org/getpro/habr/upload_files/7d7/e19/98a/7d7e1998a564103de10de0cbfa5ebe35.png" data-width="314"/><figcaption>Рисунок 5. Таблица без явных и неявных секций</figcaption></figure><p>На <em>рисунке 5</em> изображен вариант, который содержит требующую обновления таблицу, но секционирование не требуется и не найден подходящий способ сделать его неявным. В таком случае, можно отказаться от создания секций и просто выбрать поле упорядочивания. Таблица при этом видна как обычная, "плоская" структура.</p><p>Я назвал этот способ "<em>организованная таблица</em>", потому что в сравнении с предыдущим способом, данные явно упорядочены, организованы весьма строгим образом. Идея формата организованной таблицы имеет немало общего с форматами Apache Iceberg и Apache Hudi, впрочем, при разработке я был вдохновлён Apache Iceberg, а о том, что в Hudi тоже используется схожая идея (интервалы значений) узнал значительно позже, когда основная часть уже была реализована. Кроме того, я пытался не совершить ошибок, которые сделаны в формате CarbonData, мой опыт использования его скорее негативный. И то, что Delta lake тоже содержит схожий подход: zOrder, стало очевидно позже.</p><p>Вначале я представил проверку концепции (<em>далее PoC - от англ. «Proof of concept»</em>) - скрипты, которые выполнял в spark-shell. Это заняло примерно неделю. За это время я исследовал, как сохранить таблицу так, чтобы конечные файлы содержали непересекающиеся интервалы значений одного конкретного поля, первичного ключа, или поля составного первичного ключа с наибольшей селективностью. Но использование именно первичного ключа не обязательно, ведь можно взять любое другое поле, если оно обеспечивает высокую селективность (но не обязательно уникальное) и не содержит большого количества одинаковых значений, неважно null это, или какое-то конкретное значение. Очень хорошо, если это поле сможет разместить записи с высокой и редкой частотой изменения в разные группы файлов.</p><p>Также я смог выделить поле с низкой селективностью и сохранил файлы с разными значениями этого поля в отдельных группах. Это было поле неявного секционирования, которое помогло в выделении изменяющихся файлов. Я проработал сохранение нужной информации о полученных файлах, назвав дополнительную структуру картой данных (data map). И использовал карту данных для анализа дельты данных и выбора нужных файлов.</p><p>Затем я применял к этим файлам операции из шагов 3 и 4 (помните тот список над таблицей 1 выше?), записывал их и заменял старые файлы новыми.</p><p>Все операции занимали около пятнадцати минут, и я видел, что новые файлы суммарно составляют около 200 Гб (размер таблицы около 27 Тб), то есть очень небольшую часть таблицы.</p><p>Всё это было сделано стандартными средствами Spark и hadoop.FileSystem. Многие операции не были оптимальными, например, чтобы минимизировать коллизии хеширования (разделение по модулю хеша - стандартный способ партиционирования RDD), я увеличивал количество партиций RDD в восемь раз от требуемого.</p><p>Следующие два месяца ушли на разработку прототипа, который, как и PoC, содержал одну дополнительную деталь, дельта-файлы. В них должны были уходить новые записи, если они относились к файлу, в котором ни одна запись не удалялась (или не обновлялась, помните, я говорил, что обновления обрабатываются нами как удаление плюс вставка). В дальнейшем от дельта-файлов пришлось отказаться, но, возможно, кто-то предложит алгоритм, в котором они будут уместны.</p><p>Прототип работал достаточно долго, примерно год, и показал свою эффективность. Время основного обновления существенно сократилось:</p><p>Таблица 2</p><div><div class="table"><table><tbody><tr><td><p><strong>Размер таблицы, Тб</strong></p></td><td><p><strong>Количество секций</strong></p></td><td><p><strong>Обновляется секций</strong></p></td><td><p><strong>Обновляется файлов</strong></p></td><td><p><strong>Время применения дельты к основному набору данных</strong></p></td><td><p><strong>Суммарное время работы</strong></p></td></tr><tr><td><p>30</p></td><td><p>2000</p></td><td><p>500-600</p></td><td><p>6000-10000</p></td><td><p>1,5-2 часа</p></td><td><p>5,5-6 часов</p></td></tr></tbody></table></div></div><p> </p><p>Впрочем, ни одна другая операция не изменилась, поэтому мы пришли к выводу о необходимости отключения логических проверок. Но это было вынужденное решение, ведь проверки позволяют нам уверенно говорить, что мы работаем профессионально и не поставляем некорректные данные пользователям.</p><p><strong>Стало ясно, что требуется оптимизация, которая позволит вывести работу с данными на новый уровень, и эта оптимизация привела к решению о создании отдельной библиотеки, основанной на Spark.</strong></p><p>Для полноценной библиотеки, которую могли бы применять пользователи, разумно взять за основу существующие внешний и внутренний API Spark и реализовать идеи так эффективно, как он позволит.</p><p>Я писал о создании DataSource, но он будет максимально полезен в сочетании с другими структурами: партишенер, FileIndex, FileWriter, различные логические и физические планы, средства работы с метаданными и т. д.</p><p><strong>Все эти средства направлены на то, чтобы избавить нас от энтропийных процессов, а если возникает нештатная ситуация, то помочь взять энтропию под контроль.</strong></p><p>Ниже, на <em>рисунке 6 </em>показано, как должно выглядеть обновление таблицы. Нет переходов с нарастанием энтропии, метаинформация бережно сохраняется от состояния к состоянию и каждый шаг с пользой может эти метаданные применить:</p><figure class="full-width "><img src="/img/image-loader.svg" alt="Рисунок 6. Обновление секционированной таблицы" title="Рисунок 6. Обновление секционированной таблицы" height="832" data-src="https://habrastorage.org/getpro/habr/upload_files/30f/5ce/de4/30f5cede47c43a318408e4e5e7d320b9.png" data-width="1973"/><figcaption>Рисунок 6. Обновление секционированной таблицы</figcaption></figure><h3>Компоненты библиотеки: Partitioner</h3><p>Начать описание созданной библиотеки будет логично с партишенера. Дело в том, что партишенер представляет способ организации данных как для хранения, так и для работы с ними. Именно в результате его работы будут получены такие данные, которые можно записать, прочитать и эффективно обработать. Без партишенера процесс не может ни начаться, ни продолжиться, так что именно он идёт самым первым пунктом.</p><p>Общепринятый вариант для Hive и Spark – это секционирование по хешу одного или нескольких полей. Этот вариант хорош и удобен, потому что есть возможность обеспечить одинаковые партиции для одинаковых данных, при этом не требуется передавать состояние между инстансами партишенера. Формула хеширования обеспечивает одинаковое конечное распределение для любого набора данных.</p><p>Вот как это работает:</p><p>1.     Создаётся экземпляр кейс-класса логического плана выполнения org.apache.spark.sql.catalyst.plans.logical.RepartitionByExpression.</p><p>2.     Переданные в него выражения spark используются для создания экземпляра кейс-класса физического плана выполнения org.apache.spark.sql.catalyst.plans.physical.HashPartitioning</p><p>3.     Кейс-класс org.apache.spark.sql.execution.exchange.ShuffleExchangeExec обеспечивает создание для RDD анонимного класса org.apache.spark.Partitioner. Сам класс Partitioner определён как абстрактный и содержит два нереализованных метода:</p><div><div class="table"><table><tbody><tr><td><p><strong>abstract</strong> <strong>class</strong> Partitioner <strong>extends</strong> Serializable {</p><p>    <strong>def</strong> numPartitions<strong>:</strong> Int</p><p>    <strong>def</strong> getPartition(key<strong>:</strong> Any)<strong>:</strong> Int</p><p>}</p></td></tr></tbody></table></div></div><p>4. По выбранным для партиционирования полям рассчитывается неотрицательный модуль хеша по числу партиций (выражение из класса HashPartitioning):</p><div><div class="table"><table><tbody><tr><td><p>Pmod(<strong>new</strong> Murmur3Hash(expressions), Literal(numPartitions))</p></td></tr></tbody></table></div></div><p>Теперь несложно обозначить основные характеристики этого алгоритма.</p><p>Как видно, для физического расчета будет использован org.apache.spark.unsafe.hash.Murmur3_x86_32.<br/>Полученное неотрицательное число (Pmod(hash)) всегда одно и тоже для одних и тех же аргументов. Таким образом, состояние заложено в формуле хеширования и не должно передаваться между инстансами Partitioner.</p><p>Причина выбора именно такой реализации очевидна: тот же самый алгоритм применяется для формирования бакетов Hive. Поэтому таблица Hive, которую сохранили с указанием полей bucketBy(“DISTRIBUTED BY” в HiveQL), по распределению эквивалентна репартиционированному датасету.</p><p>Нужно отметить, что все расчёты производятся для сериализованных экземпляров класса InternalRow с использованием unsafe memory, и выполняются очень эффективно. Поэтому замена этого процесса на операции с RDD[Row] и попытка создания простого партишенера, который получал бы значение для хеша из конкретных полей, приведёт к большому объёму сериализации-десериализации, а это вызовет крайне серьёзное замедление.</p><p><strong>С моей точки зрения, применение HashPartitioning имеет существенные недостатки: </strong></p><p><strong>1)</strong> <strong>Нет возможности разделить значения на часто обновляемые и редко обновляемые</strong>. Значения Murmur3Hash ничего не говорят об исходных данных, задача хеша в этом случае - распределить значения для равномерного использования всех полученных элементов набора данных, будь это файл или итератор по записям.</p><p>Наша же задача - сделать так, чтобы как можно меньше файлов были перезаписаны, а практика показывает, что переписываемые значения склонны к кластеризации, то есть, с большей вероятностью изменения придут по нескольким записям, идущим достаточно близко одна к другой.</p><p>При хешировании кластер значений должен быть разбит применяемой хеш-функцией и если он будет разделён на отдельные записи, то хеш-функция хорошо справилась со своей задачей. Это равномерное распределение очень полезно, когда нужно выполнить, например, Join всех данных, но при обновлении реальных данных часто бывает, что обновления составляют тысячные доли от всего объема таблицы. Поэтому разумно кластеризовать данные по определённому признаку, а не «разбрасывать» через хеш.</p><p>Да, может помочь LSH – Locality Sensitive Hash – хэш с учётом близости значений, но его также надо реализовывать для произвольного типа данных, а образец в виде RangePartitioner уже выполняет требуемое разделение, причём совершенно точно, и делает в перспективе возможной работу с интервалами значений и временными рядами! Кроме того, LSHпартишенер не обязательно поможет в динамическом увеличении количества бакетов, ведь функцию придётся модернизировать для нового числа секций.</p><p>Говоря более общим языком, фрагментирование набора данных по хешам предназначено для глобализации использования элементов данных, фрагментирование по интервалам значений предназначено для локализации использования элементов.</p><p><strong>2)</strong> <strong>Ранее было сказано, что хеш всегда один и тот же для одинаковых аргументов, но верно и то, что хеш будет одним и тем же для разных аргументов</strong>.</p><p>Дело не в арифметическом модуле, который сильно ограничивает количество вариантов на выходе, а в том, что множество хешей в общем случае слабее множества исходных значений.</p><p>Поэтому я и обращаю внимание на то, что коллизии не просто могут появиться, а утверждаю, что они обязательно появятся в ходе работы процесса. И это не вполне оптимально, ведь в одну и ту же партицию RDD могут попасть значения, принадлежащие разным секциям Hive. Spark может корректно обработать и записать такой файл, но, как правило, это займёт больше времени, потребуются дополнительные проверки и сортировки.</p><p>Кроме того, неявное секционирование затруднено при этом способе, потому что стандартный FileWriter не сможет разделить данные, относящиеся к неявным секциям. Требуется сначала записать их как явные, а затем переименовать каждый записанный файл. Но это не главная причина, самое серьёзное последствие коллизий во время Shuffle наступает, если файл или итератор по данным включает в себя несколько (хотя бы два) непоследовательных интервала значений поля упорядочивания. «Непоследовательных» означает, что между этими интервалами в секции или таблице, если секционирование отсутствует, точно есть другие значения.</p><p>Разделить непоследовательные диапазоны не представляется возможным без дополнительных затрат, выбранная схема хранения метаданных не поддерживает несколько интервалов в одном файле. Такая коллизия приведёт к созданию "компонента", содержащего несколько файлов.</p><p>Компонент в конечном итоге содержит один интервал значений поля упорядочивания, не пересекающийся с прочими, но этот интервал может быть слишком большим и приведёт к длительному выполнению одного Spark task.</p><p><strong>3)</strong> <strong>Если хочется совместить физическое секционирование Hive и бакетирование, то в каждой партиции может появиться файл, принадлежащий определённому бакету</strong>.</p><p>Получится, что бакет разделён на несколько файлов, находящихся в разных секциях. При этом, если бакеты сортируются по некоторым полям, то сортировка будет утрачена, нет реализованного способа прочитать разные файлы так, чтобы итератор от первой записи первого файла до последней записи последнего файла был отсортирован.</p><p>Таким образом, приложенные усилия по организации данных окажутся напрасными, данные придётся сортировать снова и снова, и делать это для всего набора данных, потому что Spark из коробки не умеет находить партиции RDD, по которым нужно сделать сортировку.</p><p>Данные для нового DataSource должны быть разделены по диапазонам значений поля упорядочивания (кластеризованы по этому полю), и алгоритм будет значительно отличаться от описанного способа. Зато алгоритм будет похож на тот, что используется в dataframe.repartitionByRange().</p><p>Опишем для начала алгоритм repartitionByRange():</p><p>1.     Для итераций по RDD будет использован экземпляр класса MutablePair, первый элемент которого будет содержать всю сериализованную запись. Это делается для того, чтобы в каждой секции RDD мы работали бы только с одним объектом, а не создавали большое количество UnsafeRow</p><p>2.     Создается implicit val ordering = new LazilyGeneratedOrdering(sortingExpressions, outputAttributes). Это значение нужно для того, чтобы сравнивать и сортировать значения переданных выражений, оно тоже применяется к InternalRow;</p><p>3.     Созданный экземпляр RangePartitioner получает RDD, количество партиций, направление сортировки (по умолчанию-возрастающее, аргумент типа Boolean) и желаемое количество сэмплов в одной секции RDD;</p><p>4.     Вычисляются верхние границы для каждой будущей секции RDD:</p><p>o  Немного пересчитав желаемое количество сэмплов, класс приступает к сэмплированию RDD. Делается это так:<br/><br/>- желаемое количество первых записей помещается в массив фиксированного размера; если записи на этом кончились, то массив урезается до реального размера и возвращается;</p><p>- если же записи ещё есть, то цикл проходит по записям и записывает каждую следующую вместо одной случайной записи в массив. Одновременно подсчитывается общее количество. Полученный результат передаётся на драйвер;</p><p>o   Теперь, с известным размером партиций и полученными массивами сэмплов, класс проводит оценку, нет ли больших секций, из которых получено недостаточно записей;</p><p>o   Если такие секции встретились, то они дополнительно сэмплируются с использованием очень интересного класса PartitionPruningRDD (описание этого класса выходит за рамки этой статьи). Сэмплирование осуществляется стандартным методом RDD.sample. Полученный результат передаётся на драйвер;</p><p>o   Полученные InternalRow, а их количество сейчас заметно больше, чем будет в итоге, чтобы обеспечить приблизительно равный размер секций статистически, сортируются с использованием ordering, и производится отбор верхних границ, исходя из накопительной суммы весов.</p><p>5.     Определяется метод бинарного поиска нужного интервала (меньше или равен определённой верхней границе, больше предшествующей верхней границы), который должен ускорить обработку больших наборов данных;</p><p>6.     Во время работы класса, полученная запись сравнивается с границами методом линейного поиска, или, если будущих секций больше, чем 128, то бинарным поиском находится, к какой секции она относится;</p><p>7.     Класс должен быть сериализуемым, чтобы его можно было передавать на экзекуторы, поэтому в него добавлены методы writeObject и readObject.</p><p><strong>Я хочу уточнить, почему меня не устроила реализация RangePartitioner:</strong></p><p>1)  Он не имеет разделяемого состояния.</p><p>Псевдослучайные функции для выбора сэмпла RDD, даже сидированные одним и тем же значением, не могут обеспечить полностью стабильную выборку, границы партиций RDD могут быть определены по-разному даже для одного и того же набора данных. Если же наборы данных разные, то одни и те же значения поля упорядочивания из каждого набора могут находится в разных секциях RDD, ведь RangePartitioner знает о распределении только одного набора данных. Поле rangeBounds приватное и не может быть передано в другой RangePartitioner, а если бы оно и было открыто, RangePartitionerне может использовать готовую информацию о распределении;</p><p>2) Сэмплирование осуществляется в общем случае, два раза, если находятся слишком большие партиции;</p><p>3) Материализация предварительного результата на драйвере выглядит преждевременной, ведь часть bigdata – это тоже bigdata, не исключено, что памяти на драйвере не хватит, если мы пытаемся разделить таблицу размером в десятки или сотни терабайт. Лучше произвести отбор верхних границ в RDD, и получить на драйвере только необходимые данные;</p><p>4) Наша задача может требовать фиксированное разделение по секциям Hive, и динамическое формирование партиций RDD, "вложенное" в это строгое разделение, а в RangePartitioner существует только вариант с сортировкой по всем полям. При сохранении этот порядок может измениться, поскольку из одной партиции наверняка будут получены несколько файлов;</p><p>5) Я решил использовать как основной аргумент не желаемое количество партиций RDD, а желаемое количество записей в файле – это в конечном итоге должно привести к лучшему распределению.</p><p>Кроме того, фиксированные секции станут ключами карты (Map[InternalRow, Array[InternalRow]) и каждый элемент карты будет содержать массив с верхними границами конкретной партиции, а это позволит уменьшить порядок бинарного поиска.</p><p>6)  Получить весь InternalRow нужно, если мы собираемся выполнить <em>dataframe.repartitionAndSortWithinPartitions</em>. Это очень полезный метод, позволяющий внести локальную сортировку в shuffle. Но мы собираемся сортировать по полям, в общем случае отличным от поля упорядочивания, а значит можно сразу же выделить необходимые для партиционирования колонки и сократить объем получаемых данных потенциально в десятки или сотни раз, и заодно сделать партишенер подходящим для секционирования совершенно других наборов данных, в которых есть эквивалентные поля. Именно этот пункт поможет с реализацией передачи состояния (см. первый пункт этого списка) и с локальной сортировкой, объединённой с shuffle!</p><p>Поскольку текст уже весьма большой, сама реализация будет в следующей части.</p></div></div> <!----> <!----></div> <div class="tm-article-presenter__meta"><div class="tm-separated-list tm-article-presenter__meta-list"><span class="tm-separated-list__title">Теги:</span> <ul class="tm-separated-list__list"><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bspark%5D" class="tm-tags-list__link">spark</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D1%82%D0%B0%D0%B1%D0%BB%D0%B8%D1%86%D1%8B%5D" class="tm-tags-list__link">таблицы</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bdata%5D" class="tm-tags-list__link">data</a></li></ul></div> <div class="tm-separated-list tm-article-presenter__meta-list"><span class="tm-separated-list__title">Хабы:</span> <ul class="tm-separated-list__list"><li class="tm-separated-list__item"><a href="/ru/company/sberbank/blog/" class="tm-hubs-list__link router-link-active">
    Блог компании Сбер
  </a></li><li class="tm-separated-list__item"><a href="/ru/hub/db_admins/" class="tm-hubs-list__link">
    Администрирование баз данных
  </a></li><li class="tm-separated-list__item"><a href="/ru/hub/bigdata/" class="tm-hubs-list__link">
    Big Data
  </a></li></ul></div></div></article></div> <!----></div> <div class="tm-article-sticky-panel"><div class="tm-data-icons tm-article-sticky-panel__icons"><div class="tm-article-rating tm-data-icons__item"><div class="tm-votes-meter tm-article-rating__votes-switcher"><svg height="16" width="16" class="tm-svg-img tm-votes-meter__icon tm-votes-meter__icon_medium"><title>Всего голосов 3: ↑3 и ↓0</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-rating"></use></svg> <span title="Всего голосов 3: ↑3 и ↓0" class="tm-votes-meter__value tm-votes-meter__value_positive tm-votes-meter__value_medium">+3</span></div> <DIV class="v-portal" style="display:none;"></DIV></div> <!----> <span title="Количество просмотров" class="tm-icon-counter tm-data-icons__item"><svg height="16" width="16" class="tm-svg-img tm-icon-counter__icon"><title>Просмотры</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-views"></use></svg> <span class="tm-icon-counter__value">939</span></span> <button title="Добавить в закладки" type="button" class="bookmarks-button tm-data-icons__item"><span title="Добавить в закладки" class="tm-svg-icon__wrapper bookmarks-button__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Добавить в закладки</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-favorite"></use></svg></span> <span title="Количество пользователей, добавивших публикацию в закладки" class="bookmarks-button__counter">
    21
  </span></button> <!----> <div title="Поделиться" class="tm-sharing tm-data-icons__item"><button type="button" class="tm-sharing__button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="tm-sharing__icon"><path fill="currentColor" d="M10.33.275l9.047 7.572a.2.2 0 010 .306l-9.048 7.572a.2.2 0 01-.328-.153V11c-8 0-9.94 6-9.94 6S-1 5 10 5V.428a.2.2 0 01.328-.153z"></path></svg></button> <!----></div> <DIV class="v-portal" style="display:none;"></DIV></div> </div></div> <!----> <!----> <div class="tm-article-presenter__footer"><div class="tm-article-blocks"><!----> <section class="tm-block tm-block_spacing-bottom"><!----> <div class="tm-block__body tm-block__body_variant-balanced"><div class="tm-article-author"><div class="tm-article-author__company"><div class="tm-article-author__company-card"><div class="tm-company-snippet"><a href="/ru/company/sberbank/profile/" class="tm-company-snippet__logo-link"><div class="tm-entity-image"><img alt="" height="40" src="//habrastorage.org/getpro/habr/company/9db/3c1/ec0/9db3c1ec02265b8bcbfdfb0d23d8b9f2.jpg" width="40" class="tm-entity-image__pic"></div></a> <div class="tm-company-snippet__info"><a href="/ru/company/sberbank/profile/" class="tm-company-snippet__title">Сбер</a> <div class="tm-company-snippet__description">Больше чем банк</div></div></div> <div class="tm-article-author__buttons"><!----> <!----></div></div> <!----> <div class="tm-article-author__separator"></div></div> <div class="tm-user-card tm-article-author__user-card tm-user-card_variant-article"><div class="tm-user-card__info-container"><div class="tm-user-card__header"><div class="tm-user-card__header-data"><a href="/ru/users/Sber/" class="tm-user-card__userpic tm-user-card__userpic_size-40"><div class="tm-entity-image"><img alt="" src="//habrastorage.org/getpro/habr/avatars/b1f/595/703/b1f595703fffc92491048ef9b6882dff.png" class="tm-entity-image__pic"></div></a> <div class="tm-user-card__meta"><div title=" 35 голосов " class="tm-karma tm-user-card__karma"><div class="tm-karma__votes tm-karma__votes_positive">
    1
  </div> <div class="tm-karma__text">
    Карма
  </div></div> <div title="Рейтинг пользователя" class="tm-rating tm-user-card__rating"><div class="tm-rating__header"> <div class="tm-rating__counter">26.8</div></div> <div class="tm-rating__text">
    Рейтинг
  </div></div></div></div></div> <div class="tm-user-card__info tm-user-card__info_variant-article"><div class="tm-user-card__title tm-user-card__title_variant-article"><!----> <a href="/ru/users/Sber/" class="tm-user-card__nickname tm-user-card__nickname_variant-article">
          @Sber
        </a> <!----></div> <p class="tm-user-card__short-info tm-user-card__short-info_variant-article">Пользователь</p></div></div> <div class="tm-user-card__buttons tm-user-card__buttons_variant-article"><!----> <!----> <!----> <!----> <!----></div></div> <!----></div> <DIV class="v-portal" style="display:none;"></DIV></div> <!----></section> <div class="tm-article-blocks__comments"><div class="tm-article-page-comments"><div class="tm-article-comments-counter-link tm-article-comments-counter-button"><a href="/ru/company/sberbank/blog/582090/comments/" class="tm-article-comments-counter-link__link tm-article-comments-counter-link__link_button-style"><svg height="16" width="16" class="tm-svg-img tm-article-comments-counter-link__icon tm-article-comments-counter-link__icon_contrasted"><title>Комментарии</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-comments"></use></svg> <span class="tm-article-comments-counter-link__value tm-article-comments-counter-link__value_contrasted">
       Комментировать 
    </span></a> <!----></div></div></div>  <!---->  <!----> <!----></div></div></div></div></div> <div class="tm-page__sidebar"><div class="tm-layout-sidebar"><div class="tm-layout-sidebar__placeholder_initial"></div> <div class="tm-sexy-sidebar tm-sexy-sidebar_initial" style="margin-top:0px;"><!----> <section class="tm-block tm-block_spacing-bottom"><header class="tm-block__header"><h2 class="tm-block__title">Информация</h2> <!----></header> <div class="tm-block__body"><div class="tm-company-basic-info"><dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Дата основания</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap"><time datetime="1841-11-11T21:29:43.000Z" title="1841-11-12, 00:00">12  ноября  1841</time></dd></dl> <dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Местоположение</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap">
    Россия
  </dd></dl> <dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Сайт</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap"><a href="http://www.sber.ru/" target="_blank" class="tm-company-basic-info__link">
      www.sber.ru
    </a></dd></dl> <dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Численность</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap">
    свыше 10 000 человек
  </dd></dl> <dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Дата регистрации</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap"><time datetime="2011-02-08T10:09:33.000Z" title="2011-02-08, 13:09">8  февраля  2011</time></dd></dl> <!----></div></div> <!----></section> <div class="tm-company-widgets"></div> <!----></div></div></div></div></div></div></main> <!----></div> <div class="tm-footer-menu"><div class="tm-page-width"><div class="tm-footer-menu__container"><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Ваш аккаунт
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/kek/v1/auth/habrahabr/?back=/ru/company/sberbank/blog/582090/&amp;hl=ru" rel="nofollow" target="_self">
                Войти
              </a></li><li class="tm-footer-menu__list-item"><a href="/kek/v1/auth/habrahabr-register/?back=/ru/company/sberbank/blog/582090/&amp;hl=ru" rel="nofollow" target="_self">
                Регистрация
              </a></li></ul></div></div><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Разделы
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/ru/" class="footer-menu__item-link router-link-active">
                Публикации
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/news/" class="footer-menu__item-link">
                Новости
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/hubs/" class="footer-menu__item-link">
                Хабы
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/companies/" class="footer-menu__item-link">
                Компании
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/users/" class="footer-menu__item-link">
                Авторы
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/sandbox/" class="footer-menu__item-link">
                Песочница
              </a></li></ul></div></div><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Информация
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/ru/docs/help/" class="footer-menu__item-link">
                Устройство сайта
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/authors/codex/" class="footer-menu__item-link">
                Для авторов
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/companies/corpblogs/" class="footer-menu__item-link">
                Для компаний
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/docs/transparency/" class="footer-menu__item-link">
                Документы
              </a></li><li class="tm-footer-menu__list-item"><a href="https://account.habr.com/info/agreement" target="_blank">
                Соглашение
              </a></li><li class="tm-footer-menu__list-item"><a href="https://account.habr.com/info/confidential/" target="_blank">
                Конфиденциальность
              </a></li></ul></div></div><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Услуги
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="https://docs.google.com/presentation/d/e/2PACX-1vQLwRfQmXibiUlWaRg-BAc38s7oM3lJiaPju7qmdJsp8ysIvZ_G-Npem0njJLMozE2bPHMpDqiI5hhy/pub?start=false&amp;loop=false&amp;delayms=60000&amp;slide=id.g91a03369cd_4_297" target="_blank">
                Реклама
              </a></li><li class="tm-footer-menu__list-item"><a href="https://habrastorage.org/storage/stuff/habr/service_price.pdf" target="_blank">
                Тарифы
              </a></li><li class="tm-footer-menu__list-item"><a href="https://docs.google.com/presentation/d/e/2PACX-1vQJJds8-Di7BQSP_guHxICN7woVYoN5NP_22ra-BIo4bqnTT9FR6fB-Ku2P0AoRpX0Ds-LRkDeAoD8F/pub?start=false&amp;loop=false&amp;delayms=60000" target="_blank">
                Контент
              </a></li><li class="tm-footer-menu__list-item"><a href="https://tmtm.timepad.ru/" target="_blank">
                Семинары
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/megaprojects/" class="footer-menu__item-link">
                Мегапроекты
              </a></li></ul></div></div></div></div></div> <div class="tm-footer"><div class="tm-page-width"><div class="tm-footer__container"><!----> <div class="tm-footer__social"><a href="https://www.facebook.com/habrahabr.ru" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Facebook</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-facebook"></use></svg></a><a href="https://twitter.com/habr_com" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Twitter</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-twitter"></use></svg></a><a href="https://vk.com/habr" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>VK</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-vkontakte"></use></svg></a><a href="https://telegram.me/habr_com" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Telegram</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-telegram"></use></svg></a><a href="https://www.youtube.com/channel/UCd_sTwKqVrweTt4oAKY5y4w" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Youtube</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-youtube"></use></svg></a><a href="https://zen.yandex.ru/habr" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Яндекс Дзен</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-zen"></use></svg></a></div> <DIV class="v-portal" style="display:none;"></DIV> <button class="tm-footer__link"><!---->
        Настройка языка
      </button> <a href="/ru/about" class="tm-footer__link">
        О сайте
      </a> <a href="/ru/feedback/" class="tm-footer__link">
        Техническая поддержка
      </a> <!----> <a href="/berserk-mode-nope" class="tm-footer__link">
        Вернуться на старую версию
      </a> <div class="tm-footer-copyright"><span class="tm-copyright"><span class="tm-copyright__years">© 2006–2021 </span> <span class="tm-copyright__name">«<a href="https://company.habr.com/" rel="noopener" target="_blank" class="tm-copyright__link">Habr</a>»</span></span></div></div></div></div> <!----> <!----></div> <div class="vue-portal-target"></div></div>
<script>window.__INITIAL_STATE__={"adblock":{"hasAcceptableAdsFilter":false,"hasAdblock":false},"articlesList":{"articlesList":{"582090":{"id":"582090","timePublished":"2021-10-07T07:45:24+00:00","isCorporative":true,"lang":"ru","titleHtml":"Изменить сохранения Spark! Часть первая: разделяй и… сортируй","leadData":{"textHtml":"\u003Cp\u003EВ этой статье вы узнаете о том, какими способами мы пытались обновлять таблицы в&nbsp;Hadoop, содержащие сотни терабайт данных.\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EИ если в начале нашего пути процесс обновления длился несколько часов (до десяти-двенадцати часов), то теперь ему требуется всего тридцать-сорок минут, а использование вычислительных ресурсов уменьшено вдвое!\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003Cp\u003EПри этом была создана библиотека расширения&nbsp;Spark, которая предоставляет&nbsp;DataSource&nbsp;для преобразования данных в файлах в формат этого&nbsp;DataSource, изменения данных командой&nbsp;MERGE&nbsp;через&nbsp;DataFrame&nbsp;API&nbsp;или&nbsp;SQL, а в будущем ещё и&nbsp;UPDATE,&nbsp;DELETE&nbsp;и некоторые операции&nbsp;DDL.\u003C\u002Fp\u003E\u003Cp\u003EФайлы при этом можно будет читать любым привычным способом, ведь они не модифицированы, а метаданные не обязательны для их чтения.\u003C\u002Fp\u003E\u003Cp\u003EВы увидите код этой библиотеки на языке&nbsp;Scala, который сможете использовать, а может быть даже доработать и поделиться своими успехами.\u003C\u002Fp\u003E\u003Cp\u003EЯ постараюсь пояснить, почему был сделан тот или иной выбор, но могу умолчать о чём-то, что кажется очевидным, или, наоборот, о чём я не имею представления. Вы сможете задать вопросы, а я постараюсь ответить на них.\u003C\u002Fp\u003E\u003Cp\u003EЭто первая статья из нескольких, и в ней будет рассказано только о немногих реализованных классах (они нужны для распределения данных определённым способом), поэтому наберитесь терпения, я расскажу всё по частям.&nbsp;Впрочем, пора перейти к повествованию.\u003C\u002Fp\u003E","imageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F872\u002F21f\u002F373\u002F87221f373acb676fa493e066662a3872.jpg","buttonTextHtml":"Читать далее","image":{"url":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F872\u002F21f\u002F373\u002F87221f373acb676fa493e066662a3872.jpg","fit":"cover","positionY":0,"positionX":0}},"editorVersion":"2.0","postType":"article","postLabels":[],"author":{"scoreStats":{"score":1,"votesCount":35},"rating":26.8,"relatedData":null,"contacts":[],"authorContacts":[],"paymentDetails":{"paymentYandexMoney":null,"paymentPayPalMe":null,"paymentWebmoney":null},"id":"1713571","alias":"Sber","fullname":null,"avatarUrl":"\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Favatars\u002Fb1f\u002F595\u002F703\u002Fb1f595703fffc92491048ef9b6882dff.png","speciality":"Пользователь"},"statistics":{"commentsCount":0,"favoritesCount":21,"readingCount":939,"score":3,"votesCount":3},"hubs":[{"relatedData":null,"id":"17155","alias":"sberbank","type":"corporative","title":"Блог компании Сбер","titleHtml":"Блог компании Сбер","isProfiled":false},{"relatedData":null,"id":"17681","alias":"db_admins","type":"collective","title":"Администрирование баз данных","titleHtml":"Администрирование баз данных","isProfiled":true},{"relatedData":null,"id":"17795","alias":"bigdata","type":"collective","title":"Big Data","titleHtml":"Big Data","isProfiled":true}],"flows":[{"id":"1","alias":"develop","title":"Разработка"},{"id":"6","alias":"admin","title":"Администрирование"}],"relatedData":null,"textHtml":"\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\"\u003E\u003Cp\u003E\u003Cem\u003EАвтор: Иван Калининский, участник профессионального сообщества Сбера SberProfi DWH\u002FBigData.\u003C\u002Fem\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cem\u003EПрофессиональное сообщество SberProfi DWH\u002FBigData отвечает за развитие компетенций в таких направлениях, как экосистема Hadoop, Teradata, Oracle DB, GreenPlum, а также BI инструментах Qlik, SAP BO, Tableau и др.\u003C\u002Fem\u003E\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F872\u002F21f\u002F373\u002F87221f373acb676fa493e066662a3872.jpg\" width=\"1200\" height=\"630\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F872\u002F21f\u002F373\u002F87221f373acb676fa493e066662a3872.jpg\" data-blurred=\"true\"\u002F\u003E\u003Cfigcaption\u003E\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EВ этой статье вы узнаете о том, какими способами мы пытались обновлять таблицы в Hadoop, содержащие сотни терабайт данных.\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EИ если в начале нашего пути процесс обновления длился несколько часов (до десяти-двенадцати часов), то теперь ему требуется всего тридцать-сорок минут, а использование вычислительных ресурсов уменьшено вдвое!\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003Cp\u003EПри этом была создана библиотека расширения Spark, которая предоставляет DataSource для преобразования данных в файлах в формат этого DataSource, изменения данных командой MERGE через DataFrame API или SQL, а в будущем ещё и UPDATE, DELETE и некоторые операции DDL.\u003C\u002Fp\u003E\u003Cp\u003EФайлы при этом можно будет читать любым привычным способом, ведь они не модифицированы, а метаданные не обязательны для их чтения.\u003C\u002Fp\u003E\u003Cp\u003EВы увидите код этой библиотеки на языке Scala, который сможете использовать, а может быть даже доработать и поделиться своими успехами.\u003C\u002Fp\u003E\u003Cp\u003EЯ постараюсь пояснить, почему был сделан тот или иной выбор, но могу умолчать о чём-то, что кажется очевидным, или, наоборот, о чём я не имею представления. Вы сможете задать вопросы, а я постараюсь ответить на них.\u003C\u002Fp\u003E\u003Cp\u003EЭто первая статья из нескольких, и в ней будет рассказано только о немногих реализованных классах (они нужны для распределения данных определённым способом), поэтому наберитесь терпения, я расскажу всё по частям. Впрочем, пора перейти к повествованию.\u003C\u002Fp\u003E\u003Cfigure class=\"\"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" height=\"300\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F4ca\u002Fb99\u002F7e5\u002F4cab997e5c17f6756bc26314251202e7.png\" data-width=\"351\"\u002F\u003E\u003Cfigcaption\u003E\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EДля начала договоримся о терминах и определениях. Если это вам пока неинтересно или уже необязательно, переходите к следующему пункту, а если встретите незнакомый или сомнительный термин, вернитесь сюда. Возможно, здесь вы найдёте разъяснение.\u003C\u002Fp\u003E\u003Ch3\u003EТермины и определения\u003C\u002Fh3\u003E\u003Cp\u003E\u003Cstrong\u003EТаблица\u003C\u002Fstrong\u003E: В нашем случае это копия таблицы из какой-либо RDBMS, сохраненная в одной директории HDFS(\"\u003Ca href=\"http:\u002F\u002Fhdfs:\u002F\u002Fnameservice\u002Fpath\u002Fto\u002Ftable\"\u003E\u003Cu\u003Ehdfs:\u002F\u002Fnameservice\u002Fpath\u002Fto\u002Ftable\u003C\u002Fu\u003E\u003C\u002Fa\u003E\") в файловом формате. Используемый нами в большинстве случаев файловый формат – Parquet, допустимы и другие форматы, например ORC, Avro, и даже CSV. Определение таблицы сохраняется в Hive с типом EXTERNAL. Мы не используем MANAGED таблицы;\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EСекция\u003C\u002Fstrong\u003E: Часть таблицы, выделенная по некоторому правилу. Данные хранятся в отдельных папках в директории таблицы: \"\u002Fpath\u002Fto\u002Ftable\u002Fpart=value1\", \"\u002Fpath\u002Fto\u002Ftable\u002Fpart=value2\".  Правило секционирования может включать несколько полей, тогда секции в HDFS будут выглядеть так: \"\u002Fpath\u002Fto\u002Ftable\u002Fpart1=value_p1\u002Fpart2=value_p2\". Файлы с данными при этом находятся только в листовых папках. Информация о каждой секции должна присутствовать в HiveMetastore;\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003ERDD\u003C\u002Fstrong\u003E: Resilient Distributed Dataset, устойчивый распределённый набор данных, используется в Apache Spark для обработки данных. Он называется распределённым потому, что на кластере состоит из множества частей, которые мы будем называть партициями RDD;\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EПартиция RDD: \u003C\u002Fstrong\u003EЧасть RDD, я называю её партицией, а не секцией, чтобы отделить от секции Hive. В отношении данных - это итератор, который может пройти по всем записям, которые в нём содержатся. Как правило, будет сделан один и только один проход, после этого итератор будет считаться истощённым (exausted);\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EДатафрейм\u003C\u002Fstrong\u003E: Набор данных, можно сказать, что это RDD и план трансформаций RDD, обеспечиваемый оптимизатором Catalyst;\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EПартишенер\u003C\u002Fstrong\u003E: Класс, наследник org.apache.spark.Partitioner. Нужен для описания правила, по которому из RDD с определённым набором партиций получается RDD с другим набором партиций. Каждая запись исходного RDD должна быть по описанному правилу перемещена в определенную партицию другого RDD;\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EКлючи: \u003C\u002Fstrong\u003EПервичные ключи таблицы, как правило, это одно поле (суррогатный первичный ключ), но полей может быть несколько. Как правило, подразумеваются конкретные значения, ведь сами поля в любом случае есть в каждой записи.\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EКардинальность: \u003C\u002Fstrong\u003EКоличество уникальных значений определенного поля. Также может употребляться в значении общего количества записей или не пустых значений, но здесь мы будем говорить только об уникальных значениях.\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EСелективность: \u003C\u002Fstrong\u003EИзбирательность, мера, показывающая, насколько хорошо будет работать фильтр на основе предиката некоторого выражения. Тесно связана с кардинальностью, ведь если включить существующее значение поля с высокой кардинальностью в фильтр, то будет получено мало записей, возможно, всего одна. Значит, селективность этого фильтра высокая. В то же время, поле с низкой кардинальностью, например поле с типом BOOLEAN, у которого только два возможных значения (null не учитываем), будет иметь низкую селективность, и, если входит в фильтр, в общем случае, выберет половину набора данных.\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003ECDC\u003C\u002Fstrong\u003E:  Change data capture, программное решение для получения изменений данных из RDBMS;\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EShuffle\u003C\u002Fstrong\u003E: Перемешивание, перетасовывание, - операция, которая сопоставляет каждой записи из каждой партиции исходного RDD партицию конечного RDD;\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EБакет\u003C\u002Fstrong\u003E:  Bucket, \"Ведро\", некая объединенная группа значений. В нашем случае представляет собой записи, находящиеся в одном файле или в одной партиции RDD.\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EКейс класс\u003C\u002Fstrong\u003E: Вид класса в Scala, класс-образец.\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EТрейт:\u003C\u002Fstrong\u003E Вид класса в Scala, описывающий поведение, которое может быть «подмешано» в другой класс. Похож на JavaInterface.\u003C\u002Fp\u003E\u003Ch3\u003EСуть проблемы\u003C\u002Fh3\u003E\u003Cp\u003EApache Spark очень много всего умеет. Работает с разными файловыми форматами, с базами данных как RDBMS так и NoSQL, с потоковыми источниками данных. Но, к сожалению, иногда имеющиеся средства не подходят для решения казалось бы простой задачи. Например, есть таблица, хранящаяся в файловом формате в HDFS, а её определение сохранено в Hive. Конечно же, это копия какой-то таблицы из базы данных, а значит, она будет периодически обновляться: часть записей могут быть удалены, часть обновлены и ещё некоторое количество будет добавлено. Однако в файле формата Parquet нельзя ни обновить, ни удалить, ни даже вставить запись. Можно только создать новый файл, а старый удалить. Между тем, в таблице таких файлов может быть сотни тысяч.\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EЕсли это небольшая таблица, несложно переписать её полностью, а вот со структурой, размер которой несколько терабайт или десятков терабайт так работать нельзя.\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003Cp\u003EБольшие таблицы, с которыми мы работаем, разделены на секции, чтобы пользователи могли обращаться к отдельным частям.  Достаточно указать spark.sql.sources.partitionOverwriteMode=dynamic или dataframe.write.option(\"partitionOverwriteMode\", \"dynamic\").save(path), и, начиная с версии 2.3, Spark перепишет только те секции, которые будут обновляться. Однако в то время мы были привязаны к версии 2.2, поэтому пришлось аккуратно сохранять данные в отдельную директорию, и только после успешной записи удалять папки с устаревшими секциями и переименовывать обновлённые и совершенно новые секции, чтобы они оказались в нужной директории.\u003C\u002Fp\u003E\u003Cp\u003EКроме того, даже если бы мы работали с более новой версией Spark, понадобилось бы сделать эффективный join основного набора данных и пришедшей дельты. Spark умеет делать partition pruning только при явно указанных фильтрах при чтении данных (до версии 2.3.5 точно так). В случае join, на том этапе, когда сравниваются значения полей, Spark уже не помнит, что это \"особенные поля\", названия директорий. В одной партиции могут находиться несколько значений полей секционирования, поэтому даже если в условии join указать эти поля, то это ни к чему хорошему не приведёт.\u003C\u002Fp\u003E\u003Cp\u003EПроще говоря, Spark при самом наивном подходе прочитает всю таблицу, сделает полный shuffle, затем то же самое сделает с дельтой, объединит эти два набора данных и запишет всю таблицу заново.\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EЭто будет невообразимо долгий процесс, который может сломаться в середине или ближе к завершению, а значит, так делать нельзя. Взгляните на схему ниже, чтобы понять, что кое-где есть проблемы:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" alt=\"Рисунок 1. Обновление таблицы без потери метаинформации\" title=\"Рисунок 1. Обновление таблицы без потери метаинформации\" height=\"841\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F96f\u002Fa84\u002Fa38\u002F96fa84a385360aa6c226d57dce164d99.png\" data-width=\"1783\"\u002F\u003E\u003Cfigcaption\u003EРисунок 1. Обновление таблицы без потери метаинформации\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EПредставим себе, что мы имеем таблицу с секциями A...Z, причём пришедшая дельта затрагивает только секции A и Z. Схема выше описывает, как пойдёт процесс, если выполнить такой код (обновление таблицы наивным способом):\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode\u003EdfTable.as(\"t\")\n\n  .where('part=\"A\" || 'part=\"Z\")\n\n  .join(dfDelta.as(\"d\"), \"t.part = d.part and t.id = d.id\", \"full_outer\")\n\n  .select(columns: _*) \u002F\u002Fпредположим, что проекция подготовлена заранее и полностью описывает те поля, которые нам нужно получить\n\n  .write\n\n  .mode(\"overwrite\")\n\n  .option(\"partitionOverwriteMode\", \"dynamic\")\n\n  .partitionBy(\"part\")\n\n  .format(\"parquet\")\n\n  .option(\"compression\", \"gzip\")\n\n  .option(\"path\", \"\u002Fpath\u002Fto\u002Ftable\u002F\")\n\n  .saveAsTable(\"hive_db.hive_table\")\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003E Рассмотрим рисунок 1 подробнее (далее мы говорим исключительно о метаинформации; о потере данных речь не идёт, ведь если данные теряются, то никакого смысла в оптимизации нет и нужно полностью пересмотреть подход):\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003Eкрасные пересекающиеся стрелки обозначают энтропийный процесс, при котором теряется первоначально содержащаяся в таблице метаинформация;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eпараллельные синие стрелки обозначают процесс с сохранением метаинформации;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eжелтыми стрелками обозначен противоречивый процесс.  \u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EНа этапе подготовки RDD основную роль играет размер исходных файлов. Значения секций Hive в полученной структуре будут перемешаны, так же, как и отдельные сегменты файлов.\u003C\u002Fp\u003E\u003Cp\u003EЧтение данных полностью повторяет структуру уже перемешанного предыдущего состояния, информация не теряется.\u003C\u002Fp\u003E\u003Cp\u003ESpark понимает, что нужно соединить данные, и, если их размер достаточно большой, то будет выбран SortMergeJoin. Чтобы выполнить этот вид соединения, Spark должен сделать так, чтобы одинаковые ключи из основного набора данных и из дельты расположились совместно (в партициях RDD с одинаковыми индексами) и были отсортированы в пределах партиций. Записи можно только перераспределить и отсортировать заново, затратив на это время и ресурсы. Производится Shuffle: записи каждого набора перемешиваются по значению хешей. Перемешивание устраняет любую сортировку, которая сохранилась при чтении, теперь порядок записей никак не зависит от первоначального.\u003C\u002Fp\u003E\u003Cp\u003ESortMergeJoin делает своё дело, полученный набор данных сохраняет тот же порядок записей, что был после Shuffle и локальной сортировки.\u003C\u002Fp\u003E\u003Cp\u003EПоследний процесс записывает данные обратно в таблицу, удаляя предыдущие версии секций A и Z навсегда! Это противоречивый процесс, поэтому он изображён в виде двойных жёлтых стрелок. Дело в том, что непосредственно перед сохранением записи будут отсортированы по полю part локально (в пределах партиций RDD). Возможно, что сортировка будет получена из предыдущего состояния (SortMergeJoin), ведь часть условия соединения \"t.part = d.part\" заставляет сортировать каждую партицию по этому полю, но если в SparkUI в DAG последним виден синий прямоугольник с надписью Sort, записи всё же будут пересортированы.\u003C\u002Fp\u003E\u003Cp\u003EТакое упорядочивание записей необходимо для последовательного помещения записей в конечный файл, чтобы не переключаться между файлами из разных секций для каждой строки. Поскольку значения секций перемешаны в пределах партиций RDD, то конечные файлы будут меньше, чем ожидает разработчик.\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EКак правило, из одной партиции RDD будет получено столько файлов, сколько есть изменяющихся секций. Если их только две, это может не быть проблемой, а если их тысяча или пять тысяч?\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EПроцесс сохранения возвращает первоначальное состояние, но технические характеристики этого состояния могут быть намного хуже. Теперь очевидно, что нужна оптимизация.\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003Cp\u003EИ тут нужно вспомнить старую мудрость: \u003Cem\u003E«Нельзя заставить машину работать быстрее. Можно только сделать так, чтобы машина выполняла меньше работы»\u003C\u002Fem\u003E.\u003C\u002Fp\u003E\u003Cp\u003EМожно возразить, что затем и нужно горизонтальное масштабирование. Ведь очень просто добавить контейнеры в параметры запуска приложения Spark, подключить к кластеру новые ноды, поставить рядом несколько кластеров. Но это тушение пожара деньгами! Рано или поздно объем данных увеличится, и приложениям, запрашивающим огромные вычислительные ресурсы, будет тесно на любом кластере. Видя проблему, нужно попытаться решить её профессионально, не терять нужную информацию, а наоборот, сохранять и использовать её.\u003C\u002Fp\u003E\u003Ch3\u003EНаш путь решения выявленной проблемы\u003C\u002Fh3\u003E\u003Cp\u003EТак как полное обновление таблицы оказалось неприемлемым, мы разработали приложение, которое обновляет только имеющиеся секции. Способ работы с конкретными изменяющимися секциями получил развитие, и через некоторое время мы достигли такого результата (пошагово):\u003C\u002Fp\u003E\u003Col\u003E\u003Cli\u003E\u003Cp\u003EИзменения, полученные извне, сохраняются с точно таким же секционированием, как и сама таблица. Записи уже предобработаны CDC, и поставляются в таком виде: записи на удаление и на вставку в одном экземпляре, записи на обновление в двух вариантах: предыдущая версия и новая версия. \u003Cbr\u002F\u003E\u003Cbr\u002F\u003EТаким образом, мы работаем с набором данных, в котором есть записи для удаления и записи для добавления. Можно сказать, что каждая секция дельты состоит из двух датафреймов, назовём их dataframeInsert и dataframeDelete. При этом решается проблема миграции записей из одной секции в другую - нет нужды проверять, обновлены ли поля, по которым делается секционирование, или нет, и как-то обрабатывать эти изменения;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EКаждая изменяемая секция основной таблицы (назовём её dataframeMain) проверяется: в ней должны быть ключи на удаление и не должно быть ключей на вставку;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EКаждая секция основной таблицы соединяется с секцией данных, которые должны быть удалены, методом антисоединения: dataframeMain.join(dataframeDelete, primaryKeys, \"left_anti\");\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EК каждой секции основной таблицы добавляется секция данных, которые должны быть вставлены, методомdataframeMain.union(dataframeInsert);\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EКаждая секция основной таблицы проверяется: в ней должны быть ключи вставленных записей и не должно быть ключей на удаление;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EРассчитывается контрольная сумма данных для всех изменённых секций. Это просто функция functions.hashпримененная к каждому полю и просуммированная по всей секции;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EРассчитанные контрольные суммы сравниваются с контрольными суммами части полей исходной таблицы. У исходной таблицы нет предрасчитанных контрольных сумм, поэтому их приходится вычислять каждый раз.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp\u003EК сожалению, и этот подход оказался наивным, что видно из следующей таблицы, которая содержит практические показатели работы приложения:\u003C\u002Fp\u003E\u003Cp\u003EТаблица 1\u003C\u002Fp\u003E\u003Cdiv\u003E\u003Cdiv class=\"table\"\u003E\u003Ctable\u003E\u003Ctbody\u003E\u003Ctr\u003E\u003Ctd\u003E\u003Cp\u003E\u003Cstrong\u003EРазмер таблицы, Тб\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E\u003Cstrong\u003EКоличество секций\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E\u003Cstrong\u003EОбновляется секций\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E\u003Cstrong\u003EВремя работы шага 2\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E\u003Cstrong\u003EВремя работы шагов 3 и 4\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E\u003Cstrong\u003EВремя работы шага 5\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E\u003Cstrong\u003EСуммарное время работы\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd\u003E\u003Cp\u003E30\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E2000\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E500-600\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E2 часа\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E8-10 часов\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E2 часа\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E12-14 часов\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd\u003E\u003Cp\u003E7\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E2500\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E1000-1800\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E3 часа и более\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E4 часа\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E3 часа\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E10 часов\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cp\u003E \u003C\u002Fp\u003E\u003Cp\u003EТребовалось найти лучший способ применения дельты к основному набору данных, и он появился.  Основная идея – сделать так, чтобы при обращении к таблице, особенно, если нужно переписать часть файлов, было можно работать только с нужными файлами, которых, как правило, меньшинство, и получить результат значительно быстрее.\u003C\u002Fp\u003E\u003Cp\u003EЭто достигается тремя уровнями разделения файлов:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003EОпциональные:\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E1.     Секционирование Hive: создание директорий, в названиях которых содержатся значения некоторого поля, а в них, в свою очередь, содержатся файлы с данными.\u003C\u002Fp\u003E\u003Cp\u003E2.     Неявное секционирование: при этом партиции Hive не создаются, но в каждый файл должны попадать записи с единственным значением поля неявного секционирования. Значения неявных секций будут сохраняться в именах файлов, но получать их, читая все файлы, непрактично, поэтому информация будет сохраняться в отдельной структуре, называемой «карта данных».\u003Cbr\u002F\u003EПри этом важно избегать коллизий, поэтому стандартный способ HashPartitioning, используемый в Spark для shuffleможет оказаться неприемлемым.\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003EОбязательный:\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E1.     Разделение по непересекающимся интервалам значений единственного поля. Это поле в дальнейшем будет называться \"\u003Cem\u003Eполе упорядочивания\u003C\u002Fem\u003E\".\u003C\u002Fp\u003E\u003Cp\u003EРассмотрим это в схематичном виде:\u003C\u002Fp\u003E\u003Cfigure class=\"\"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" alt=\"Рисунок 2. Таблица с явными и неявными секциями\" title=\"Рисунок 2. Таблица с явными и неявными секциями\" height=\"432\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F909\u002F5b1\u002Fbcc\u002F9095b1bcc15e9091691ccd20069175c3.png\" data-width=\"325\"\u002F\u003E\u003Cfigcaption\u003EРисунок 2. Таблица с явными и неявными секциями\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EНа \u003Cem\u003Eрисунке 2\u003C\u002Fem\u003E показан самый полный вариант использования DataSource. Обеспечивает секции, которые видны обычным, не знающим о внутреннем устройстве этого источника данных, инструментам: select * from hive_db.hive_table; sparkSession.table(\"hive_db.hive_table\"); sparkSession.read.format(\"any_format\").load(\"\u003Ca href=\"http:\u002F\u002Fhdfs:\u002F\u002Fnameservice\u002Fpath\u002Fto\u002Ftable\"\u003E\u003Cu\u003Ehdfs:\u002F\u002Fnameservice\u002Fpath\u002Fto\u002Ftable\u003C\u002Fu\u003E\u003C\u002Fa\u003E\").\u003C\u002Fp\u003E\u003Cp\u003EПользователи, пользующиеся такими инструментами, будут видеть эту таблицу как совершенно обычную таблицу с секциями, возможно, многоуровневыми (по нескольким полям, не обязательно по одному).\u003C\u002Fp\u003E\u003Cp\u003EВ то же время в каждой папке есть секции, которые внешним средствам не видны. Да, они содержатся в имени файла, но только для предупреждения, что файл не так прост, как может показаться. Чтение метаинформации в любом случае производится из карты данных.\u003C\u002Fp\u003E\u003Cfigure class=\"\"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" alt=\"Рисунок 3. Таблица с только явными секциями Hive\" title=\"Рисунок 3. Таблица с только явными секциями Hive\" height=\"432\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F673\u002F63e\u002Fa2c\u002F67363ea2c8432f75acdc8d9ab1244485.png\" data-width=\"325\"\u002F\u003E\u003Cfigcaption\u003EРисунок 3. Таблица с только явными секциями Hive\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EЕсли посмотреть \u003Cem\u003Eна рисунок 3\u003C\u002Fem\u003E, то видно, что для пользователей с обычными, не знающими этого DataSource, средствами это точно такая же таблица, как обычная таблица с секционированием Hive или таблица на рисунке 1. По сравнению с первым вариантом разницы совсем немного, отсутствие неявных секций не блокирует работу источника данных.\u003C\u002Fp\u003E\u003Cfigure class=\"\"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" alt=\"Рисунок 4. Таблица только с неявными секциями\" title=\"Рисунок 4. Таблица только с неявными секциями\" height=\"440\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fe4f\u002F0b0\u002F4a0\u002Fe4f0b04a01b8ed7f3dc7827b54ef6d18.png\" data-width=\"314\"\u002F\u003E\u003Cfigcaption\u003EРисунок 4. Таблица только с неявными секциями\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EНа \u003Cem\u003Eрисунке 4\u003C\u002Fem\u003E показан вариант таблицы, в которой нет явных секций (да, так тоже можно), поэтому пользователи ванильных инструментов будут видеть её как обычную таблицу, не разделённую на части. При этом процесс обновления получает преимущества от выделения файлов по значениям полей неявного секционирования.\u003C\u002Fp\u003E\u003Cp\u003EЕсли пользователи не требуют секционирование, а данные, содержащиеся в таблице, дают возможность фрагментирования, то этим вариантом стоит воспользоваться.\u003C\u002Fp\u003E\u003Cfigure class=\"\"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" alt=\"Рисунок 5. Таблица без явных и неявных секций\" title=\"Рисунок 5. Таблица без явных и неявных секций\" height=\"422\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F7d7\u002Fe19\u002F98a\u002F7d7e1998a564103de10de0cbfa5ebe35.png\" data-width=\"314\"\u002F\u003E\u003Cfigcaption\u003EРисунок 5. Таблица без явных и неявных секций\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EНа \u003Cem\u003Eрисунке 5\u003C\u002Fem\u003E изображен вариант, который содержит требующую обновления таблицу, но секционирование не требуется и не найден подходящий способ сделать его неявным. В таком случае, можно отказаться от создания секций и просто выбрать поле упорядочивания. Таблица при этом видна как обычная, \"плоская\" структура.\u003C\u002Fp\u003E\u003Cp\u003EЯ назвал этот способ \"\u003Cem\u003Eорганизованная таблица\u003C\u002Fem\u003E\", потому что в сравнении с предыдущим способом, данные явно упорядочены, организованы весьма строгим образом. Идея формата организованной таблицы имеет немало общего с форматами Apache Iceberg и Apache Hudi, впрочем, при разработке я был вдохновлён Apache Iceberg, а о том, что в Hudi тоже используется схожая идея (интервалы значений) узнал значительно позже, когда основная часть уже была реализована. Кроме того, я пытался не совершить ошибок, которые сделаны в формате CarbonData, мой опыт использования его скорее негативный. И то, что Delta lake тоже содержит схожий подход: zOrder, стало очевидно позже.\u003C\u002Fp\u003E\u003Cp\u003EВначале я представил проверку концепции (\u003Cem\u003Eдалее PoC - от англ. «Proof of concept»\u003C\u002Fem\u003E) - скрипты, которые выполнял в spark-shell. Это заняло примерно неделю. За это время я исследовал, как сохранить таблицу так, чтобы конечные файлы содержали непересекающиеся интервалы значений одного конкретного поля, первичного ключа, или поля составного первичного ключа с наибольшей селективностью. Но использование именно первичного ключа не обязательно, ведь можно взять любое другое поле, если оно обеспечивает высокую селективность (но не обязательно уникальное) и не содержит большого количества одинаковых значений, неважно null это, или какое-то конкретное значение. Очень хорошо, если это поле сможет разместить записи с высокой и редкой частотой изменения в разные группы файлов.\u003C\u002Fp\u003E\u003Cp\u003EТакже я смог выделить поле с низкой селективностью и сохранил файлы с разными значениями этого поля в отдельных группах. Это было поле неявного секционирования, которое помогло в выделении изменяющихся файлов. Я проработал сохранение нужной информации о полученных файлах, назвав дополнительную структуру картой данных (data map). И использовал карту данных для анализа дельты данных и выбора нужных файлов.\u003C\u002Fp\u003E\u003Cp\u003EЗатем я применял к этим файлам операции из шагов 3 и 4 (помните тот список над таблицей 1 выше?), записывал их и заменял старые файлы новыми.\u003C\u002Fp\u003E\u003Cp\u003EВсе операции занимали около пятнадцати минут, и я видел, что новые файлы суммарно составляют около 200 Гб (размер таблицы около 27 Тб), то есть очень небольшую часть таблицы.\u003C\u002Fp\u003E\u003Cp\u003EВсё это было сделано стандартными средствами Spark и hadoop.FileSystem. Многие операции не были оптимальными, например, чтобы минимизировать коллизии хеширования (разделение по модулю хеша - стандартный способ партиционирования RDD), я увеличивал количество партиций RDD в восемь раз от требуемого.\u003C\u002Fp\u003E\u003Cp\u003EСледующие два месяца ушли на разработку прототипа, который, как и PoC, содержал одну дополнительную деталь, дельта-файлы. В них должны были уходить новые записи, если они относились к файлу, в котором ни одна запись не удалялась (или не обновлялась, помните, я говорил, что обновления обрабатываются нами как удаление плюс вставка). В дальнейшем от дельта-файлов пришлось отказаться, но, возможно, кто-то предложит алгоритм, в котором они будут уместны.\u003C\u002Fp\u003E\u003Cp\u003EПрототип работал достаточно долго, примерно год, и показал свою эффективность. Время основного обновления существенно сократилось:\u003C\u002Fp\u003E\u003Cp\u003EТаблица 2\u003C\u002Fp\u003E\u003Cdiv\u003E\u003Cdiv class=\"table\"\u003E\u003Ctable\u003E\u003Ctbody\u003E\u003Ctr\u003E\u003Ctd\u003E\u003Cp\u003E\u003Cstrong\u003EРазмер таблицы, Тб\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E\u003Cstrong\u003EКоличество секций\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E\u003Cstrong\u003EОбновляется секций\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E\u003Cstrong\u003EОбновляется файлов\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E\u003Cstrong\u003EВремя применения дельты к основному набору данных\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E\u003Cstrong\u003EСуммарное время работы\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd\u003E\u003Cp\u003E30\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E2000\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E500-600\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E6000-10000\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E1,5-2 часа\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003Cp\u003E5,5-6 часов\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cp\u003E \u003C\u002Fp\u003E\u003Cp\u003EВпрочем, ни одна другая операция не изменилась, поэтому мы пришли к выводу о необходимости отключения логических проверок. Но это было вынужденное решение, ведь проверки позволяют нам уверенно говорить, что мы работаем профессионально и не поставляем некорректные данные пользователям.\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EСтало ясно, что требуется оптимизация, которая позволит вывести работу с данными на новый уровень, и эта оптимизация привела к решению о создании отдельной библиотеки, основанной на Spark.\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003Cp\u003EДля полноценной библиотеки, которую могли бы применять пользователи, разумно взять за основу существующие внешний и внутренний API Spark и реализовать идеи так эффективно, как он позволит.\u003C\u002Fp\u003E\u003Cp\u003EЯ писал о создании DataSource, но он будет максимально полезен в сочетании с другими структурами: партишенер, FileIndex, FileWriter, различные логические и физические планы, средства работы с метаданными и т. д.\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EВсе эти средства направлены на то, чтобы избавить нас от энтропийных процессов, а если возникает нештатная ситуация, то помочь взять энтропию под контроль.\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003Cp\u003EНиже, на \u003Cem\u003Eрисунке 6 \u003C\u002Fem\u003Eпоказано, как должно выглядеть обновление таблицы. Нет переходов с нарастанием энтропии, метаинформация бережно сохраняется от состояния к состоянию и каждый шаг с пользой может эти метаданные применить:\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" alt=\"Рисунок 6. Обновление секционированной таблицы\" title=\"Рисунок 6. Обновление секционированной таблицы\" height=\"832\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F30f\u002F5ce\u002Fde4\u002F30f5cede47c43a318408e4e5e7d320b9.png\" data-width=\"1973\"\u002F\u003E\u003Cfigcaption\u003EРисунок 6. Обновление секционированной таблицы\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003EКомпоненты библиотеки: Partitioner\u003C\u002Fh3\u003E\u003Cp\u003EНачать описание созданной библиотеки будет логично с партишенера. Дело в том, что партишенер представляет способ организации данных как для хранения, так и для работы с ними. Именно в результате его работы будут получены такие данные, которые можно записать, прочитать и эффективно обработать. Без партишенера процесс не может ни начаться, ни продолжиться, так что именно он идёт самым первым пунктом.\u003C\u002Fp\u003E\u003Cp\u003EОбщепринятый вариант для Hive и Spark – это секционирование по хешу одного или нескольких полей. Этот вариант хорош и удобен, потому что есть возможность обеспечить одинаковые партиции для одинаковых данных, при этом не требуется передавать состояние между инстансами партишенера. Формула хеширования обеспечивает одинаковое конечное распределение для любого набора данных.\u003C\u002Fp\u003E\u003Cp\u003EВот как это работает:\u003C\u002Fp\u003E\u003Cp\u003E1.     Создаётся экземпляр кейс-класса логического плана выполнения org.apache.spark.sql.catalyst.plans.logical.RepartitionByExpression.\u003C\u002Fp\u003E\u003Cp\u003E2.     Переданные в него выражения spark используются для создания экземпляра кейс-класса физического плана выполнения org.apache.spark.sql.catalyst.plans.physical.HashPartitioning\u003C\u002Fp\u003E\u003Cp\u003E3.     Кейс-класс org.apache.spark.sql.execution.exchange.ShuffleExchangeExec обеспечивает создание для RDD анонимного класса org.apache.spark.Partitioner. Сам класс Partitioner определён как абстрактный и содержит два нереализованных метода:\u003C\u002Fp\u003E\u003Cdiv\u003E\u003Cdiv class=\"table\"\u003E\u003Ctable\u003E\u003Ctbody\u003E\u003Ctr\u003E\u003Ctd\u003E\u003Cp\u003E\u003Cstrong\u003Eabstract\u003C\u002Fstrong\u003E \u003Cstrong\u003Eclass\u003C\u002Fstrong\u003E Partitioner \u003Cstrong\u003Eextends\u003C\u002Fstrong\u003E Serializable {\u003C\u002Fp\u003E\u003Cp\u003E    \u003Cstrong\u003Edef\u003C\u002Fstrong\u003E numPartitions\u003Cstrong\u003E:\u003C\u002Fstrong\u003E Int\u003C\u002Fp\u003E\u003Cp\u003E    \u003Cstrong\u003Edef\u003C\u002Fstrong\u003E getPartition(key\u003Cstrong\u003E:\u003C\u002Fstrong\u003E Any)\u003Cstrong\u003E:\u003C\u002Fstrong\u003E Int\u003C\u002Fp\u003E\u003Cp\u003E}\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cp\u003E4. По выбранным для партиционирования полям рассчитывается неотрицательный модуль хеша по числу партиций (выражение из класса HashPartitioning):\u003C\u002Fp\u003E\u003Cdiv\u003E\u003Cdiv class=\"table\"\u003E\u003Ctable\u003E\u003Ctbody\u003E\u003Ctr\u003E\u003Ctd\u003E\u003Cp\u003EPmod(\u003Cstrong\u003Enew\u003C\u002Fstrong\u003E Murmur3Hash(expressions), Literal(numPartitions))\u003C\u002Fp\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cp\u003EТеперь несложно обозначить основные характеристики этого алгоритма.\u003C\u002Fp\u003E\u003Cp\u003EКак видно, для физического расчета будет использован org.apache.spark.unsafe.hash.Murmur3_x86_32.\u003Cbr\u002F\u003EПолученное неотрицательное число (Pmod(hash)) всегда одно и тоже для одних и тех же аргументов. Таким образом, состояние заложено в формуле хеширования и не должно передаваться между инстансами Partitioner.\u003C\u002Fp\u003E\u003Cp\u003EПричина выбора именно такой реализации очевидна: тот же самый алгоритм применяется для формирования бакетов Hive. Поэтому таблица Hive, которую сохранили с указанием полей bucketBy(“DISTRIBUTED BY” в HiveQL), по распределению эквивалентна репартиционированному датасету.\u003C\u002Fp\u003E\u003Cp\u003EНужно отметить, что все расчёты производятся для сериализованных экземпляров класса InternalRow с использованием unsafe memory, и выполняются очень эффективно. Поэтому замена этого процесса на операции с RDD[Row] и попытка создания простого партишенера, который получал бы значение для хеша из конкретных полей, приведёт к большому объёму сериализации-десериализации, а это вызовет крайне серьёзное замедление.\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EС моей точки зрения, применение HashPartitioning имеет существенные недостатки: \u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003E1)\u003C\u002Fstrong\u003E \u003Cstrong\u003EНет возможности разделить значения на часто обновляемые и редко обновляемые\u003C\u002Fstrong\u003E. Значения Murmur3Hash ничего не говорят об исходных данных, задача хеша в этом случае - распределить значения для равномерного использования всех полученных элементов набора данных, будь это файл или итератор по записям.\u003C\u002Fp\u003E\u003Cp\u003EНаша же задача - сделать так, чтобы как можно меньше файлов были перезаписаны, а практика показывает, что переписываемые значения склонны к кластеризации, то есть, с большей вероятностью изменения придут по нескольким записям, идущим достаточно близко одна к другой.\u003C\u002Fp\u003E\u003Cp\u003EПри хешировании кластер значений должен быть разбит применяемой хеш-функцией и если он будет разделён на отдельные записи, то хеш-функция хорошо справилась со своей задачей. Это равномерное распределение очень полезно, когда нужно выполнить, например, Join всех данных, но при обновлении реальных данных часто бывает, что обновления составляют тысячные доли от всего объема таблицы. Поэтому разумно кластеризовать данные по определённому признаку, а не «разбрасывать» через хеш.\u003C\u002Fp\u003E\u003Cp\u003EДа, может помочь LSH – Locality Sensitive Hash – хэш с учётом близости значений, но его также надо реализовывать для произвольного типа данных, а образец в виде RangePartitioner уже выполняет требуемое разделение, причём совершенно точно, и делает в перспективе возможной работу с интервалами значений и временными рядами! Кроме того, LSHпартишенер не обязательно поможет в динамическом увеличении количества бакетов, ведь функцию придётся модернизировать для нового числа секций.\u003C\u002Fp\u003E\u003Cp\u003EГоворя более общим языком, фрагментирование набора данных по хешам предназначено для глобализации использования элементов данных, фрагментирование по интервалам значений предназначено для локализации использования элементов.\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003E2)\u003C\u002Fstrong\u003E \u003Cstrong\u003EРанее было сказано, что хеш всегда один и тот же для одинаковых аргументов, но верно и то, что хеш будет одним и тем же для разных аргументов\u003C\u002Fstrong\u003E.\u003C\u002Fp\u003E\u003Cp\u003EДело не в арифметическом модуле, который сильно ограничивает количество вариантов на выходе, а в том, что множество хешей в общем случае слабее множества исходных значений.\u003C\u002Fp\u003E\u003Cp\u003EПоэтому я и обращаю внимание на то, что коллизии не просто могут появиться, а утверждаю, что они обязательно появятся в ходе работы процесса. И это не вполне оптимально, ведь в одну и ту же партицию RDD могут попасть значения, принадлежащие разным секциям Hive. Spark может корректно обработать и записать такой файл, но, как правило, это займёт больше времени, потребуются дополнительные проверки и сортировки.\u003C\u002Fp\u003E\u003Cp\u003EКроме того, неявное секционирование затруднено при этом способе, потому что стандартный FileWriter не сможет разделить данные, относящиеся к неявным секциям. Требуется сначала записать их как явные, а затем переименовать каждый записанный файл. Но это не главная причина, самое серьёзное последствие коллизий во время Shuffle наступает, если файл или итератор по данным включает в себя несколько (хотя бы два) непоследовательных интервала значений поля упорядочивания. «Непоследовательных» означает, что между этими интервалами в секции или таблице, если секционирование отсутствует, точно есть другие значения.\u003C\u002Fp\u003E\u003Cp\u003EРазделить непоследовательные диапазоны не представляется возможным без дополнительных затрат, выбранная схема хранения метаданных не поддерживает несколько интервалов в одном файле. Такая коллизия приведёт к созданию \"компонента\", содержащего несколько файлов.\u003C\u002Fp\u003E\u003Cp\u003EКомпонент в конечном итоге содержит один интервал значений поля упорядочивания, не пересекающийся с прочими, но этот интервал может быть слишком большим и приведёт к длительному выполнению одного Spark task.\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003E3)\u003C\u002Fstrong\u003E \u003Cstrong\u003EЕсли хочется совместить физическое секционирование Hive и бакетирование, то в каждой партиции может появиться файл, принадлежащий определённому бакету\u003C\u002Fstrong\u003E.\u003C\u002Fp\u003E\u003Cp\u003EПолучится, что бакет разделён на несколько файлов, находящихся в разных секциях. При этом, если бакеты сортируются по некоторым полям, то сортировка будет утрачена, нет реализованного способа прочитать разные файлы так, чтобы итератор от первой записи первого файла до последней записи последнего файла был отсортирован.\u003C\u002Fp\u003E\u003Cp\u003EТаким образом, приложенные усилия по организации данных окажутся напрасными, данные придётся сортировать снова и снова, и делать это для всего набора данных, потому что Spark из коробки не умеет находить партиции RDD, по которым нужно сделать сортировку.\u003C\u002Fp\u003E\u003Cp\u003EДанные для нового DataSource должны быть разделены по диапазонам значений поля упорядочивания (кластеризованы по этому полю), и алгоритм будет значительно отличаться от описанного способа. Зато алгоритм будет похож на тот, что используется в dataframe.repartitionByRange().\u003C\u002Fp\u003E\u003Cp\u003EОпишем для начала алгоритм repartitionByRange():\u003C\u002Fp\u003E\u003Cp\u003E1.     Для итераций по RDD будет использован экземпляр класса MutablePair, первый элемент которого будет содержать всю сериализованную запись. Это делается для того, чтобы в каждой секции RDD мы работали бы только с одним объектом, а не создавали большое количество UnsafeRow\u003C\u002Fp\u003E\u003Cp\u003E2.     Создается implicit val ordering = new LazilyGeneratedOrdering(sortingExpressions, outputAttributes). Это значение нужно для того, чтобы сравнивать и сортировать значения переданных выражений, оно тоже применяется к InternalRow;\u003C\u002Fp\u003E\u003Cp\u003E3.     Созданный экземпляр RangePartitioner получает RDD, количество партиций, направление сортировки (по умолчанию-возрастающее, аргумент типа Boolean) и желаемое количество сэмплов в одной секции RDD;\u003C\u002Fp\u003E\u003Cp\u003E4.     Вычисляются верхние границы для каждой будущей секции RDD:\u003C\u002Fp\u003E\u003Cp\u003Eo  Немного пересчитав желаемое количество сэмплов, класс приступает к сэмплированию RDD. Делается это так:\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E- желаемое количество первых записей помещается в массив фиксированного размера; если записи на этом кончились, то массив урезается до реального размера и возвращается;\u003C\u002Fp\u003E\u003Cp\u003E- если же записи ещё есть, то цикл проходит по записям и записывает каждую следующую вместо одной случайной записи в массив. Одновременно подсчитывается общее количество. Полученный результат передаётся на драйвер;\u003C\u002Fp\u003E\u003Cp\u003Eo   Теперь, с известным размером партиций и полученными массивами сэмплов, класс проводит оценку, нет ли больших секций, из которых получено недостаточно записей;\u003C\u002Fp\u003E\u003Cp\u003Eo   Если такие секции встретились, то они дополнительно сэмплируются с использованием очень интересного класса PartitionPruningRDD (описание этого класса выходит за рамки этой статьи). Сэмплирование осуществляется стандартным методом RDD.sample. Полученный результат передаётся на драйвер;\u003C\u002Fp\u003E\u003Cp\u003Eo   Полученные InternalRow, а их количество сейчас заметно больше, чем будет в итоге, чтобы обеспечить приблизительно равный размер секций статистически, сортируются с использованием ordering, и производится отбор верхних границ, исходя из накопительной суммы весов.\u003C\u002Fp\u003E\u003Cp\u003E5.     Определяется метод бинарного поиска нужного интервала (меньше или равен определённой верхней границе, больше предшествующей верхней границы), который должен ускорить обработку больших наборов данных;\u003C\u002Fp\u003E\u003Cp\u003E6.     Во время работы класса, полученная запись сравнивается с границами методом линейного поиска, или, если будущих секций больше, чем 128, то бинарным поиском находится, к какой секции она относится;\u003C\u002Fp\u003E\u003Cp\u003E7.     Класс должен быть сериализуемым, чтобы его можно было передавать на экзекуторы, поэтому в него добавлены методы writeObject и readObject.\u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EЯ хочу уточнить, почему меня не устроила реализация RangePartitioner:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003Cp\u003E1)  Он не имеет разделяемого состояния.\u003C\u002Fp\u003E\u003Cp\u003EПсевдослучайные функции для выбора сэмпла RDD, даже сидированные одним и тем же значением, не могут обеспечить полностью стабильную выборку, границы партиций RDD могут быть определены по-разному даже для одного и того же набора данных. Если же наборы данных разные, то одни и те же значения поля упорядочивания из каждого набора могут находится в разных секциях RDD, ведь RangePartitioner знает о распределении только одного набора данных. Поле rangeBounds приватное и не может быть передано в другой RangePartitioner, а если бы оно и было открыто, RangePartitionerне может использовать готовую информацию о распределении;\u003C\u002Fp\u003E\u003Cp\u003E2) Сэмплирование осуществляется в общем случае, два раза, если находятся слишком большие партиции;\u003C\u002Fp\u003E\u003Cp\u003E3) Материализация предварительного результата на драйвере выглядит преждевременной, ведь часть bigdata – это тоже bigdata, не исключено, что памяти на драйвере не хватит, если мы пытаемся разделить таблицу размером в десятки или сотни терабайт. Лучше произвести отбор верхних границ в RDD, и получить на драйвере только необходимые данные;\u003C\u002Fp\u003E\u003Cp\u003E4) Наша задача может требовать фиксированное разделение по секциям Hive, и динамическое формирование партиций RDD, \"вложенное\" в это строгое разделение, а в RangePartitioner существует только вариант с сортировкой по всем полям. При сохранении этот порядок может измениться, поскольку из одной партиции наверняка будут получены несколько файлов;\u003C\u002Fp\u003E\u003Cp\u003E5) Я решил использовать как основной аргумент не желаемое количество партиций RDD, а желаемое количество записей в файле – это в конечном итоге должно привести к лучшему распределению.\u003C\u002Fp\u003E\u003Cp\u003EКроме того, фиксированные секции станут ключами карты (Map[InternalRow, Array[InternalRow]) и каждый элемент карты будет содержать массив с верхними границами конкретной партиции, а это позволит уменьшить порядок бинарного поиска.\u003C\u002Fp\u003E\u003Cp\u003E6)  Получить весь InternalRow нужно, если мы собираемся выполнить \u003Cem\u003Edataframe.repartitionAndSortWithinPartitions\u003C\u002Fem\u003E. Это очень полезный метод, позволяющий внести локальную сортировку в shuffle. Но мы собираемся сортировать по полям, в общем случае отличным от поля упорядочивания, а значит можно сразу же выделить необходимые для партиционирования колонки и сократить объем получаемых данных потенциально в десятки или сотни раз, и заодно сделать партишенер подходящим для секционирования совершенно других наборов данных, в которых есть эквивалентные поля. Именно этот пункт поможет с реализацией передачи состояния (см. первый пункт этого списка) и с локальной сортировкой, объединённой с shuffle!\u003C\u002Fp\u003E\u003Cp\u003EПоскольку текст уже весьма большой, сама реализация будет в следующей части.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E","tags":[{"titleHtml":"spark"},{"titleHtml":"таблицы"},{"titleHtml":"data"}],"metadata":{"stylesUrls":[],"scriptUrls":[],"shareImageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F872\u002F21f\u002F373\u002F87221f373acb676fa493e066662a3872.jpg","shareImageWidth":1200,"shareImageHeight":630,"vkShareImageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F872\u002F21f\u002F373\u002F87221f373acb676fa493e066662a3872.jpg","schemaJsonLd":"{\"@context\":\"http:\\\u002F\\\u002Fschema.org\",\"@type\":\"Article\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https:\\\u002F\\\u002Fhabr.com\\\u002Fru\\\u002Fcompany\\\u002Fsberbank\\\u002Fblog\\\u002F582090\\\u002F\"},\"headline\":\"Изменить сохранения Spark! Часть первая: разделяй и… сортируй\",\"datePublished\":\"2021-10-07T10:45:24+03:00\",\"dateModified\":\"2021-10-15T11:36:41+03:00\",\"author\":{\"@type\":\"Person\",\"name\":\"Sber\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Habr\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fa_\\\u002Flk\\\u002F9m\\\u002Fa_lk9mjkccjox-zccjrpfolmkmq.png\"}},\"description\":\"Автор: Иван Калининский, участник профессионального сообщества Сбера&nbsp;SberProfi&nbsp;DWH\\\u002FBigData.Профессиональное сообщество&nbsp;SberProfi&nbsp;DWH\\\u002FBigData&nbsp;отвечает за развитие...\",\"url\":\"https:\\\u002F\\\u002Fhabr.com\\\u002Fru\\\u002Fcompany\\\u002Fsberbank\\\u002Fblog\\\u002F582090\\\u002F#post-content-body\",\"about\":[\"c_sberbank\",\"h_db_admins\",\"h_bigdata\",\"f_develop\",\"f_admin\"],\"image\":[\"https:\\\u002F\\\u002Fhabr.com\\\u002Fshare\\\u002Fpublication\\\u002F582090\\\u002F5ea735b56f9a531032f1bdf9e372feaf\\\u002F\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F872\\\u002F21f\\\u002F373\\\u002F87221f373acb676fa493e066662a3872.jpg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F4ca\\\u002Fb99\\\u002F7e5\\\u002F4cab997e5c17f6756bc26314251202e7.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F96f\\\u002Fa84\\\u002Fa38\\\u002F96fa84a385360aa6c226d57dce164d99.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F909\\\u002F5b1\\\u002Fbcc\\\u002F9095b1bcc15e9091691ccd20069175c3.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F673\\\u002F63e\\\u002Fa2c\\\u002F67363ea2c8432f75acdc8d9ab1244485.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Fe4f\\\u002F0b0\\\u002F4a0\\\u002Fe4f0b04a01b8ed7f3dc7827b54ef6d18.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F7d7\\\u002Fe19\\\u002F98a\\\u002F7d7e1998a564103de10de0cbfa5ebe35.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F30f\\\u002F5ce\\\u002Fde4\\\u002F30f5cede47c43a318408e4e5e7d320b9.png\"]}","metaDescription":"Автор: Иван Калининский, участник профессионального сообщества Сбера&nbsp;SberProfi&nbsp;DWH\u002FBigData.Профессиональное сообщество&nbsp;SberProfi&nbsp;DWH\u002FBigData&nbsp;отвечает за развитие компетенций в...","mainImageUrl":null,"amp":true},"polls":[],"commentsEnabled":true,"rulesRemindEnabled":false,"votesEnabled":true,"status":"published","plannedPublishTime":null,"checked":null,"isEditorial":false}},"articlesIds":{},"isLoading":false,"pagesCount":{},"route":{},"reasonsList":null,"view":"cards","lastVisitedRoute":{},"ssrCommentsArticleIds":[""],"karma":{}},"authorContribution":{"authors":{}},"betaTest":{"currentAnnouncement":null,"announcements":{},"announcementCards":null,"announcementComments":{},"announcementCommentThreads":{},"announcementCommentingStatuses":{},"archivedList":[]},"authorStatistics":{"articleRefs":{},"articleIds":{},"pagesCount":{},"route":{},"viewsCount":[],"maxStatsCount":{}},"career":{"seoLandings":[],"hubs":""},"comments":{"articleComments":{},"searchCommentsResults":null,"previewComment":null,"pagesCount":null,"commentAccess":{},"scrollParents":{},"pageArticleComments":{"lastViewedComment":0,"postId":null,"lastCommentTimestamp":"","moderated":[],"moderatedIds":[],"commentRoute":""}},"companies":{"companyRefs":{"sberbank":{"alias":"sberbank","imageUrl":"\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fcompany\u002F9db\u002F3c1\u002Fec0\u002F9db3c1ec02265b8bcbfdfb0d23d8b9f2.jpg","titleHtml":"Сбер","descriptionHtml":"Больше чем банк","relatedData":null,"statistics":{"postsCount":229,"newsCount":7,"vacanciesCount":156,"employeesCount":159,"careerRating":4.07,"subscribersCount":32020,"rating":229.68,"invest":null},"foundationDate":{"year":"1841","month":"11","day":"12"},"location":{"city":{"id":"447159","title":"Москва"},"region":{"id":"1885","title":"Москва и Московская обл."},"country":{"id":"168","title":"Россия"}},"siteUrl":"http:\u002F\u002Fwww.sber.ru\u002F","staffNumber":"свыше 10 000 человек","registrationDate":"2011-02-08T10:09:33+00:00","representativeUser":null,"contacts":[],"settings":{"analyticsSettings":[{"type":"ga","trackingId":"UA-170457662-1"}],"branding":{"imageUrl":"\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fbranding\u002F6ac\u002F725\u002F4eb\u002F6ac7254eb269ffa84589b75da04eb5ae.png","linkUrl":"","pixelUrl":""},"status":"active"},"metadata":{"titleHtml":"Сбер, Москва - Больше чем банк с 12 ноября 1841 г.","title":"Сбер, Москва - Больше чем банк с 12 ноября 1841 г.","keywords":["Управление проектами","Машинное обучение","Искусственный интеллект","Natural Language Processing","Блог компании SberDevices"],"descriptionHtml":"229 статей от авторов компании Сбер","description":"229 статей от авторов компании Сбер"},"aDeskSettings":null,"careerAlias":"sberbank-russia","maxCustomTrackerLinks":3}},"companyIds":{},"companyTopIds":{},"pagesCount":{},"companyProfiles":{},"companiesCategories":[],"companiesCategoriesTotalCount":0,"companiesWidgets":{},"companiesWorkers":{},"companiesFans":{},"route":{},"isLoading":false,"companyWorkersLoading":false,"companyFansLoading":false,"vacancies":{}},"companiesContribution":{"hubs":{},"flows":{},"companyRefs":{}},"companyHubsContribution":{"contributionRefs":{"hubRefs":{},"hubIds":{}}},"conversation":{"messages":[],"respondent":null,"isLoadMore":false},"conversations":{"conversations":[],"unreadCount":0,"pagesCount":0,"isLoadMore":false},"desktopState":{"desktopFl":null,"desktopHl":null,"isChecked":false,"isLoginDemanded":false},"dfp":{"slotsDict":{}},"docs":{"menu":{},"articles":{},"mainMenu":[],"loading":{"main":false,"dropdown":false,"article":false}},"feature":{"isProbablyVisible":"true"},"flows":{"flows":[{"alias":"develop","id":1,"route":{"name":"FLOW_PAGE","params":{"flowName":"develop"}}},{"alias":"admin","id":6,"route":{"name":"FLOW_PAGE","params":{"flowName":"admin"}}},{"alias":"design","id":2,"route":{"name":"FLOW_PAGE","params":{"flowName":"design"}}},{"alias":"management","id":3,"route":{"name":"FLOW_PAGE","params":{"flowName":"management"}}},{"alias":"marketing","id":4,"route":{"name":"FLOW_PAGE","params":{"flowName":"marketing"}}},{"alias":"popsci","id":7,"route":{"name":"FLOW_PAGE","params":{"flowName":"popsci"}}}]},"global":{"isPwa":false,"device":"desktop","isHabrCom":true},"hubs":{"hubRefs":{},"hubIds":{},"pagesCount":{},"isLoading":false,"route":{}},"hubsBlock":{"hubRefs":{},"hubIds":{}},"i18n":{"fl":"ru","hl":"ru"},"info":{"infoPage":{},"isLoading":true},"location":{"urlStruct":{"protocol":null,"slashes":null,"auth":null,"host":null,"port":null,"hostname":null,"hash":null,"search":null,"query":{},"pathname":null,"path":null,"href":""},"searchQuery":null},"me":{"user":null,"ppgDemanded":false,"karmaResetInfo":{"canReincarnate":null,"wasReincarnated":null,"currentScore":null},"notes":null},"mostReadingList":{"mostReadingListIds":[],"mostReadingListRefs":null,"promoPost":null},"pinnedPost":{"pinnedPost":null},"ppa":{"articles":{},"card":null,"transactions":null,"totalTransactions":null,"isAccessible":null},"projectsBlocks":{"activeBlocks":{}},"pullRefresh":{"shouldRefresh":false},"sandbox":{"articleIds":[],"articleRefs":{},"pagesCount":null,"route":{},"lastVisitedRoute":{},"isLoading":false},"settingsOther":{"inputs":{"uiLang":{"errors":[],"ref":null,"value":""},"articlesLangEnglish":{"errors":[],"ref":null,"value":false},"articlesLangRussian":{"errors":[],"ref":null,"value":false},"agreement":{"errors":[],"ref":null,"value":false},"email":{"errors":[],"ref":null,"value":true},"digest":{"errors":[],"ref":null,"value":true}}},"similarList":{"similarListIds":[],"similarListRefs":null},"ssr":{"error":null,"isDataLoaded":false,"isDataLoading":false,"isHydrationFailed":false,"isServer":false},"userHubsContribution":{"contributionRefs":{"hubRefs":{},"hubIds":{}}},"userInvites":{"availableInvites":0,"usedInvitesIds":[],"usedInvitesRefs":{},"usedInvitesPagesCount":0,"unusedInvitesIds":[],"unusedInvitesRefs":{},"unusedInvitesPagesCount":0},"users":{"authorRefs":{},"authorIds":{},"pagesCount":{},"authorProfiles":{},"userHubs":{},"userInvitations":{},"authorFollowers":{},"authorFollowed":{},"karmaStats":[],"statistics":null,"isLoading":false,"authorFollowersLoading":false,"authorFollowedLoading":false,"userHubsLoading":false,"userInvitationsLoading":false,"route":{}},"viewport":{"prevScrollY":{},"scrollY":0,"width":0},"tracker":{"items":{},"pagesCache":{},"markedViewedSilently":{},"markedRead":{},"unreadCounters":{"applications":null,"system":null,"mentions":null,"subscribers":null,"posts_and_comments":null},"unviewedCounters":{"applications":null,"system":null,"mentions":null,"subscribers":null,"posts_and_comments":null}}};(function(){var s;(s=document.currentScript||document.scripts[document.scripts.length-1]).parentNode.removeChild(s);}());</script>
<script src="https://assets.habr.com/habr-web/js/chunk-vendors.c2c3fc9a.js" defer></script><script src="https://assets.habr.com/habr-web/js/app.c0af73e7.js" defer></script>



    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    </script>
  
  <script type="text/javascript" >
    (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
    m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
    (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

    ym(24049213, "init", {
      defer:true,
      trackLinks:true,
      accurateTrackBounce:true,
      webvisor:false,
    });
  </script>
  <noscript>
    <div>
      <img src="https://mc.yandex.ru/watch/24049213" style="position:absolute; left:-9999px;" alt="" />
    </div>
  </noscript>
  
    <script type="text/javascript">
      window.addEventListener('load', function () {
        setTimeout(() => {
          const img = new Image();
          img.src = 'https://vk.com/rtrg?p=VK-RTRG-421343-57vKE';
        }, 0);
      });
    </script>
  
</body>
</html>
